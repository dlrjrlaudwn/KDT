{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in c:\\users\\msi\\anaconda3\\envs\\text_018_230_38\\lib\\site-packages (0.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\msi\\anaconda3\\envs\\text_018_230_38\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\msi\\anaconda3\\envs\\text_018_230_38\\lib\\site-packages (from gensim) (1.22.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\msi\\anaconda3\\envs\\text_018_230_38\\lib\\site-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\msi\\anaconda3\\envs\\text_018_230_38\\lib\\site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in c:\\users\\msi\\anaconda3\\envs\\text_018_230_38\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Korpora import Korpora\n",
    "from konlpy.tag import Okt\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#문장 분류 모델\n",
    "\n",
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(self,n_vocab,hidden_dim,embedding_dim,n_layers,dropout=0.5,bidirectional=True,model_type='lstm'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding=nn.Embedding(num_embeddings=n_vocab,embedding_dim=embedding_dim,padding_idx=0)\n",
    "\n",
    "        if model_type=='rnn':\n",
    "            self.model=nn.RNN(input_size=embedding_dim,hidden_size=hidden_dim,num_layers=n_layers, bidirectional=bidirectional,dropout=dropout,batch_first=True)\n",
    "\n",
    "        elif model_type=='lstm':\n",
    "            self.model=nn.LSTM(input_size=embedding_dim,hidden_size=hidden_dim,num_layers=n_layers,bidirectional=bidirectional,dropout=dropout,batch_first=True)\n",
    "\n",
    "        if bidirectional:\n",
    "            self.classifier=nn.Linear(hidden_dim*2,1)\n",
    "        else:\n",
    "            self.classifier=nn.Linear(hidden_dim,1)\n",
    "\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        embeddings=self.embedding(inputs)\n",
    "        output,_=self.model(embeddings)\n",
    "        last_output=output[:,-1,:]\n",
    "        last_output=self.dropout(last_output)\n",
    "        logits=self.classifier(last_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\MSI\\Korpora\\nsmc\\ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\MSI\\Korpora\\nsmc\\ratings_test.txt\n",
      "|       | text                                                                                     |   label |\n",
      "|------:|:-----------------------------------------------------------------------------------------|--------:|\n",
      "| 33553 | 모든 편견을 날려 버리는 가슴 따뜻한 영화. 로버트 드 니로, 필립 세이모어 호프만 영원하라. |       1 |\n",
      "|  9427 | 무한 리메이크의 소재. 감독의 역량은 항상 그 자리에...                                    |       0 |\n",
      "|   199 | 신날 것 없는 애니.                                                                       |       0 |\n",
      "| 12447 | 잔잔 격동                                                                                |       1 |\n",
      "| 39489 | 오랜만에 찾은 주말의 명화의 보석                                                         |       1 |\n",
      "Training Data Size: 45000\n",
      "Testing Data Size: 5000\n"
     ]
    }
   ],
   "source": [
    "#데이터세트 불러오기\n",
    "\n",
    "corpus=Korpora.load('nsmc')\n",
    "corpus_df=pd.DataFrame(corpus.test)\n",
    "\n",
    "train=corpus_df.sample(frac=0.9,random_state=42)\n",
    "test=corpus_df.drop(train.index)\n",
    "\n",
    "print(train.head(5).to_markdown())\n",
    "print('Training Data Size:',len(train))\n",
    "print('Testing Data Size:',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 토큰화 및 단어 사전 구축\n",
    "\n",
    "def build_vocab(corpus,n_vocab,special_tokens):\n",
    "    counter=Counter()\n",
    "    for tokens in corpus:\n",
    "        counter.update(tokens)\n",
    "    vocab=special_tokens\n",
    "    for token,count in counter.most_common(n_vocab):\n",
    "        vocab.append(token)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', '.', '이', '영화', '의', '..', '가', '에', '...']\n",
      "5002\n"
     ]
    }
   ],
   "source": [
    "tokenizer=Okt()\n",
    "train_tokens=[tokenizer.morphs(review) for review in train.text]\n",
    "test_tokens=[tokenizer.morphs(review) for review in test.text]\n",
    "\n",
    "vocab=build_vocab(corpus=train_tokens,n_vocab=5000,special_tokens=['<pad>','<unk>'])\n",
    "token_to_id={token:idx for idx,token in enumerate(vocab)}\n",
    "id_to_token={idx:token for idx,token in enumerate(vocab)}\n",
    "\n",
    "print(vocab[:10])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정수 인코딩 및 패딩\n",
    "\n",
    "def pad_sequences(sequences,max_length,pad_value):\n",
    "    result=list()\n",
    "    for sequence in sequences:\n",
    "        sequence=sequence[:max_length]\n",
    "        pad_length=max_length-len(sequence)\n",
    "        padded_sequence=sequence+[pad_value]*pad_length\n",
    "        result.append(padded_sequence)\n",
    "    return np.asarray(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 223 1716   10 4036 2095  193  755    4    2 2330 1031  220   26   13\n",
      " 4839    1    1    1    2    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "[3307    5 1997  456    8    1 1013 3906    5    1    1   13  223   51\n",
      "    3    1 4684    6    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "unk_id=token_to_id['<unk>']\n",
    "train_ids=[[token_to_id.get(token,unk_id) for token in review] for review in train_tokens]\n",
    "test_ids=[[token_to_id.get(token,unk_id) for token in review] for review in test_tokens]\n",
    "\n",
    "max_length=32\n",
    "pad_id=token_to_id['<pad>']\n",
    "train_ids=pad_sequences(train_ids,max_length,pad_id)\n",
    "test_ids=pad_sequences(test_ids,max_length,pad_id)\n",
    "\n",
    "print(train_ids[0])\n",
    "print(test_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['모든',\n",
       " '편견',\n",
       " '을',\n",
       " '날려',\n",
       " '버리는',\n",
       " '가슴',\n",
       " '따뜻한',\n",
       " '영화',\n",
       " '.',\n",
       " '로버트',\n",
       " '드',\n",
       " '니',\n",
       " '로',\n",
       " ',',\n",
       " '필립',\n",
       " '세이모어',\n",
       " '호프만',\n",
       " '영원하라',\n",
       " '.']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['이별',\n",
       " '의',\n",
       " '아픔',\n",
       " '뒤',\n",
       " '에',\n",
       " '찾아오는',\n",
       " '새로운',\n",
       " '인연',\n",
       " '의',\n",
       " '기쁨',\n",
       " 'But',\n",
       " ',',\n",
       " '모든',\n",
       " '사람',\n",
       " '이',\n",
       " '그렇지는',\n",
       " '않네',\n",
       " '..']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids=torch.tensor(train_ids)\n",
    "test_ids=torch.tensor(test_ids)\n",
    "\n",
    "train_labels=torch.tensor(train.label.values,dtype=torch.float32)\n",
    "test_labels=torch.tensor(test.label.values,dtype=torch.float32)\n",
    "\n",
    "train_dataset=TensorDataset(train_ids,train_labels)\n",
    "test_dataset=TensorDataset(test_ids,test_labels)\n",
    "\n",
    "train_loader=DataLoader(train_dataset,batch_size=16,shuffle=True)\n",
    "test_loader=DataLoader(test_dataset,batch_size=16,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#손실함수와 최적화 함수 정의\n",
    "\n",
    "n_vocab=len(token_to_id)\n",
    "hidden_dim=64\n",
    "embedding_dim=128\n",
    "n_layers=2\n",
    "\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "classifier=SentenceClassifier(n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim, n_layers=n_layers).to(device)\n",
    "criterion=nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer=optim.RMSprop(classifier.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 학습 및 테스트\n",
    "def train(model,datasets, criterion,optimizer,device,interval):\n",
    "    model.train()\n",
    "    losses=list()\n",
    "\n",
    "    for step,(inputs_ids,labels) in enumerate(datasets):\n",
    "        inputs_ids=inputs_ids.to(device)\n",
    "        labels=labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits=model(inputs_ids)\n",
    "        loss=criterion(logits,labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step%interval==0:\n",
    "            print(f'Trainn Loss {step}: {np.mean(losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, datasets,criterion,device):\n",
    "    model.eval()\n",
    "    losses=list()\n",
    "    corrects=list()\n",
    "\n",
    "    for step,(input_ids,labels) in enumerate(datasets):\n",
    "        input_ids=input_ids.to(device)\n",
    "        labels=labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits=model(input_ids)\n",
    "        loss=criterion(logits,labels)\n",
    "        losses.append(loss.item())\n",
    "        yhat=torch.sigmoid(logits)>.5\n",
    "        corrects.extend(torch.eq(yhat,labels).cpu().tolist())\n",
    "    print(f'Val Loss: {np.mean(losses)}, Val Accuracy: {np.mean(corrects)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainn Loss 0: 0.6923755407333374\n",
      "Trainn Loss 500: 0.693011573093856\n",
      "Trainn Loss 1000: 0.692117984299655\n",
      "Trainn Loss 1500: 0.6798009841661307\n",
      "Trainn Loss 2000: 0.6656260613886372\n",
      "Trainn Loss 2500: 0.6550038043664294\n",
      "Val Loss: 0.6041284230189582, Val Accuracy: 0.676\n",
      "Trainn Loss 0: 0.44447994232177734\n",
      "Trainn Loss 500: 0.5811614636294619\n",
      "Trainn Loss 1000: 0.5763473912076159\n",
      "Trainn Loss 1500: 0.5726397355185756\n",
      "Trainn Loss 2000: 0.570101874223773\n",
      "Trainn Loss 2500: 0.5587293476652785\n",
      "Val Loss: 0.4782313286020352, Val Accuracy: 0.7752\n",
      "Trainn Loss 0: 0.3493023216724396\n",
      "Trainn Loss 500: 0.4706128910332621\n",
      "Trainn Loss 1000: 0.457756325796053\n",
      "Trainn Loss 1500: 0.4493404351060665\n",
      "Trainn Loss 2000: 0.44373881154093725\n",
      "Trainn Loss 2500: 0.43817531561455886\n",
      "Val Loss: 0.4332203692711961, Val Accuracy: 0.802\n",
      "Trainn Loss 0: 0.4356110692024231\n",
      "Trainn Loss 500: 0.39436917744591804\n",
      "Trainn Loss 1000: 0.38678825131945915\n",
      "Trainn Loss 1500: 0.3846074265615889\n",
      "Trainn Loss 2000: 0.3822411625017261\n",
      "Trainn Loss 2500: 0.38255462018711145\n",
      "Val Loss: 0.41167542881097274, Val Accuracy: 0.8102\n",
      "Trainn Loss 0: 0.688123345375061\n",
      "Trainn Loss 500: 0.34023198244457475\n",
      "Trainn Loss 1000: 0.34354919661115574\n",
      "Trainn Loss 1500: 0.34477655298705107\n",
      "Trainn Loss 2000: 0.34067142182688365\n",
      "Trainn Loss 2500: 0.34000649341031675\n",
      "Val Loss: 0.3918961394613924, Val Accuracy: 0.8212\n"
     ]
    }
   ],
   "source": [
    "epochs=5\n",
    "interval=500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(classifier,train_loader,criterion,optimizer,device,interval)\n",
    "    test(classifier,test_loader,criterion,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "보고싶다 [ 0.93049943 -0.4485399   1.4206077  -0.27747744 -1.1286433  -0.09754803\n",
      " -0.5689745  -1.3475473   0.75959384 -0.20456786 -0.66203004 -1.3312459\n",
      "  0.5068424  -0.332373    0.5859555   0.36862454 -0.4461149   0.6443309\n",
      " -0.8920234  -0.545456    1.2616271   0.3397996   1.5604184  -0.658941\n",
      "  0.68777114 -0.86183417  0.85466355 -0.32745874  1.0780392  -0.80722123\n",
      " -0.37858653 -1.8825042  -0.2599015   0.41788548 -2.049854    0.03660351\n",
      " -1.986395   -0.6441814  -0.5370713   0.30739018  2.1693003   0.8093875\n",
      " -1.8111361  -0.425444   -1.3921982   0.09051845 -1.5179828   1.8653108\n",
      " -1.2423953  -0.766455    0.5589561  -0.6365321   1.001903   -1.3328335\n",
      " -1.2263488  -0.31271574  0.13457614  0.42126116 -2.0591753   0.8467508\n",
      "  1.8132298   0.63114214  0.1685237   1.0012673   1.1539768  -0.32910252\n",
      " -0.79264176 -0.27544707 -0.25350335 -0.5939801  -1.0158545  -2.4215739\n",
      " -0.77588964  0.43241242 -2.4584782   0.43895575  0.3485663  -1.1287143\n",
      " -0.62887126 -1.1895298  -0.55301994  2.1574662   1.1629438  -3.2203703\n",
      "  0.9491572   0.19499895  1.2907153  -1.6698738   0.07090335 -0.3848469\n",
      "  0.13439251 -0.04005032 -0.2753653  -1.4135864  -1.2359635  -1.4078034\n",
      "  1.6224219   0.06775263  0.37534246 -1.4242927   0.88037336  0.16808233\n",
      "  0.54708743 -0.75297165  0.26574287 -0.04410527  1.5875683   0.7462\n",
      "  0.30860165 -0.47228923 -0.35207018 -0.1109485  -0.7066867  -0.65726936\n",
      "  2.178734    0.36326984  1.4173249  -0.2569912  -1.35789    -0.5439209\n",
      " -1.8965467   1.6704887   0.7577419  -0.37417316 -0.9738207  -1.059672\n",
      " -0.34224918 -1.1396105 ]\n"
     ]
    }
   ],
   "source": [
    "#학습된 모델로부터 임베딩 추출\n",
    "token_to_embedding=dict()\n",
    "embedding_metrix=classifier.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "for word,emb in zip(vocab,embedding_metrix):\n",
    "    token_to_embedding[word]=emb\n",
    "\n",
    "token=vocab[1000]\n",
    "print(token, token_to_embedding[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=[tokenizer.morphs(review) for review in corpus_df.text]\n",
    "word2vec=Word2Vec(sentences=tokens, vector_size=128, window=5, min_count=1, sg=1, epochs=3, max_final_vocab=10000)\n",
    "\n",
    "word2vec.save('../models/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#사전 학습된 모델로 임베딩 계층 초기화\n",
    "word2vec=Word2Vec.load('../models/word2vec.model')\n",
    "init_embeddings=np.zeros((n_vocab,embedding_dim))\n",
    "\n",
    "for index,token in id_to_token.items():\n",
    "    if token not in ['<pad>','<unk>']:\n",
    "        init_embeddings[index]=word2vec.wv[token]\n",
    "\n",
    "embedding_layer=nn.Embedding.from_pretrained(torch.tensor(init_embeddings,dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#문장 분류 모델\n",
    "\n",
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(self,n_vocab,hidden_dim,embedding_dim,n_layers,dropout=0.5,bidirectional=True,model_type='lstm',pretrained_embedding=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding=nn.Embedding(num_embeddings=n_vocab,embedding_dim=embedding_dim,padding_idx=0)\n",
    "\n",
    "        if model_type=='rnn':\n",
    "            self.model=nn.RNN(input_size=embedding_dim,hidden_size=hidden_dim,num_layers=n_layers, bidirectional=bidirectional,dropout=dropout,batch_first=True)\n",
    "\n",
    "        elif model_type=='lstm':\n",
    "            self.model=nn.LSTM(input_size=embedding_dim,hidden_size=hidden_dim,num_layers=n_layers,bidirectional=bidirectional,dropout=dropout,batch_first=True)\n",
    "\n",
    "        if bidirectional:\n",
    "            self.classifier=nn.Linear(hidden_dim*2,1)\n",
    "        else:\n",
    "            self.classifier=nn.Linear(hidden_dim,1)\n",
    "\n",
    "        if pretrained_embedding is not None:\n",
    "            self.embedding=nn.Embedding.from_pretrained(torch.tensor(pretrained_embedding,dtype=torch.float32))\n",
    "        else:\n",
    "            self.embedding=nn.Embedding(num_embeddings=n_vocab,embedding_dim=embedding_dim,padding_idx=0)\n",
    "\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        embeddings=self.embedding(inputs)\n",
    "        output,_=self.model(embeddings)\n",
    "        last_output=output[:,-1,:]\n",
    "        last_output=self.dropout(last_output)\n",
    "        logits=self.classifier(last_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainn Loss 0: 0.6932138800621033\n",
      "Trainn Loss 500: 0.6806622262367469\n",
      "Trainn Loss 1000: 0.6459312205250327\n",
      "Trainn Loss 1500: 0.6303231384379319\n",
      "Trainn Loss 2000: 0.6119799771379197\n",
      "Trainn Loss 2500: 0.5942872163177919\n",
      "Val Loss: 0.47723259041294125, Val Accuracy: 0.7694\n",
      "Trainn Loss 0: 0.5867852568626404\n",
      "Trainn Loss 500: 0.48554271587473663\n",
      "Trainn Loss 1000: 0.48511802724429537\n",
      "Trainn Loss 1500: 0.4801067741591481\n",
      "Trainn Loss 2000: 0.48137028840766555\n",
      "Trainn Loss 2500: 0.4789809270626733\n",
      "Val Loss: 0.4617430915752539, Val Accuracy: 0.7746\n",
      "Trainn Loss 0: 0.47082436084747314\n",
      "Trainn Loss 500: 0.46430438349465886\n",
      "Trainn Loss 1000: 0.4568564516383332\n",
      "Trainn Loss 1500: 0.4555144006613491\n",
      "Trainn Loss 2000: 0.45327121367876316\n",
      "Trainn Loss 2500: 0.4525008214408996\n",
      "Val Loss: 0.43470091636950214, Val Accuracy: 0.797\n",
      "Trainn Loss 0: 0.4236885905265808\n",
      "Trainn Loss 500: 0.43712030105783556\n",
      "Trainn Loss 1000: 0.4409991873504518\n",
      "Trainn Loss 1500: 0.44162708560102865\n",
      "Trainn Loss 2000: 0.43961852168229626\n",
      "Trainn Loss 2500: 0.43657584856303394\n",
      "Val Loss: 0.42780066348207646, Val Accuracy: 0.7984\n",
      "Trainn Loss 0: 0.5698575973510742\n",
      "Trainn Loss 500: 0.429299482001278\n",
      "Trainn Loss 1000: 0.42455156885839246\n",
      "Trainn Loss 1500: 0.42439726786284665\n",
      "Trainn Loss 2000: 0.42093043283990833\n",
      "Trainn Loss 2500: 0.4209800919131344\n",
      "Val Loss: 0.42947403096352904, Val Accuracy: 0.805\n"
     ]
    }
   ],
   "source": [
    "classifier=SentenceClassifier(n_vocab=n_vocab, hidden_dim=hidden_dim,embedding_dim=embedding_dim,n_layers=n_layers, pretrained_embedding=init_embeddings).to(device)\n",
    "criterion=nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer=optim.RMSprop(classifier.parameters(),lr=0.001)\n",
    "\n",
    "epochs=5\n",
    "interval=500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(classifier,train_loader,criterion,optimizer,device,interval)\n",
    "    test(classifier,test_loader,criterion,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TEXT_018_230_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
