{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 모듈 로드\n",
        "import pandas as pd\n",
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch import optim\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {},
      "outputs": [],
      "source": [
        "class reviewClassifierModel(nn.Module):\n",
        "    def __init__(self, n_vocab, hidden_dim, embedding_dim, n_classes,\n",
        "                 n_layers, dropout=0.5, bidirectional=True) -> None:\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=n_vocab,         # num_embeddings = vocab이 들어감\n",
        "            embedding_dim=embedding_dim,\n",
        "            padding_idx=0\n",
        "        )\n",
        "        self.model = nn.LSTM(\n",
        "            input_size = embedding_dim,         # Input의 사이즈에 해당하는 수\n",
        "            hidden_size=hidden_dim,             # 은닉층의 사이즈에 해당하는 수\n",
        "            num_layers=n_layers,                # RNN의 은닉층 레이어 개수, default = 1\n",
        "            bidirectional=bidirectional,        # bidrectional True일시 양방향 RNN, default = False\n",
        "            dropout=dropout,                    # dropout 비율설정 기본값 0\n",
        "            batch_first=True,                   # True일 경우 Output 사이즈는 (batch, seq, feature) 기본값 False\n",
        "        )\n",
        "        if bidirectional:\n",
        "            self.classifier1 = nn.Linear(hidden_dim*2,n_classes)\n",
        "            self.classifier2 = nn.Linear(hidden_dim*2,1)\n",
        "        else:\n",
        "            self.classifier1 = nn.Linear(hidden_dim,n_classes)\n",
        "            self.classifier2 = nn.Linear(hidden_dim,1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,inputs):\n",
        "        embeddings = self.embedding(inputs)\n",
        "        output, _ = self.model(embeddings)\n",
        "        last_output = output[:, -1, :]\n",
        "        last_output = self.dropout(last_output)\n",
        "        classesd = self.classifier1(last_output)\n",
        "        logits = self.classifier2(last_output)\n",
        "\n",
        "        # LogSoftmax 적용\n",
        "        classesd = nn.LogSoftmax(dim=1)(classesd)  # 다중 클래스 출력에 LogSoftmax 적용\n",
        "\n",
        "        return classesd, logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 데이터 로드함수\n",
        "def load_data(csvfile1,csvfile2):                                     # csv 파일 읽기\n",
        "    trainDF = pd.read_csv(csvfile1, usecols=[1, 2, 4])      # 필요한 컬럼 추출\n",
        "\n",
        "    testDF = pd.read_csv(csvfile2, usecols=[1, 2, 4])\n",
        "    return trainDF, testDF                                  # 리턴\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 데이터 인코딩 함수\n",
        "def data_encoding(DF):\n",
        "    labelCD = DF.Aspect.unique().tolist()                   # Aspect 컬럼의 유니크 값 리스트 \n",
        "    DF['Aspect'] = DF['Aspect'].map(lambda x: labelCD.index(x))         # 다중 분류 라벨링 인코딩\n",
        "    DF.loc[DF['SentimentPolarity'] == -1, 'SentimentPolarity'] = 0      # 2진 분류 인코딩\n",
        "    return DF, labelCD\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 단어사전 만드는 함수\n",
        "def build_vocab(corpus, n_vocab, special_tokens):\n",
        "    counter = Counter()                                     # Counter 인스턴스 생성\n",
        "    for tokens in corpus:                                   # 입력받은 corpus로 카운터 모델 초기화\n",
        "        counter.update(tokens)\n",
        "    vocab = special_tokens.copy()                           \n",
        "    for token, count in counter.most_common(n_vocab):       # 상위 중복 언어 단어사전에 추가\n",
        "        vocab.append(token)\n",
        "    return vocab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 패딩함수\n",
        "def pad_sequences(sequences, max_length, pad_value):\n",
        "    result = list()\n",
        "    for sequence in sequences:                              \n",
        "        sequence = sequence[:max_length]                    # max_length 만큼 자르기\n",
        "        pad_length = max_length - len(sequence)             # max_length보다 단어가 적다면\n",
        "        padded_sequence = sequence + [pad_value] * pad_length   # 정해진 수 채우기\n",
        "        result.append(padded_sequence)\n",
        "    return np.asarray(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 자연어 인코딩 함수\n",
        "def encoding_ids(token_to_id, tokens, unk_id):\n",
        "    return [\n",
        "        [token_to_id.get(token, unk_id) for token in review] for review in tokens\n",
        "    ]   # 자연어 인코딩\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 학습함수\n",
        "def model_train(model, datasets, cl_criterion, bn_criterion, optimizer, device, interval):\n",
        "    model.train()\n",
        "    losses = []\n",
        "\n",
        "    for step, (input_ids, labels) in enumerate(datasets):\n",
        "        input_ids = input_ids.to(device)                    # 인풋데이터\n",
        "        cl_labels = labels[:, 0].to(device)                 # 라벨 다중분류\n",
        "        bn_labels = labels[:, 1].to(device).float()         # 라벨 2진분류  (float형)\n",
        "\n",
        "        # Forward pass\n",
        "        classesd, logits = model(input_ids)\n",
        "\n",
        "        # Calculate losses\n",
        "        loss_cl = cl_criterion(classesd, cl_labels)         # \n",
        "        loss_bn = bn_criterion(logits.squeeze(), bn_labels) # \n",
        "        loss = loss_cl + loss_bn\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % interval == 0:\n",
        "            print(f'Train Loss {step} : {np.mean(losses)}')\n",
        "\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def model_test(model, datasets, cl_criterion, bn_criterion, device, epoch, results_df):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    cl_score = []\n",
        "    bn_score = []\n",
        "    \n",
        "    all_cl_predictions = []\n",
        "    all_cl_labels = []\n",
        "\n",
        "    with torch.no_grad(): \n",
        "        for step, (input_ids, labels) in enumerate(datasets):\n",
        "            input_ids = input_ids.to(device)\n",
        "            cl_labels = labels[:, 0].to(device).long()  # 다중 클래스 레이블\n",
        "            bn_labels = labels[:, 1].to(device).float()  # 이진 분류 레이블\n",
        "\n",
        "            classesd, logits = model(input_ids)\n",
        "\n",
        "            loss_cl = cl_criterion(classesd, cl_labels)\n",
        "            loss_bn = bn_criterion(logits.squeeze(), bn_labels)\n",
        "            loss = loss_cl + loss_bn\n",
        "            losses.append(loss.item())\n",
        "            \n",
        "            cl_predictions = torch.argmax(torch.softmax(classesd, dim=1), dim=1)  # 다중 클래스 예측\n",
        "            cl_score.extend(cl_predictions.eq(cl_labels).cpu().numpy())  # 정확도 계산\n",
        "            \n",
        "            bn_predictions = (torch.sigmoid(logits) > 0.5).int().squeeze()  # 이진 예측\n",
        "            bn_score.extend(bn_predictions.eq(bn_labels.int()).cpu().numpy())  # 정확도 계산\n",
        "            \n",
        "            #F1-score 계산\n",
        "            all_cl_predictions.extend(cl_predictions.cpu().numpy())\n",
        "            all_cl_labels.extend(cl_labels.cpu().numpy())\n",
        "\n",
        "        # F1-스코어 계산\n",
        "        f1 = f1_score(all_cl_labels, all_cl_predictions, average='micro')\n",
        "        \n",
        "        # 정확도 계산\n",
        "        cl_accuracy = np.mean(cl_score)\n",
        "        bn_accuracy = np.mean(bn_score)\n",
        "        \n",
        "        print(f'Epoch {epoch} - Val Loss: {np.mean(losses)}, bn_score Val Accuracy: {bn_accuracy}, cl_score Val Accuracy: {cl_accuracy}, F1 Score: {f1}')\n",
        "    \n",
        "    return cl_accuracy, bn_accuracy, f1,cl_score,bn_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 실행함수\n",
        "N_VOCAB = 5000\n",
        "MAX_LENGTH =15\n",
        "EPOCHS =100\n",
        "INTERVAL = 500\n",
        "BATCH_SIZE = 32\n",
        "LR = 0.001\n",
        "special_tokens = ['<pad>', '<unk>']\n",
        "\n",
        "raw_trainDF, raw_testDF = load_data('./DATA/Train_Fashion_reivew.csv','./DATA/Val_Fashion_reivew.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Aspect</th>\n",
              "      <th>SentimentText</th>\n",
              "      <th>SentimentPolarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>가격</td>\n",
              "      <td>가격이 착하고</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>디자인</td>\n",
              "      <td>디자인이 예쁩니다</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>가격</td>\n",
              "      <td>싸고</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>디자인</td>\n",
              "      <td>디자인이 예뻐요</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>가격</td>\n",
              "      <td>가성비 가심비 입니다</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120211</th>\n",
              "      <td>품질</td>\n",
              "      <td>바퀴도 크고 부드럽게 잘 움직여서</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120212</th>\n",
              "      <td>사용성/편의성</td>\n",
              "      <td>편하게 사용하겠어요.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120213</th>\n",
              "      <td>디자인</td>\n",
              "      <td>캐리어 찾을때도 비슷비슷한 모양들 많은중에 눈에 띌것같네요^^</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120214</th>\n",
              "      <td>색상</td>\n",
              "      <td>색상도 좋으네요</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120215</th>\n",
              "      <td>사용성/편의성</td>\n",
              "      <td>4종류 용도에 맞게 사용하기 편리해요</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>120216 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Aspect                       SentimentText  SentimentPolarity\n",
              "0            가격                             가격이 착하고                  1\n",
              "1           디자인                           디자인이 예쁩니다                  1\n",
              "2            가격                                  싸고                  1\n",
              "3           디자인                            디자인이 예뻐요                  1\n",
              "4            가격                         가성비 가심비 입니다                  1\n",
              "...         ...                                 ...                ...\n",
              "120211       품질                  바퀴도 크고 부드럽게 잘 움직여서                  1\n",
              "120212  사용성/편의성                         편하게 사용하겠어요.                  1\n",
              "120213      디자인  캐리어 찾을때도 비슷비슷한 모양들 많은중에 눈에 띌것같네요^^                  1\n",
              "120214       색상                            색상도 좋으네요                  1\n",
              "120215  사용성/편의성                4종류 용도에 맞게 사용하기 편리해요                  1\n",
              "\n",
              "[120216 rows x 3 columns]"
            ]
          },
          "execution_count": 249,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_trainDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Aspect\n",
              "디자인            13290\n",
              "가격             11029\n",
              "사이즈            10633\n",
              "품질              9162\n",
              "착화감             8561\n",
              "소재              8374\n",
              "기능              8331\n",
              "색상              7165\n",
              "착용감             6130\n",
              "치수/사이즈          4791\n",
              "무게              4649\n",
              "핏               4145\n",
              "길이              3299\n",
              "두께              3095\n",
              "신축성             2917\n",
              "활용성             2896\n",
              "촉감              2075\n",
              "기능성             1525\n",
              "사이즈/폭/길이/두께     1428\n",
              "제품구성            1378\n",
              "마감              1306\n",
              "사용성             1150\n",
              "내구성              911\n",
              "사용성/편의성          835\n",
              "굽                542\n",
              "냄새               448\n",
              "수납               151\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 250,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_trainDF['Aspect'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_trainDF.loc[raw_trainDF['Aspect'] == '길이', 'Aspect'] = '사이즈'\n",
        "raw_trainDF.loc[raw_trainDF['Aspect'] == '핏', 'Aspect'] = '사이즈'\n",
        "raw_trainDF.loc[raw_trainDF['Aspect'] == '무게', 'Aspect'] = '착용감'\n",
        "raw_trainDF.loc[raw_trainDF['Aspect'] == '신축성', 'Aspect'] = '착용감'\n",
        "raw_trainDF.loc[raw_trainDF['Aspect'] == '마감', 'Aspect'] = '품질'\n",
        "raw_trainDF.loc[raw_trainDF['Aspect'] == '치수/사이즈', 'Aspect'] = '사이즈'\n",
        "raw_trainDF.loc[raw_trainDF['Aspect'] == '사용성', 'Aspect'] = '활용성'\n",
        "raw_trainDF.loc[raw_trainDF['Aspect'] == '사이즈/폭/길이/두께', 'Aspect'] = '사이즈'\n",
        "raw_trainDF.loc[raw_trainDF['Aspect'] == '사용성/편의성', 'Aspect'] = '활용성'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Aspect</th>\n",
              "      <th>SentimentText</th>\n",
              "      <th>SentimentPolarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>가격</td>\n",
              "      <td>가격이 착하고</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>디자인</td>\n",
              "      <td>디자인이 예쁩니다</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>가격</td>\n",
              "      <td>싸고</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>디자인</td>\n",
              "      <td>디자인이 예뻐요</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>가격</td>\n",
              "      <td>가성비 가심비 입니다</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120211</th>\n",
              "      <td>품질</td>\n",
              "      <td>바퀴도 크고 부드럽게 잘 움직여서</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120212</th>\n",
              "      <td>활용성</td>\n",
              "      <td>편하게 사용하겠어요.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120213</th>\n",
              "      <td>디자인</td>\n",
              "      <td>캐리어 찾을때도 비슷비슷한 모양들 많은중에 눈에 띌것같네요^^</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120214</th>\n",
              "      <td>색상</td>\n",
              "      <td>색상도 좋으네요</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120215</th>\n",
              "      <td>활용성</td>\n",
              "      <td>4종류 용도에 맞게 사용하기 편리해요</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>120216 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Aspect                       SentimentText  SentimentPolarity\n",
              "0          가격                             가격이 착하고                  1\n",
              "1         디자인                           디자인이 예쁩니다                  1\n",
              "2          가격                                  싸고                  1\n",
              "3         디자인                            디자인이 예뻐요                  1\n",
              "4          가격                         가성비 가심비 입니다                  1\n",
              "...       ...                                 ...                ...\n",
              "120211     품질                  바퀴도 크고 부드럽게 잘 움직여서                  1\n",
              "120212    활용성                         편하게 사용하겠어요.                  1\n",
              "120213    디자인  캐리어 찾을때도 비슷비슷한 모양들 많은중에 눈에 띌것같네요^^                  1\n",
              "120214     색상                            색상도 좋으네요                  1\n",
              "120215    활용성                4종류 용도에 맞게 사용하기 편리해요                  1\n",
              "\n",
              "[120216 rows x 3 columns]"
            ]
          },
          "execution_count": 252,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_trainDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Aspect\n",
              "사이즈     24296\n",
              "착용감     13696\n",
              "디자인     13290\n",
              "가격      11029\n",
              "품질      10468\n",
              "착화감      8561\n",
              "소재       8374\n",
              "기능       8331\n",
              "색상       7165\n",
              "활용성      4881\n",
              "두께       3095\n",
              "촉감       2075\n",
              "기능성      1525\n",
              "제품구성     1378\n",
              "내구성       911\n",
              "굽         542\n",
              "냄새        448\n",
              "수납        151\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 253,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_trainDF['Aspect'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Aspect</th>\n",
              "      <th>SentimentText</th>\n",
              "      <th>SentimentPolarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>기능</td>\n",
              "      <td>배도 편하게 눌러주고</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>길이</td>\n",
              "      <td>미디가 길어 편하다고..</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>색상</td>\n",
              "      <td>다섯가지 색상 모두 이쁘네요^^</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>기능</td>\n",
              "      <td>입자마자 시원하고</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>기능</td>\n",
              "      <td>배 부분도 잘 잡아주고</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15755</th>\n",
              "      <td>품질</td>\n",
              "      <td>품질 좋아요</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15756</th>\n",
              "      <td>품질</td>\n",
              "      <td>우선 박음질이 허접합니다.</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15757</th>\n",
              "      <td>색상</td>\n",
              "      <td>색상은 이뻐요</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15758</th>\n",
              "      <td>품질</td>\n",
              "      <td>검정백은 지퍼가 뻑뻑해서 잘 안닫혀요</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15759</th>\n",
              "      <td>가격</td>\n",
              "      <td>50프로 적립 아니였음 반품이였음</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15760 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Aspect         SentimentText  SentimentPolarity\n",
              "0         기능          배도 편하게 눌러주고                   1\n",
              "1         길이         미디가 길어 편하다고..                  1\n",
              "2         색상     다섯가지 색상 모두 이쁘네요^^                  1\n",
              "3         기능             입자마자 시원하고                  1\n",
              "4         기능          배 부분도 잘 잡아주고                  1\n",
              "...      ...                   ...                ...\n",
              "15755     품질                품질 좋아요                  1\n",
              "15756     품질        우선 박음질이 허접합니다.                 -1\n",
              "15757     색상              색상은 이뻐요                   1\n",
              "15758     품질  검정백은 지퍼가 뻑뻑해서 잘 안닫혀요                 -1\n",
              "15759     가격    50프로 적립 아니였음 반품이였음                  1\n",
              "\n",
              "[15760 rows x 3 columns]"
            ]
          },
          "execution_count": 254,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_testDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Aspect\n",
              "사이즈            1828\n",
              "디자인            1421\n",
              "가격             1282\n",
              "품질             1213\n",
              "착용감            1211\n",
              "기능             1197\n",
              "소재              966\n",
              "착화감             867\n",
              "핏               845\n",
              "색상              840\n",
              "신축성             728\n",
              "치수/사이즈          525\n",
              "길이              494\n",
              "무게              484\n",
              "두께              418\n",
              "활용성             223\n",
              "제품구성            200\n",
              "촉감              186\n",
              "사용성/편의성         160\n",
              "사용성             159\n",
              "기능성             139\n",
              "사이즈/폭/길이/두께     125\n",
              "마감               93\n",
              "내구성              61\n",
              "수납               40\n",
              "굽                28\n",
              "냄새               27\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 255,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_testDF['Aspect'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_testDF.loc[raw_testDF['Aspect'] == '길이', 'Aspect'] = '사이즈'\n",
        "raw_testDF.loc[raw_testDF['Aspect'] == '핏', 'Aspect'] = '사이즈'\n",
        "raw_testDF.loc[raw_testDF['Aspect'] == '무게', 'Aspect'] = '착용감'\n",
        "raw_testDF.loc[raw_testDF['Aspect'] == '신축성', 'Aspect'] = '착용감'\n",
        "raw_testDF.loc[raw_testDF['Aspect'] == '마감', 'Aspect'] = '품질'\n",
        "raw_testDF.loc[raw_testDF['Aspect'] == '치수/사이즈', 'Aspect'] = '사이즈'\n",
        "raw_testDF.loc[raw_testDF['Aspect'] == '사용성', 'Aspect'] = '활용성'\n",
        "raw_testDF.loc[raw_testDF['Aspect'] == '사이즈/폭/길이/두께', 'Aspect'] = '사이즈'\n",
        "raw_testDF.loc[raw_testDF['Aspect'] == '사용성/편의성', 'Aspect'] = '활용성'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Aspect\n",
              "사이즈     3817\n",
              "착용감     2423\n",
              "디자인     1421\n",
              "품질      1306\n",
              "가격      1282\n",
              "기능      1197\n",
              "소재       966\n",
              "착화감      867\n",
              "색상       840\n",
              "활용성      542\n",
              "두께       418\n",
              "제품구성     200\n",
              "촉감       186\n",
              "기능성      139\n",
              "내구성       61\n",
              "수납        40\n",
              "굽         28\n",
              "냄새        27\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 257,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_testDF['Aspect'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {},
      "outputs": [],
      "source": [
        "top10_train=raw_trainDF['Aspect'].value_counts().nlargest(n=10).index.tolist()\n",
        "trainDF=raw_trainDF.drop(index=raw_trainDF[~raw_trainDF['Aspect'].isin(values=top10_train)].index).reset_index()\n",
        "\n",
        "top10_test=raw_testDF['Aspect'].value_counts().nlargest(n=10).index.tolist()\n",
        "testDF=raw_testDF.drop(index=raw_testDF[~raw_testDF['Aspect'].isin(values=top10_test)].index).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Aspect</th>\n",
              "      <th>SentimentText</th>\n",
              "      <th>SentimentPolarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>가격</td>\n",
              "      <td>가격이 착하고</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>디자인</td>\n",
              "      <td>디자인이 예쁩니다</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>가격</td>\n",
              "      <td>싸고</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>디자인</td>\n",
              "      <td>디자인이 예뻐요</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>가격</td>\n",
              "      <td>가성비 가심비 입니다</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110086</th>\n",
              "      <td>120211</td>\n",
              "      <td>품질</td>\n",
              "      <td>바퀴도 크고 부드럽게 잘 움직여서</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110087</th>\n",
              "      <td>120212</td>\n",
              "      <td>활용성</td>\n",
              "      <td>편하게 사용하겠어요.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110088</th>\n",
              "      <td>120213</td>\n",
              "      <td>디자인</td>\n",
              "      <td>캐리어 찾을때도 비슷비슷한 모양들 많은중에 눈에 띌것같네요^^</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110089</th>\n",
              "      <td>120214</td>\n",
              "      <td>색상</td>\n",
              "      <td>색상도 좋으네요</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110090</th>\n",
              "      <td>120215</td>\n",
              "      <td>활용성</td>\n",
              "      <td>4종류 용도에 맞게 사용하기 편리해요</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>110091 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         index Aspect                       SentimentText  SentimentPolarity\n",
              "0            0     가격                             가격이 착하고                  1\n",
              "1            1    디자인                           디자인이 예쁩니다                  1\n",
              "2            2     가격                                  싸고                  1\n",
              "3            3    디자인                            디자인이 예뻐요                  1\n",
              "4            4     가격                         가성비 가심비 입니다                  1\n",
              "...        ...    ...                                 ...                ...\n",
              "110086  120211     품질                  바퀴도 크고 부드럽게 잘 움직여서                  1\n",
              "110087  120212    활용성                         편하게 사용하겠어요.                  1\n",
              "110088  120213    디자인  캐리어 찾을때도 비슷비슷한 모양들 많은중에 눈에 띌것같네요^^                  1\n",
              "110089  120214     색상                            색상도 좋으네요                  1\n",
              "110090  120215    활용성                4종류 용도에 맞게 사용하기 편리해요                  1\n",
              "\n",
              "[110091 rows x 4 columns]"
            ]
          },
          "execution_count": 259,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainDF, aspectCD = data_encoding(trainDF)\n",
        "testDF, _ = data_encoding(testDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Aspect</th>\n",
              "      <th>SentimentText</th>\n",
              "      <th>SentimentPolarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>가격이 착하고</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>디자인이 예쁩니다</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>싸고</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>디자인이 예뻐요</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>가성비 가심비 입니다</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110086</th>\n",
              "      <td>120211</td>\n",
              "      <td>8</td>\n",
              "      <td>바퀴도 크고 부드럽게 잘 움직여서</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110087</th>\n",
              "      <td>120212</td>\n",
              "      <td>5</td>\n",
              "      <td>편하게 사용하겠어요.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110088</th>\n",
              "      <td>120213</td>\n",
              "      <td>1</td>\n",
              "      <td>캐리어 찾을때도 비슷비슷한 모양들 많은중에 눈에 띌것같네요^^</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110089</th>\n",
              "      <td>120214</td>\n",
              "      <td>6</td>\n",
              "      <td>색상도 좋으네요</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110090</th>\n",
              "      <td>120215</td>\n",
              "      <td>5</td>\n",
              "      <td>4종류 용도에 맞게 사용하기 편리해요</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>110091 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         index  Aspect                       SentimentText  SentimentPolarity\n",
              "0            0       0                             가격이 착하고                  1\n",
              "1            1       1                           디자인이 예쁩니다                  1\n",
              "2            2       0                                  싸고                  1\n",
              "3            3       1                            디자인이 예뻐요                  1\n",
              "4            4       0                         가성비 가심비 입니다                  1\n",
              "...        ...     ...                                 ...                ...\n",
              "110086  120211       8                  바퀴도 크고 부드럽게 잘 움직여서                  1\n",
              "110087  120212       5                         편하게 사용하겠어요.                  1\n",
              "110088  120213       1  캐리어 찾을때도 비슷비슷한 모양들 많은중에 눈에 띌것같네요^^                  1\n",
              "110089  120214       6                            색상도 좋으네요                  1\n",
              "110090  120215       5                4종류 용도에 맞게 사용하기 편리해요                  1\n",
              "\n",
              "[110091 rows x 4 columns]"
            ]
          },
          "execution_count": 261,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['가격', '디자인', '착용감', '기능', '소재', '활용성', '색상', '사이즈', '품질', '착화감']"
            ]
          },
          "execution_count": 262,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "aspectCD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss 0 : 3.0111489295959473\n",
            "Train Loss 500 : 2.389176887428451\n",
            "Train Loss 1000 : 2.020701436074702\n",
            "Train Loss 1500 : 1.8140389317675165\n",
            "Train Loss 2000 : 1.6781155996951742\n",
            "Train Loss 2500 : 1.5749138439001917\n",
            "Train Loss 3000 : 1.4933968568813478\n",
            "Epoch 0 - Val Loss: 5.50941648254727, bn_score Val Accuracy: 0.8650160289202646, cl_score Val Accuracy: 0.0643203055726076, F1 Score: 0.0643203055726076\n",
            "Model saved at ./saved_model/review_classifier_epoch_1.pt\n",
            "Train Loss 0 : 0.9443352222442627\n",
            "Train Loss 500 : 0.8991360986780026\n",
            "Train Loss 1000 : 0.8961583862414251\n",
            "Train Loss 1500 : 0.8797433856008531\n",
            "Train Loss 2000 : 0.8716041432476711\n",
            "Train Loss 2500 : 0.8621159505005218\n",
            "Train Loss 3000 : 0.8552485938093655\n",
            "Epoch 1 - Val Loss: 6.081223690431882, bn_score Val Accuracy: 0.8892981379169225, cl_score Val Accuracy: 0.06513880362867472, F1 Score: 0.06513880362867472\n",
            "Model saved at ./saved_model/review_classifier_epoch_2.pt\n",
            "Train Loss 0 : 0.6413862705230713\n",
            "Train Loss 500 : 0.6844003293031228\n",
            "Train Loss 1000 : 0.7004825274099956\n",
            "Train Loss 1500 : 0.7033364170694415\n",
            "Train Loss 2000 : 0.7020491850355278\n",
            "Train Loss 2500 : 0.7029271057692589\n",
            "Train Loss 3000 : 0.7011985878215874\n",
            "Epoch 2 - Val Loss: 6.274127763860366, bn_score Val Accuracy: 0.8991201145897278, cl_score Val Accuracy: 0.05845440283745993, F1 Score: 0.05845440283745993\n",
            "Model saved at ./saved_model/review_classifier_epoch_3.pt\n",
            "Train Loss 0 : 0.26872605085372925\n",
            "Train Loss 500 : 0.6094481650881187\n",
            "Train Loss 1000 : 0.6128837630733267\n",
            "Train Loss 1500 : 0.611901124384624\n",
            "Train Loss 2000 : 0.6151746023019989\n",
            "Train Loss 2500 : 0.617372409575751\n",
            "Train Loss 3000 : 0.6166013348940729\n",
            "Epoch 3 - Val Loss: 6.65652571368581, bn_score Val Accuracy: 0.9076461360070937, cl_score Val Accuracy: 0.05695382306800355, F1 Score: 0.05695382306800355\n",
            "Model saved at ./saved_model/review_classifier_epoch_4.pt\n",
            "Train Loss 0 : 0.8102405071258545\n",
            "Train Loss 500 : 0.5460505373404173\n",
            "Train Loss 1000 : 0.5574497579039632\n",
            "Train Loss 1500 : 0.5620561412409653\n",
            "Train Loss 2000 : 0.5606079248973037\n",
            "Train Loss 2500 : 0.5639794662785453\n",
            "Train Loss 3000 : 0.5660415152481936\n",
            "Epoch 4 - Val Loss: 6.717445112781068, bn_score Val Accuracy: 0.9075779278357547, cl_score Val Accuracy: 0.052247459245617626, F1 Score: 0.052247459245617626\n",
            "Model saved at ./saved_model/review_classifier_epoch_5.pt\n",
            "Train Loss 0 : 0.44288700819015503\n",
            "Train Loss 500 : 0.504703416200931\n",
            "Train Loss 1000 : 0.5094957923912978\n",
            "Train Loss 1500 : 0.5105781644523183\n",
            "Train Loss 2000 : 0.5170158877231549\n",
            "Train Loss 2500 : 0.520068105231257\n",
            "Train Loss 3000 : 0.5211937649796821\n",
            "Epoch 5 - Val Loss: 6.884041713473584, bn_score Val Accuracy: 0.9088056749198554, cl_score Val Accuracy: 0.06629834254143646, F1 Score: 0.06629834254143646\n",
            "Model saved at ./saved_model/review_classifier_epoch_6.pt\n",
            "Train Loss 0 : 0.585364818572998\n",
            "Train Loss 500 : 0.48015649298469937\n",
            "Train Loss 1000 : 0.4723100685826191\n",
            "Train Loss 1500 : 0.47815357111419227\n",
            "Train Loss 2000 : 0.47842229734817665\n",
            "Train Loss 2500 : 0.4816082915017863\n",
            "Train Loss 3000 : 0.48510226312626287\n",
            "Epoch 6 - Val Loss: 7.290519564759498, bn_score Val Accuracy: 0.9100334220039561, cl_score Val Accuracy: 0.053134165473023666, F1 Score: 0.053134165473023666\n",
            "Model saved at ./saved_model/review_classifier_epoch_7.pt\n",
            "Train Loss 0 : 0.3013342022895813\n",
            "Train Loss 500 : 0.4455966764165018\n",
            "Train Loss 1000 : 0.4451607365931545\n",
            "Train Loss 1500 : 0.45131038973662635\n",
            "Train Loss 2000 : 0.45355153125131326\n",
            "Train Loss 2500 : 0.4570040000451417\n",
            "Train Loss 3000 : 0.4571236523662119\n",
            "Epoch 7 - Val Loss: 7.117147321015402, bn_score Val Accuracy: 0.9089420912625332, cl_score Val Accuracy: 0.06336539117386263, F1 Score: 0.06336539117386263\n",
            "Model saved at ./saved_model/review_classifier_epoch_8.pt\n",
            "Train Loss 0 : 0.4635406732559204\n",
            "Train Loss 500 : 0.39584554989121395\n",
            "Train Loss 1000 : 0.40553770710032183\n",
            "Train Loss 1500 : 0.4147516002780513\n",
            "Train Loss 2000 : 0.4217840816931746\n",
            "Train Loss 2500 : 0.4283252066907669\n",
            "Train Loss 3000 : 0.42900551001391424\n",
            "Epoch 8 - Val Loss: 7.339943390266568, bn_score Val Accuracy: 0.9081918013778051, cl_score Val Accuracy: 0.0547711615851579, F1 Score: 0.0547711615851579\n",
            "Model saved at ./saved_model/review_classifier_epoch_9.pt\n",
            "Train Loss 0 : 0.32034119963645935\n",
            "Train Loss 500 : 0.37291645399943557\n",
            "Train Loss 1000 : 0.3856245812687364\n",
            "Train Loss 1500 : 0.3948714528170766\n",
            "Train Loss 2000 : 0.40166656015024726\n",
            "Train Loss 2500 : 0.40393261908174083\n",
            "Train Loss 3000 : 0.4064206078757806\n",
            "Epoch 9 - Val Loss: 7.517697897352164, bn_score Val Accuracy: 0.9060773480662984, cl_score Val Accuracy: 0.0671850487688425, F1 Score: 0.0671850487688425\n",
            "Model saved at ./saved_model/review_classifier_epoch_10.pt\n",
            "Train Loss 0 : 0.28126513957977295\n",
            "Train Loss 500 : 0.36422743464628615\n",
            "Train Loss 1000 : 0.3683078004964641\n",
            "Train Loss 1500 : 0.3738440848331027\n",
            "Train Loss 2000 : 0.37810120169540634\n",
            "Train Loss 2500 : 0.3833353032053756\n",
            "Train Loss 3000 : 0.3857741819453196\n",
            "Epoch 10 - Val Loss: 7.8591731617913005, bn_score Val Accuracy: 0.9083282177204829, cl_score Val Accuracy: 0.06084168883432235, F1 Score: 0.06084168883432235\n",
            "Model saved at ./saved_model/review_classifier_epoch_11.pt\n",
            "Train Loss 0 : 0.3391217589378357\n",
            "Train Loss 500 : 0.3395373844011815\n",
            "Train Loss 1000 : 0.34722596871016265\n",
            "Train Loss 1500 : 0.3544727297503499\n",
            "Train Loss 2000 : 0.35961647301182514\n",
            "Train Loss 2500 : 0.36456590745507217\n",
            "Train Loss 3000 : 0.3693798960052165\n",
            "Epoch 11 - Val Loss: 7.746337580005587, bn_score Val Accuracy: 0.9071686788077211, cl_score Val Accuracy: 0.06281972580315122, F1 Score: 0.06281972580315122\n",
            "Model saved at ./saved_model/review_classifier_epoch_12.pt\n",
            "Train Loss 0 : 0.26127690076828003\n",
            "Train Loss 500 : 0.339846609520936\n",
            "Train Loss 1000 : 0.3394832543649159\n",
            "Train Loss 1500 : 0.3425715442493727\n",
            "Train Loss 2000 : 0.3448684049266642\n",
            "Train Loss 2500 : 0.3475284064415751\n",
            "Train Loss 3000 : 0.35221248187412724\n",
            "Epoch 12 - Val Loss: 8.278690773417487, bn_score Val Accuracy: 0.9048496009821977, cl_score Val Accuracy: 0.0557942841552418, F1 Score: 0.0557942841552418\n",
            "Model saved at ./saved_model/review_classifier_epoch_13.pt\n",
            "Train Loss 0 : 0.6420941352844238\n",
            "Train Loss 500 : 0.31472149238496006\n",
            "Train Loss 1000 : 0.31794339089796736\n",
            "Train Loss 1500 : 0.32370990902671887\n",
            "Train Loss 2000 : 0.32714760650334684\n",
            "Train Loss 2500 : 0.3319674694626892\n",
            "Train Loss 3000 : 0.3350084101218297\n",
            "Epoch 13 - Val Loss: 8.001913893456553, bn_score Val Accuracy: 0.9053952663529091, cl_score Val Accuracy: 0.0620012277470841, F1 Score: 0.0620012277470841\n",
            "Model saved at ./saved_model/review_classifier_epoch_14.pt\n",
            "Train Loss 0 : 0.2212805449962616\n",
            "Train Loss 500 : 0.3038802072628886\n",
            "Train Loss 1000 : 0.3118481405793906\n",
            "Train Loss 1500 : 0.3189720579874091\n",
            "Train Loss 2000 : 0.32016951286479245\n",
            "Train Loss 2500 : 0.32171911982724855\n",
            "Train Loss 3000 : 0.32293887393253023\n",
            "Epoch 14 - Val Loss: 8.374857869283307, bn_score Val Accuracy: 0.9063501807516541, cl_score Val Accuracy: 0.05674919855398677, F1 Score: 0.05674919855398677\n",
            "Model saved at ./saved_model/review_classifier_epoch_15.pt\n",
            "Train Loss 0 : 0.11492523550987244\n",
            "Train Loss 500 : 0.2939212354059943\n",
            "Train Loss 1000 : 0.30061660917637706\n",
            "Train Loss 1500 : 0.30422008053947497\n",
            "Train Loss 2000 : 0.30896773218349\n",
            "Train Loss 2500 : 0.3125369150038125\n",
            "Train Loss 3000 : 0.3149017831636484\n",
            "Epoch 15 - Val Loss: 8.468600951508499, bn_score Val Accuracy: 0.90089352704454, cl_score Val Accuracy: 0.056476365868631064, F1 Score: 0.056476365868631064\n",
            "Model saved at ./saved_model/review_classifier_epoch_16.pt\n",
            "Train Loss 0 : 0.24774500727653503\n",
            "Train Loss 500 : 0.27895430673263266\n",
            "Train Loss 1000 : 0.2835329329514956\n",
            "Train Loss 1500 : 0.2901850327809678\n",
            "Train Loss 2000 : 0.29495450548980306\n",
            "Train Loss 2500 : 0.29806964943518355\n",
            "Train Loss 3000 : 0.3005188609517165\n",
            "Epoch 16 - Val Loss: 8.199907505434323, bn_score Val Accuracy: 0.9051906418388923, cl_score Val Accuracy: 0.06513880362867472, F1 Score: 0.06513880362867472\n",
            "Model saved at ./saved_model/review_classifier_epoch_17.pt\n",
            "Train Loss 0 : 0.3789774477481842\n",
            "Train Loss 500 : 0.2845365401922526\n",
            "Train Loss 1000 : 0.2797294017932304\n",
            "Train Loss 1500 : 0.28177408813104915\n",
            "Train Loss 2000 : 0.2849341109906999\n",
            "Train Loss 2500 : 0.2865622958554918\n",
            "Train Loss 3000 : 0.2902784994147005\n",
            "Epoch 17 - Val Loss: 8.221333776943565, bn_score Val Accuracy: 0.9060773480662984, cl_score Val Accuracy: 0.06166018689038947, F1 Score: 0.06166018689038947\n",
            "Model saved at ./saved_model/review_classifier_epoch_18.pt\n",
            "Train Loss 0 : 0.13412857055664062\n",
            "Train Loss 500 : 0.26671870449509566\n",
            "Train Loss 1000 : 0.26740819357659346\n",
            "Train Loss 1500 : 0.2706885423720985\n",
            "Train Loss 2000 : 0.2744019771495584\n",
            "Train Loss 2500 : 0.2780827303278874\n",
            "Train Loss 3000 : 0.2805709690622109\n",
            "Epoch 18 - Val Loss: 8.536581456011936, bn_score Val Accuracy: 0.9046449764681809, cl_score Val Accuracy: 0.06766250596821499, F1 Score: 0.06766250596821499\n",
            "Model saved at ./saved_model/review_classifier_epoch_19.pt\n",
            "Train Loss 0 : 0.32600635290145874\n",
            "Train Loss 500 : 0.25141447148994056\n",
            "Train Loss 1000 : 0.2550411008179307\n",
            "Train Loss 1500 : 0.2583220420848283\n",
            "Train Loss 2000 : 0.26310519923617515\n",
            "Train Loss 2500 : 0.2653622043947252\n",
            "Train Loss 3000 : 0.2700896867635011\n",
            "Epoch 19 - Val Loss: 8.956620935261379, bn_score Val Accuracy: 0.9043039356114863, cl_score Val Accuracy: 0.06104631334833913, F1 Score: 0.06104631334833913\n",
            "Model saved at ./saved_model/review_classifier_epoch_20.pt\n",
            "Train Loss 0 : 0.20190128684043884\n",
            "Train Loss 500 : 0.2516708941033441\n",
            "Train Loss 1000 : 0.25630086474492775\n",
            "Train Loss 1500 : 0.25923888752054547\n",
            "Train Loss 2000 : 0.26309189523269105\n",
            "Train Loss 2500 : 0.2655486073326327\n",
            "Train Loss 3000 : 0.2695081764931447\n",
            "Epoch 20 - Val Loss: 9.150703172538275, bn_score Val Accuracy: 0.9026669394993521, cl_score Val Accuracy: 0.06397926471591296, F1 Score: 0.06397926471591296\n",
            "Model saved at ./saved_model/review_classifier_epoch_21.pt\n",
            "Train Loss 0 : 0.5253710746765137\n",
            "Train Loss 500 : 0.23762799471155732\n",
            "Train Loss 1000 : 0.24293084092788048\n",
            "Train Loss 1500 : 0.24855787322287518\n",
            "Train Loss 2000 : 0.2518892831614484\n",
            "Train Loss 2500 : 0.2531622214125889\n",
            "Train Loss 3000 : 0.25722008627758386\n",
            "Epoch 21 - Val Loss: 8.920641060748132, bn_score Val Accuracy: 0.9043039356114863, cl_score Val Accuracy: 0.06704863242616466, F1 Score: 0.06704863242616466\n",
            "Model saved at ./saved_model/review_classifier_epoch_22.pt\n",
            "Train Loss 0 : 0.3682021200656891\n",
            "Train Loss 500 : 0.23904630884587647\n",
            "Train Loss 1000 : 0.24308020616700124\n",
            "Train Loss 1500 : 0.24771697255475653\n",
            "Train Loss 2000 : 0.2500720657501323\n",
            "Train Loss 2500 : 0.2562959904267472\n",
            "Train Loss 3000 : 0.25699029259460643\n",
            "Epoch 22 - Val Loss: 9.176817235344116, bn_score Val Accuracy: 0.9050542254962144, cl_score Val Accuracy: 0.0581133619807653, F1 Score: 0.0581133619807653\n",
            "Model saved at ./saved_model/review_classifier_epoch_23.pt\n",
            "Train Loss 0 : 0.08805146813392639\n",
            "Train Loss 500 : 0.23527902167684542\n",
            "Train Loss 1000 : 0.24120507135682112\n",
            "Train Loss 1500 : 0.24458572254215258\n",
            "Train Loss 2000 : 0.24340711381083455\n",
            "Train Loss 2500 : 0.24739332358222135\n",
            "Train Loss 3000 : 0.24911933173498424\n",
            "Epoch 23 - Val Loss: 8.960059176343199, bn_score Val Accuracy: 0.903621853898097, cl_score Val Accuracy: 0.06063706432030557, F1 Score: 0.06063706432030557\n",
            "Model saved at ./saved_model/review_classifier_epoch_24.pt\n",
            "Train Loss 0 : 0.24706970155239105\n",
            "Train Loss 500 : 0.22374179231713037\n",
            "Train Loss 1000 : 0.23193702153437265\n",
            "Train Loss 1500 : 0.23558640408315396\n",
            "Train Loss 2000 : 0.23570311450627984\n",
            "Train Loss 2500 : 0.2383703333349889\n",
            "Train Loss 3000 : 0.24098283103805299\n",
            "Epoch 24 - Val Loss: 9.064723702557465, bn_score Val Accuracy: 0.9053952663529091, cl_score Val Accuracy: 0.06582088534206398, F1 Score: 0.06582088534206398\n",
            "Model saved at ./saved_model/review_classifier_epoch_25.pt\n",
            "Train Loss 0 : 0.21049413084983826\n",
            "Train Loss 500 : 0.2197687250363107\n",
            "Train Loss 1000 : 0.22456723943652285\n",
            "Train Loss 1500 : 0.23466738373652934\n",
            "Train Loss 2000 : 0.23803675301798408\n",
            "Train Loss 2500 : 0.2379350818163741\n",
            "Train Loss 3000 : 0.24043187568329152\n",
            "Epoch 25 - Val Loss: 9.328444277279258, bn_score Val Accuracy: 0.9019848577859627, cl_score Val Accuracy: 0.0684810040242821, F1 Score: 0.0684810040242821\n",
            "Model saved at ./saved_model/review_classifier_epoch_26.pt\n",
            "Train Loss 0 : 0.0673891231417656\n",
            "Train Loss 500 : 0.22490100774118404\n",
            "Train Loss 1000 : 0.21973556841534753\n",
            "Train Loss 1500 : 0.2267672369972636\n",
            "Train Loss 2000 : 0.23072663137874475\n",
            "Train Loss 2500 : 0.2327577764319568\n",
            "Train Loss 3000 : 0.23537215907145284\n",
            "Epoch 26 - Val Loss: 9.139778290958446, bn_score Val Accuracy: 0.9041675192688085, cl_score Val Accuracy: 0.06179660323306732, F1 Score: 0.06179660323306732\n",
            "Model saved at ./saved_model/review_classifier_epoch_27.pt\n",
            "Train Loss 0 : 0.14266744256019592\n",
            "Train Loss 500 : 0.20384469483709503\n",
            "Train Loss 1000 : 0.21681553823826882\n",
            "Train Loss 1500 : 0.22136114606463775\n",
            "Train Loss 2000 : 0.22672725271532262\n",
            "Train Loss 2500 : 0.2301498532990833\n",
            "Train Loss 3000 : 0.23220414479822665\n",
            "Epoch 27 - Val Loss: 9.256507155682266, bn_score Val Accuracy: 0.9018484414432849, cl_score Val Accuracy: 0.06343359934520156, F1 Score: 0.06343359934520156\n",
            "Model saved at ./saved_model/review_classifier_epoch_28.pt\n",
            "Train Loss 0 : 0.4072243571281433\n",
            "Train Loss 500 : 0.210380491627726\n",
            "Train Loss 1000 : 0.218841549256569\n",
            "Train Loss 1500 : 0.22339764298085646\n",
            "Train Loss 2000 : 0.2244231495494279\n",
            "Train Loss 2500 : 0.22740189445728973\n",
            "Train Loss 3000 : 0.22879788780293836\n",
            "Epoch 28 - Val Loss: 9.421778325681332, bn_score Val Accuracy: 0.898915490075711, cl_score Val Accuracy: 0.059068276379510266, F1 Score: 0.059068276379510266\n",
            "Model saved at ./saved_model/review_classifier_epoch_29.pt\n",
            "Train Loss 0 : 0.39882320165634155\n",
            "Train Loss 500 : 0.2139170970265826\n",
            "Train Loss 1000 : 0.21381353224152244\n",
            "Train Loss 1500 : 0.21557119813282974\n",
            "Train Loss 2000 : 0.22041104276427653\n",
            "Train Loss 2500 : 0.22371959910079722\n",
            "Train Loss 3000 : 0.22567091659096253\n",
            "Epoch 29 - Val Loss: 9.474691730698728, bn_score Val Accuracy: 0.9020530659573017, cl_score Val Accuracy: 0.06657117522679216, F1 Score: 0.06657117522679216\n",
            "Model saved at ./saved_model/review_classifier_epoch_30.pt\n",
            "Train Loss 0 : 0.43622079491615295\n",
            "Train Loss 500 : 0.20213681926947094\n",
            "Train Loss 1000 : 0.21228102940999038\n",
            "Train Loss 1500 : 0.21599298054854982\n",
            "Train Loss 2000 : 0.2173493729805623\n",
            "Train Loss 2500 : 0.22119402363062238\n",
            "Train Loss 3000 : 0.22262567749525639\n",
            "Epoch 30 - Val Loss: 9.420634536701609, bn_score Val Accuracy: 0.9044403519541641, cl_score Val Accuracy: 0.05852261100879885, F1 Score: 0.05852261100879885\n",
            "Model saved at ./saved_model/review_classifier_epoch_31.pt\n",
            "Train Loss 0 : 0.07829200476408005\n",
            "Train Loss 500 : 0.19749970163428734\n",
            "Train Loss 1000 : 0.2008789664145548\n",
            "Train Loss 1500 : 0.21050955115976946\n",
            "Train Loss 2000 : 0.21287076326430604\n",
            "Train Loss 2500 : 0.21637563039844737\n",
            "Train Loss 3000 : 0.22031214881951522\n",
            "Epoch 31 - Val Loss: 9.685017192026109, bn_score Val Accuracy: 0.9016438169292681, cl_score Val Accuracy: 0.06391105654457405, F1 Score: 0.06391105654457405\n",
            "Model saved at ./saved_model/review_classifier_epoch_32.pt\n",
            "Train Loss 0 : 0.2477106750011444\n",
            "Train Loss 500 : 0.20397037357285233\n",
            "Train Loss 1000 : 0.205791232897179\n",
            "Train Loss 1500 : 0.21198273563557749\n",
            "Train Loss 2000 : 0.2132121849300592\n",
            "Train Loss 2500 : 0.21502996011540704\n",
            "Train Loss 3000 : 0.2167551862342815\n",
            "Epoch 32 - Val Loss: 9.55562010829485, bn_score Val Accuracy: 0.900757110701862, cl_score Val Accuracy: 0.06629834254143646, F1 Score: 0.06629834254143646\n",
            "Model saved at ./saved_model/review_classifier_epoch_33.pt\n",
            "Train Loss 0 : 0.1676044464111328\n",
            "Train Loss 500 : 0.19496946773322638\n",
            "Train Loss 1000 : 0.198873760249395\n",
            "Train Loss 1500 : 0.20295695222102647\n",
            "Train Loss 2000 : 0.20846636201497512\n",
            "Train Loss 2500 : 0.2105577908078732\n",
            "Train Loss 3000 : 0.21397606530237628\n",
            "Epoch 33 - Val Loss: 9.48483366955859, bn_score Val Accuracy: 0.9034854375554191, cl_score Val Accuracy: 0.05954573357888275, F1 Score: 0.05954573357888275\n",
            "Model saved at ./saved_model/review_classifier_epoch_34.pt\n",
            "Train Loss 0 : 0.16647344827651978\n",
            "Train Loss 500 : 0.18946229168650752\n",
            "Train Loss 1000 : 0.1992517288976147\n",
            "Train Loss 1500 : 0.2029618444948083\n",
            "Train Loss 2000 : 0.20763912971346998\n",
            "Train Loss 2500 : 0.20990713216161064\n",
            "Train Loss 3000 : 0.21093725763377386\n",
            "Epoch 34 - Val Loss: 9.425220124861774, bn_score Val Accuracy: 0.9032808130414024, cl_score Val Accuracy: 0.05900006820817134, F1 Score: 0.05900006820817134\n",
            "Model saved at ./saved_model/review_classifier_epoch_35.pt\n",
            "Train Loss 0 : 0.17841166257858276\n",
            "Train Loss 500 : 0.2072084417153975\n",
            "Train Loss 1000 : 0.20209565220008066\n",
            "Train Loss 1500 : 0.20284570180261874\n",
            "Train Loss 2000 : 0.2058908422112182\n",
            "Train Loss 2500 : 0.2102092898032609\n",
            "Train Loss 3000 : 0.21262471998214066\n",
            "Epoch 35 - Val Loss: 9.790818376478805, bn_score Val Accuracy: 0.9032126048700634, cl_score Val Accuracy: 0.05975035809289953, F1 Score: 0.05975035809289953\n",
            "Model saved at ./saved_model/review_classifier_epoch_36.pt\n",
            "Train Loss 0 : 0.21620789170265198\n",
            "Train Loss 500 : 0.19881739235388304\n",
            "Train Loss 1000 : 0.20715629288575033\n",
            "Train Loss 1500 : 0.2071994309680292\n",
            "Train Loss 2000 : 0.2072010907641929\n",
            "Train Loss 2500 : 0.2079190665622149\n",
            "Train Loss 3000 : 0.20794336555723655\n",
            "Epoch 36 - Val Loss: 9.825812497689574, bn_score Val Accuracy: 0.9024623149853352, cl_score Val Accuracy: 0.06527521997135256, F1 Score: 0.06527521997135256\n",
            "Model saved at ./saved_model/review_classifier_epoch_37.pt\n",
            "Train Loss 0 : 0.12649783492088318\n",
            "Train Loss 500 : 0.19314653206125884\n",
            "Train Loss 1000 : 0.19796604730773132\n",
            "Train Loss 1500 : 0.19965356952144078\n",
            "Train Loss 2000 : 0.19886387742428424\n",
            "Train Loss 2500 : 0.2014417233577118\n",
            "Train Loss 3000 : 0.2042116999650366\n",
            "Epoch 37 - Val Loss: 9.930113495305212, bn_score Val Accuracy: 0.9042357274401474, cl_score Val Accuracy: 0.058727235522815634, F1 Score: 0.058727235522815634\n",
            "Model saved at ./saved_model/review_classifier_epoch_38.pt\n",
            "Train Loss 0 : 0.057652972638607025\n",
            "Train Loss 500 : 0.1943053127974718\n",
            "Train Loss 1000 : 0.19197227992582602\n",
            "Train Loss 1500 : 0.19711284673301818\n",
            "Train Loss 2000 : 0.1998318113002187\n",
            "Train Loss 2500 : 0.2023131079820604\n",
            "Train Loss 3000 : 0.2053756375019077\n",
            "Epoch 38 - Val Loss: 9.933704027163436, bn_score Val Accuracy: 0.9031443966987245, cl_score Val Accuracy: 0.06745788145419822, F1 Score: 0.06745788145419822\n",
            "Model saved at ./saved_model/review_classifier_epoch_39.pt\n",
            "Train Loss 0 : 0.3900812566280365\n",
            "Train Loss 500 : 0.18682965715443658\n",
            "Train Loss 1000 : 0.18834037531063064\n",
            "Train Loss 1500 : 0.19004039451179605\n",
            "Train Loss 2000 : 0.19408168306091111\n",
            "Train Loss 2500 : 0.19799021846789425\n",
            "Train Loss 3000 : 0.20161854831569495\n",
            "Epoch 39 - Val Loss: 9.402243476028277, bn_score Val Accuracy: 0.9035536457267581, cl_score Val Accuracy: 0.06513880362867472, F1 Score: 0.06513880362867472\n",
            "Model saved at ./saved_model/review_classifier_epoch_40.pt\n",
            "Train Loss 0 : 0.1405719518661499\n",
            "Train Loss 500 : 0.19583497738870914\n",
            "Train Loss 1000 : 0.1894032205720167\n",
            "Train Loss 1500 : 0.19236713337918607\n",
            "Train Loss 2000 : 0.19633590051162428\n",
            "Train Loss 2500 : 0.1997324229839568\n",
            "Train Loss 3000 : 0.20170602932949735\n",
            "Epoch 40 - Val Loss: 9.507993996273198, bn_score Val Accuracy: 0.900825318873201, cl_score Val Accuracy: 0.058931860036832415, F1 Score: 0.058931860036832415\n",
            "Model saved at ./saved_model/review_classifier_epoch_41.pt\n",
            "Train Loss 0 : 0.20741106569766998\n",
            "Train Loss 500 : 0.1780369207746314\n",
            "Train Loss 1000 : 0.18869989767175663\n",
            "Train Loss 1500 : 0.19414829328869637\n",
            "Train Loss 2000 : 0.19463713274852582\n",
            "Train Loss 2500 : 0.19547047714901264\n",
            "Train Loss 3000 : 0.19941388673724106\n",
            "Epoch 41 - Val Loss: 9.858154218960431, bn_score Val Accuracy: 0.9051906418388923, cl_score Val Accuracy: 0.06029602346361094, F1 Score: 0.060296023463610945\n",
            "Model saved at ./saved_model/review_classifier_epoch_42.pt\n",
            "Train Loss 0 : 0.18958233296871185\n",
            "Train Loss 500 : 0.18013325513353545\n",
            "Train Loss 1000 : 0.18445591698982172\n",
            "Train Loss 1500 : 0.18692746634491478\n",
            "Train Loss 2000 : 0.190528545480013\n",
            "Train Loss 2500 : 0.19460225498351436\n",
            "Train Loss 3000 : 0.19568227322912418\n",
            "Epoch 42 - Val Loss: 10.30351423022534, bn_score Val Accuracy: 0.9043721437828252, cl_score Val Accuracy: 0.06595730168474183, F1 Score: 0.06595730168474183\n",
            "Model saved at ./saved_model/review_classifier_epoch_43.pt\n",
            "Train Loss 0 : 0.10883210599422455\n",
            "Train Loss 500 : 0.18691636821414956\n",
            "Train Loss 1000 : 0.1867190260062372\n",
            "Train Loss 1500 : 0.18845629588025142\n",
            "Train Loss 2000 : 0.19155144309466784\n",
            "Train Loss 2500 : 0.19332709346135638\n",
            "Train Loss 3000 : 0.1966011210829479\n",
            "Epoch 43 - Val Loss: 9.722230370008349, bn_score Val Accuracy: 0.9021894822999795, cl_score Val Accuracy: 0.06929950208034923, F1 Score: 0.06929950208034923\n",
            "Model saved at ./saved_model/review_classifier_epoch_44.pt\n",
            "Train Loss 0 : 0.18345947563648224\n",
            "Train Loss 500 : 0.18868988264396697\n",
            "Train Loss 1000 : 0.18721988813655746\n",
            "Train Loss 1500 : 0.18745182526932605\n",
            "Train Loss 2000 : 0.1912720903111012\n",
            "Train Loss 2500 : 0.19172764777048368\n",
            "Train Loss 3000 : 0.1928538928718934\n",
            "Epoch 44 - Val Loss: 10.173155943552652, bn_score Val Accuracy: 0.9025305231566741, cl_score Val Accuracy: 0.06220585226110088, F1 Score: 0.06220585226110088\n",
            "Model saved at ./saved_model/review_classifier_epoch_45.pt\n",
            "Train Loss 0 : 0.10712650418281555\n",
            "Train Loss 500 : 0.1778891883454027\n",
            "Train Loss 1000 : 0.17250424905491252\n",
            "Train Loss 1500 : 0.17956856068355254\n",
            "Train Loss 2000 : 0.18351449532274236\n",
            "Train Loss 2500 : 0.18649836615236645\n",
            "Train Loss 3000 : 0.19141050468108026\n",
            "Epoch 45 - Val Loss: 9.9948088078717, bn_score Val Accuracy: 0.9037582702407748, cl_score Val Accuracy: 0.06445672191528545, F1 Score: 0.06445672191528545\n",
            "Model saved at ./saved_model/review_classifier_epoch_46.pt\n",
            "Train Loss 0 : 0.048176057636737823\n",
            "Train Loss 500 : 0.18306584967980663\n",
            "Train Loss 1000 : 0.18001759340698187\n",
            "Train Loss 1500 : 0.183620088601573\n",
            "Train Loss 2000 : 0.1890056477282485\n",
            "Train Loss 2500 : 0.18972459557342952\n",
            "Train Loss 3000 : 0.19331307917763813\n",
            "Epoch 46 - Val Loss: 9.726064135565997, bn_score Val Accuracy: 0.9017120251006071, cl_score Val Accuracy: 0.05599890866925858, F1 Score: 0.05599890866925858\n",
            "Model saved at ./saved_model/review_classifier_epoch_47.pt\n",
            "Train Loss 0 : 0.2894887924194336\n",
            "Train Loss 500 : 0.18430192727119682\n",
            "Train Loss 1000 : 0.18549937067774835\n",
            "Train Loss 1500 : 0.18804551581253387\n",
            "Train Loss 2000 : 0.1875056945932922\n",
            "Train Loss 2500 : 0.1919868841511486\n",
            "Train Loss 3000 : 0.19417916429212012\n",
            "Epoch 47 - Val Loss: 9.840121646332586, bn_score Val Accuracy: 0.9028715640133688, cl_score Val Accuracy: 0.0544983288998022, F1 Score: 0.0544983288998022\n",
            "Model saved at ./saved_model/review_classifier_epoch_48.pt\n",
            "Train Loss 0 : 0.1743539273738861\n",
            "Train Loss 500 : 0.18101985733807385\n",
            "Train Loss 1000 : 0.18205901525903503\n",
            "Train Loss 1500 : 0.18405758730910823\n",
            "Train Loss 2000 : 0.18544670952477496\n",
            "Train Loss 2500 : 0.18744960535423416\n",
            "Train Loss 3000 : 0.1889143837885841\n",
            "Epoch 48 - Val Loss: 10.094710479894236, bn_score Val Accuracy: 0.9023941068139963, cl_score Val Accuracy: 0.06616192619875862, F1 Score: 0.06616192619875862\n",
            "Model saved at ./saved_model/review_classifier_epoch_49.pt\n",
            "Train Loss 0 : 0.14670972526073456\n",
            "Train Loss 500 : 0.17606349609338653\n",
            "Train Loss 1000 : 0.17750881442225183\n",
            "Train Loss 1500 : 0.18182075686807397\n",
            "Train Loss 2000 : 0.18289150695234113\n",
            "Train Loss 2500 : 0.18686908624358015\n",
            "Train Loss 3000 : 0.18678822039813672\n",
            "Epoch 49 - Val Loss: 10.228241433245424, bn_score Val Accuracy: 0.9034172293840802, cl_score Val Accuracy: 0.05879544369415456, F1 Score: 0.05879544369415456\n",
            "Model saved at ./saved_model/review_classifier_epoch_50.pt\n",
            "Train Loss 0 : 0.09693043678998947\n",
            "Train Loss 500 : 0.17724880351203526\n",
            "Train Loss 1000 : 0.1824690784019726\n",
            "Train Loss 1500 : 0.18100570131224802\n",
            "Train Loss 2000 : 0.18409109370795074\n",
            "Train Loss 2500 : 0.18799988547193316\n",
            "Train Loss 3000 : 0.19015644680800506\n",
            "Epoch 50 - Val Loss: 10.132225849010327, bn_score Val Accuracy: 0.9001432371598117, cl_score Val Accuracy: 0.0645931382579633, F1 Score: 0.0645931382579633\n",
            "Model saved at ./saved_model/review_classifier_epoch_51.pt\n",
            "Train Loss 0 : 0.022293150424957275\n",
            "Train Loss 500 : 0.1610122559045603\n",
            "Train Loss 1000 : 0.1746523110977524\n",
            "Train Loss 1500 : 0.1780998706408118\n",
            "Train Loss 2000 : 0.18143602902663575\n",
            "Train Loss 2500 : 0.18401021656801092\n",
            "Train Loss 3000 : 0.1865653964648658\n",
            "Epoch 51 - Val Loss: 10.353776783205586, bn_score Val Accuracy: 0.9037582702407748, cl_score Val Accuracy: 0.06623013437009753, F1 Score: 0.06623013437009753\n",
            "Model saved at ./saved_model/review_classifier_epoch_52.pt\n",
            "Train Loss 0 : 0.38500210642814636\n",
            "Train Loss 500 : 0.176379191618554\n",
            "Train Loss 1000 : 0.1815767344608382\n",
            "Train Loss 1500 : 0.1831302597186839\n",
            "Train Loss 2000 : 0.1870374526337035\n",
            "Train Loss 2500 : 0.1872307234255672\n",
            "Train Loss 3000 : 0.185626787648675\n",
            "Epoch 52 - Val Loss: 10.414680944288998, bn_score Val Accuracy: 0.9023258986426574, cl_score Val Accuracy: 0.06500238728599686, F1 Score: 0.06500238728599686\n",
            "Model saved at ./saved_model/review_classifier_epoch_53.pt\n",
            "Train Loss 0 : 0.2772364318370819\n",
            "Train Loss 500 : 0.1752066127658485\n",
            "Train Loss 1000 : 0.1822676652471175\n",
            "Train Loss 1500 : 0.18130390004482588\n",
            "Train Loss 2000 : 0.18015956484909537\n",
            "Train Loss 2500 : 0.18394732228243818\n",
            "Train Loss 3000 : 0.1859889140132888\n",
            "Epoch 53 - Val Loss: 10.318904806066442, bn_score Val Accuracy: 0.902735147670691, cl_score Val Accuracy: 0.06704863242616466, F1 Score: 0.06704863242616466\n",
            "Model saved at ./saved_model/review_classifier_epoch_54.pt\n",
            "Train Loss 0 : 0.2752764821052551\n",
            "Train Loss 500 : 0.1668412727508285\n",
            "Train Loss 1000 : 0.17004692438128335\n",
            "Train Loss 1500 : 0.17441645002819048\n",
            "Train Loss 2000 : 0.17871334257166552\n",
            "Train Loss 2500 : 0.18206848746414384\n",
            "Train Loss 3000 : 0.18429970618300773\n",
            "Epoch 54 - Val Loss: 10.161354108573564, bn_score Val Accuracy: 0.9019848577859627, cl_score Val Accuracy: 0.06084168883432235, F1 Score: 0.06084168883432235\n",
            "Model saved at ./saved_model/review_classifier_epoch_55.pt\n",
            "Train Loss 0 : 0.23707818984985352\n",
            "Train Loss 500 : 0.17121529333877142\n",
            "Train Loss 1000 : 0.1725673477889246\n",
            "Train Loss 1500 : 0.17286655792146227\n",
            "Train Loss 2000 : 0.17825798215209573\n",
            "Train Loss 2500 : 0.18142880235643374\n",
            "Train Loss 3000 : 0.1842138491004894\n",
            "Epoch 55 - Val Loss: 10.024796482784296, bn_score Val Accuracy: 0.9038264784121137, cl_score Val Accuracy: 0.06050064797762772, F1 Score: 0.06050064797762772\n",
            "Model saved at ./saved_model/review_classifier_epoch_56.pt\n",
            "Train Loss 0 : 0.13035884499549866\n",
            "Train Loss 500 : 0.16463283174011314\n",
            "Train Loss 1000 : 0.1732145187763179\n",
            "Train Loss 1500 : 0.1757756634081759\n",
            "Train Loss 2000 : 0.1775310352881686\n",
            "Train Loss 2500 : 0.18175454896113954\n",
            "Train Loss 3000 : 0.18401951590858298\n",
            "Epoch 56 - Val Loss: 10.488113440719305, bn_score Val Accuracy: 0.9004160698451674, cl_score Val Accuracy: 0.06077348066298342, F1 Score: 0.06077348066298342\n",
            "Model saved at ./saved_model/review_classifier_epoch_57.pt\n",
            "Train Loss 0 : 0.03216051310300827\n",
            "Train Loss 500 : 0.15959703760452107\n",
            "Train Loss 1000 : 0.16475579516076438\n",
            "Train Loss 1500 : 0.17063514834398938\n",
            "Train Loss 2000 : 0.17244652040541983\n",
            "Train Loss 2500 : 0.1765795549255286\n",
            "Train Loss 3000 : 0.1798053789787337\n",
            "Epoch 57 - Val Loss: 10.297156994638879, bn_score Val Accuracy: 0.9002796535024896, cl_score Val Accuracy: 0.058659027351476704, F1 Score: 0.058659027351476704\n",
            "Model saved at ./saved_model/review_classifier_epoch_58.pt\n",
            "Train Loss 0 : 0.035483840852975845\n",
            "Train Loss 500 : 0.16871793235851054\n",
            "Train Loss 1000 : 0.17079106902379868\n",
            "Train Loss 1500 : 0.17456968282371124\n",
            "Train Loss 2000 : 0.1800062088519223\n",
            "Train Loss 2500 : 0.18216601917792002\n",
            "Train Loss 3000 : 0.18362850536831574\n",
            "Epoch 58 - Val Loss: 10.4977602273031, bn_score Val Accuracy: 0.9028033558420299, cl_score Val Accuracy: 0.062137644089761954, F1 Score: 0.062137644089761954\n",
            "Model saved at ./saved_model/review_classifier_epoch_59.pt\n",
            "Train Loss 0 : 0.08351778984069824\n",
            "Train Loss 500 : 0.16582620923152702\n",
            "Train Loss 1000 : 0.16757734392776147\n",
            "Train Loss 1500 : 0.17538296565510267\n",
            "Train Loss 2000 : 0.17666152591720738\n",
            "Train Loss 2500 : 0.1790664239459988\n",
            "Train Loss 3000 : 0.1817408824917978\n",
            "Epoch 59 - Val Loss: 10.194036220932839, bn_score Val Accuracy: 0.8955732896801036, cl_score Val Accuracy: 0.06643475888411432, F1 Score: 0.06643475888411432\n",
            "Model saved at ./saved_model/review_classifier_epoch_60.pt\n",
            "Train Loss 0 : 0.24803417921066284\n",
            "Train Loss 500 : 0.16034166356585847\n",
            "Train Loss 1000 : 0.16257386362425738\n",
            "Train Loss 1500 : 0.16689060964552732\n",
            "Train Loss 2000 : 0.17054703014589537\n",
            "Train Loss 2500 : 0.1752675321690071\n",
            "Train Loss 3000 : 0.17887242663212888\n",
            "Epoch 60 - Val Loss: 10.390825965565535, bn_score Val Accuracy: 0.9015756087579292, cl_score Val Accuracy: 0.06404747288725189, F1 Score: 0.06404747288725189\n",
            "Model saved at ./saved_model/review_classifier_epoch_61.pt\n",
            "Train Loss 0 : 0.08010352402925491\n",
            "Train Loss 500 : 0.16561913611125684\n",
            "Train Loss 1000 : 0.16971143615617232\n",
            "Train Loss 1500 : 0.17302354978090978\n",
            "Train Loss 2000 : 0.17436161046537726\n",
            "Train Loss 2500 : 0.17686680728187565\n",
            "Train Loss 3000 : 0.17921979328222085\n",
            "Epoch 61 - Val Loss: 10.193845095457855, bn_score Val Accuracy: 0.9019166496146238, cl_score Val Accuracy: 0.06868562853829889, F1 Score: 0.06868562853829889\n",
            "Model saved at ./saved_model/review_classifier_epoch_62.pt\n",
            "Train Loss 0 : 0.20569661259651184\n",
            "Train Loss 500 : 0.1609866377914291\n",
            "Train Loss 1000 : 0.17093924279106068\n",
            "Train Loss 1500 : 0.17569289521204381\n",
            "Train Loss 2000 : 0.17630978698946018\n",
            "Train Loss 2500 : 0.17891353534635862\n",
            "Train Loss 3000 : 0.180773125916424\n",
            "Epoch 62 - Val Loss: 10.354485404777112, bn_score Val Accuracy: 0.9040993110974694, cl_score Val Accuracy: 0.06909487756633245, F1 Score: 0.06909487756633245\n",
            "Model saved at ./saved_model/review_classifier_epoch_63.pt\n",
            "Train Loss 0 : 0.2245102971792221\n",
            "Train Loss 500 : 0.1656303503578473\n",
            "Train Loss 1000 : 0.16558291482716822\n",
            "Train Loss 1500 : 0.16790553999690588\n",
            "Train Loss 2000 : 0.17239621428966892\n",
            "Train Loss 2500 : 0.17372914605721107\n",
            "Train Loss 3000 : 0.17772757998817862\n",
            "Epoch 63 - Val Loss: 10.283386190992035, bn_score Val Accuracy: 0.899870404474456, cl_score Val Accuracy: 0.07004979196507742, F1 Score: 0.07004979196507742\n",
            "Model saved at ./saved_model/review_classifier_epoch_64.pt\n",
            "Train Loss 0 : 0.192080557346344\n",
            "Train Loss 500 : 0.1680267222129002\n",
            "Train Loss 1000 : 0.17104433094440671\n",
            "Train Loss 1500 : 0.17398211534918695\n",
            "Train Loss 2000 : 0.17678820520526786\n",
            "Train Loss 2500 : 0.17855161019931973\n",
            "Train Loss 3000 : 0.17915597946145032\n",
            "Epoch 64 - Val Loss: 10.553458453783023, bn_score Val Accuracy: 0.9003478616738285, cl_score Val Accuracy: 0.0666393833981311, F1 Score: 0.0666393833981311\n",
            "Model saved at ./saved_model/review_classifier_epoch_65.pt\n",
            "Train Loss 0 : 0.021293241530656815\n",
            "Train Loss 500 : 0.1671973094753103\n",
            "Train Loss 1000 : 0.1674544416292646\n",
            "Train Loss 1500 : 0.1719816335610956\n",
            "Train Loss 2000 : 0.1743931322079757\n",
            "Train Loss 2500 : 0.17626057507716766\n",
            "Train Loss 3000 : 0.177750057392803\n",
            "Epoch 65 - Val Loss: 10.885757062949386, bn_score Val Accuracy: 0.90089352704454, cl_score Val Accuracy: 0.060091398949594164, F1 Score: 0.060091398949594164\n",
            "Model saved at ./saved_model/review_classifier_epoch_66.pt\n",
            "Train Loss 0 : 0.31406474113464355\n",
            "Train Loss 500 : 0.15357923616251784\n",
            "Train Loss 1000 : 0.16056561526553062\n",
            "Train Loss 1500 : 0.16525628858898642\n",
            "Train Loss 2000 : 0.1691077942414249\n",
            "Train Loss 2500 : 0.17246168897478287\n",
            "Train Loss 3000 : 0.17463834362527525\n",
            "Epoch 66 - Val Loss: 10.427419647931533, bn_score Val Accuracy: 0.9000750289884728, cl_score Val Accuracy: 0.0666393833981311, F1 Score: 0.0666393833981311\n",
            "Model saved at ./saved_model/review_classifier_epoch_67.pt\n",
            "Train Loss 0 : 0.09866280853748322\n",
            "Train Loss 500 : 0.16343887515195057\n",
            "Train Loss 1000 : 0.16899590982546608\n",
            "Train Loss 1500 : 0.17407805947022267\n",
            "Train Loss 2000 : 0.1742256827818147\n",
            "Train Loss 2500 : 0.17427225100447755\n",
            "Train Loss 3000 : 0.17643817245677207\n",
            "Epoch 67 - Val Loss: 10.07438249525681, bn_score Val Accuracy: 0.9029397721847078, cl_score Val Accuracy: 0.06391105654457405, F1 Score: 0.06391105654457405\n",
            "Model saved at ./saved_model/review_classifier_epoch_68.pt\n",
            "Train Loss 0 : 0.17226222157478333\n",
            "Train Loss 500 : 0.15280273395726185\n",
            "Train Loss 1000 : 0.1645499255459804\n",
            "Train Loss 1500 : 0.1634871629746178\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[263], line 71\u001b[0m\n\u001b[0;32m     68\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVal Loss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbn_score Val Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcl_score Val Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m---> 71\u001b[0m     \u001b[43mmodel_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcl_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbn_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mINTERVAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m     model_test(classifier, test_loader, cl_criterion, bn_criterion, device, epoch, results_df)  \u001b[38;5;66;03m# DataFrame 전달\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;66;03m# 모델 저장 (에포크 번호 추가)\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[246], line 21\u001b[0m, in \u001b[0;36mmodel_train\u001b[1;34m(model, datasets, cl_criterion, bn_criterion, optimizer, device, interval)\u001b[0m\n\u001b[0;32m     18\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\envs\\TEXT_018_230_38\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\envs\\TEXT_018_230_38\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\envs\\TEXT_018_230_38\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 토큰화 및 불용어 처리 ------------------------------------------------------------------------------------------------------------\n",
        "punc=string.punctuation\n",
        "\n",
        "for p in punc:\n",
        "    trainDF['SentimentText'] = trainDF['SentimentText'].str.replace(p, '')\n",
        "    testDF['SentimentText']=testDF['SentimentText'].str.replace(p,'')\n",
        "\n",
        "m=re.compile('[^ ㄱ-ㅣ가-힣]+')\n",
        "\n",
        "trainDF['SentimentText']=trainDF['SentimentText'].apply(lambda x: m.sub(' ', x))\n",
        "testDF['SentimentText']=testDF['SentimentText'].apply(lambda x: m.sub(' ', x))\n",
        "\n",
        "stop_word='./stopwords.txt'\n",
        "\n",
        "with open(stop_word, 'r', encoding='utf-8') as f:\n",
        "    stop_words = [line.strip() for line in f]\n",
        "\n",
        "\n",
        "tokenizer = Okt()\n",
        "train_tokens = [[token for token in tokenizer.morphs(text) if token not in stop_words] for text in trainDF['SentimentText']]\n",
        "test_tokens = [[token for token in tokenizer.morphs(text) if token not in stop_words] for text in testDF['SentimentText']]\n",
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "vocab = build_vocab(train_tokens, N_VOCAB, special_tokens)\n",
        "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
        "id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
        "\n",
        "pad_id = token_to_id['<pad>']\n",
        "unk_id = token_to_id['<unk>']\n",
        "train_ids = encoding_ids(token_to_id, train_tokens, unk_id)\n",
        "test_ids = encoding_ids(token_to_id, test_tokens, unk_id)\n",
        "train_ids = pad_sequences(train_ids, MAX_LENGTH, pad_id)\n",
        "test_ids = pad_sequences(test_ids, MAX_LENGTH, pad_id)\n",
        "\n",
        "# 텐서화\n",
        "train_ids = torch.tensor(train_ids, dtype=torch.long)\n",
        "test_ids = torch.tensor(test_ids, dtype=torch.long)\n",
        "\n",
        "# 레이블 텐서화\n",
        "train_labels = torch.tensor(list(zip(trainDF['Aspect'].values, trainDF['SentimentPolarity'].values)), dtype=torch.long)\n",
        "test_labels = torch.tensor(list(zip(testDF['Aspect'].values, testDF['SentimentPolarity'].values)), dtype=torch.float32)\n",
        "\n",
        "# 데이터셋 생성\n",
        "train_dataset = TensorDataset(train_ids, train_labels)\n",
        "test_dataset = TensorDataset(test_ids, test_labels)\n",
        "\n",
        "# 데이터 로더 생성\n",
        "train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# 모델 초기화\n",
        "n_vocab = len(token_to_id)  # 어휘 크기 계산\n",
        "hidden_dim = 64 \n",
        "embedding_dim = 128\n",
        "n_layers = 2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "classifier = reviewClassifierModel(\n",
        "    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim, n_classes=len(aspectCD), n_layers=n_layers\n",
        ").to(device)\n",
        "\n",
        "# 손실 함수 및 최적화기 설정\n",
        "cl_criterion = nn.NLLLoss().to(device)\n",
        "bn_criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "optimizer = torch.optim.AdamW(classifier.parameters(), lr=LR, weight_decay=0.01)\n",
        "\n",
        "# 결과를 저장할 DataFrame 생성\n",
        "results_df = pd.DataFrame(columns=['Val Loss', 'bn_score Val Accuracy', 'cl_score Val Accuracy'])\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model_train(classifier, train_loader, cl_criterion, bn_criterion, optimizer, device, INTERVAL)\n",
        "    model_test(classifier, test_loader, cl_criterion, bn_criterion, device, epoch, results_df)  # DataFrame 전달\n",
        "\n",
        "    # 모델 저장 (에포크 번호 추가)\n",
        "    model_save_path = f'./saved_model/review_classifier_epoch_{epoch + 1}.pt'  # 에포크 번호 포함\n",
        "    torch.save(classifier.state_dict(), model_save_path)\n",
        "    print(f'Model saved at {model_save_path}')\n",
        "\n",
        "# 결과 DataFrame 저장\n",
        "results_df.to_csv('./saved_model/evaluation_results.csv', index=True)\n",
        "print(\"평가 결과가 저장되었습니다.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# 단어 사전 저장\n",
        "with open('vocab.pkl', 'wb') as f:\n",
        "    pickle.dump(token_to_id, f)\n",
        "\n",
        "print(\"단어 사전이 저장되었습니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['윈하', '는', '색', '은', '없지만']\n",
            "tensor([[0.0043, 0.0855, 0.2323, 0.6315, 0.0043, 0.0016, 0.0016, 0.0016, 0.0016,\n",
            "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
            "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
            "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016]])\n",
            "Predicted label: 3\n"
          ]
        }
      ],
      "source": [
        "def predict(model, text, tokenizer, num_classes):\n",
        "    model.eval()\n",
        "    \n",
        "    # 1. 입력 텍스트를 토큰화 (Okt 사용)\n",
        "    tokens = tokenizer.morphs(text)\n",
        "    print(tokens)\n",
        "    \n",
        "    vocab = build_vocab(tokens, 5000, ['<pad>', '<unk>'])\n",
        "    token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
        "    id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
        "\n",
        "    # 2. 토큰을 ID로 변환 (예: token_to_id)\n",
        "    input_ids = [token_to_id.get(token, token_to_id[\"<unk>\"]) for token in tokens]\n",
        "    \n",
        "    # 3. 입력 길이에 맞게 패딩\n",
        "    max_length = 32  # 원하는 최대 길이\n",
        "    pad_id = token_to_id[\"<pad>\"]\n",
        "    input_ids = input_ids[:max_length] + [pad_id] * (max_length - len(input_ids))\n",
        "    \n",
        "    # 4. 입력 텐서 생성\n",
        "    input_tensor = torch.tensor([input_ids], dtype=torch.float32)  # 배치 차원 추가 및 float로 변환\n",
        "    \n",
        "    # 5. 모델에 입력하여 예측\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_tensor)\n",
        "    \n",
        "    # 6. 소프트맥스 함수로 확률 계산\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    print(probs)\n",
        "    \n",
        "    # 7. 최대 확률을 가진 클래스를 예측\n",
        "    prediction = torch.argmax(probs, dim=1)\n",
        "\n",
        "    # 8. 예측 결과 출력\n",
        "    print(f\"Predicted label: {prediction.item()}\")\n",
        "\n",
        "# 입력 예시\n",
        "num_classes = 9  # 예시: 분류할 클래스 수\n",
        "predict(reviewClassifierModel, \"한여름에 더울듯\", tokenizer, num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#학습 결과 확인(학습과 검증의 Loss, 성능지표 변화 확인) w. 시각화\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fg,axes=plt.subplots(1,2,figsize=(10,5))\n",
        "axes[0].plot(range(1,len(model_train.losses)),model_train.losses,label='Train')\n",
        "axes[0].plot(range(1,len(model_test.losses)),model_test.losses,label='Test')\n",
        "axes[0].set_title('LOSS')\n",
        "axes[0].grid()\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].plot(range(1,len(model_test.bn_score)),model_test.bn_score,label='Binary')\n",
        "axes[1].plot(range(1,len(model_test.cl_score)),model_test.cl_score,label='Multi')\n",
        "axes[1].plot(range(1,len(model_test.f1)),model_test.f1,label='F1')\n",
        "axes[1].set_title('SCORE')\n",
        "axes[1].grid()\n",
        "axes[1].legend()\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "TEXT_018_230_38",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
