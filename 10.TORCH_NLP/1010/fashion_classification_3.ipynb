{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch import optim\n",
        "from torch import nn\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import string\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing folder: TL_쇼핑몰_01.패션_1-1.여성의류\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(1).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(10).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(100).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(101).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(102).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(103).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(104).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(105).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(106).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(107).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(108).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(109).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(11).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(110).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(111).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(112).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(113).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(114).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(115).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(116).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(117).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(118).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(119).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(12).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(120).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(121).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(122).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(123).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(124).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(125).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(126).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(127).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(128).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(129).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(13).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(130).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(131).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(132).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(133).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(134).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(135).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(136).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(137).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(138).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(139).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(14).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(140).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(141).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(142).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(143).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(144).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(145).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(146).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(147).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(148).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(149).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(15).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(150).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(151).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(152).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(153).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(154).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(155).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(16).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(17).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(18).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(19).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(2).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(20).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(21).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(22).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(23).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(24).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(25).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(26).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(27).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(28).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(29).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(3).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(30).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(31).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(32).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(33).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(34).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(35).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(36).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(37).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(38).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(39).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(4).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(40).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(41).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(42).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(43).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(44).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(45).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(46).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(47).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(48).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(49).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(5).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(50).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(51).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(52).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(53).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(54).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(55).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(56).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(57).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(58).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(59).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(6).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(60).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(61).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(62).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(63).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(64).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(65).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(66).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(67).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(68).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(69).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(7).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(70).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(71).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(72).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(73).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(74).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(75).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(76).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(77).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(78).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(79).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(8).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(80).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(81).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(82).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(83).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(84).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(85).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(86).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(87).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(88).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(89).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(9).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(90).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(91).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(92).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(93).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(94).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(95).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(96).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(97).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(98).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(99).json\n",
            "Processing folder: TL_쇼핑몰_01.패션_1-2.남성의류\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(1).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(10).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(11).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(12).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(13).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(14).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(15).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(16).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(17).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(18).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(19).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(2).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(20).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(21).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(22).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(23).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(24).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(25).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(26).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(27).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(28).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(29).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(3).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(30).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(31).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(32).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(33).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(34).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(35).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(36).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(37).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(38).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(39).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(4).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(40).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(41).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(42).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(43).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(44).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(45).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(46).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(47).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(48).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(49).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(5).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(50).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(51).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(52).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(53).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(54).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(55).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(56).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(57).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(58).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(59).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(6).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(60).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(61).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(62).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(63).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(64).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(65).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(66).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(67).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(68).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(69).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(7).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(70).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(71).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(72).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(73).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(74).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(75).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(76).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(77).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(78).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(79).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(8).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(80).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(9).json\n",
            "Processing folder: TL_쇼핑몰_01.패션_1-3.패션슈즈\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(1).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(10).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(11).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(12).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(13).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(14).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(15).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(16).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(17).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(18).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(19).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(2).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(20).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(21).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(22).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(23).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(24).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(25).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(26).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(27).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(28).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(29).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(3).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(30).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(31).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(32).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(33).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(34).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(35).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(36).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(37).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(38).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(39).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(4).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(40).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(41).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(42).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(43).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(44).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(45).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(46).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(47).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(48).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(49).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(5).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(50).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(51).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(52).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(53).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(54).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(55).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(56).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(57).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(58).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(59).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(6).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(60).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(61).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(62).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(63).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(64).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(65).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(66).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(67).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(68).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(69).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(7).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(70).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(71).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(72).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(73).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(74).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(75).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(76).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(77).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(78).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(79).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(8).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(80).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(81).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(82).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(83).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(84).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(85).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(86).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(87).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(88).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(89).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(9).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(90).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(91).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(92).json\n",
            "Processing folder: TL_쇼핑몰_01.패션_1-4.잡화\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(1).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(10).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(11).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(12).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(13).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(14).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(15).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(16).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(17).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(18).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(19).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(2).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(20).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(21).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(22).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(23).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(24).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(25).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(26).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(27).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(28).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(29).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(3).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(30).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(31).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(4).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(5).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(6).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(7).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(8).json\n",
            "Loading file: ./DATA/Training/TL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(9).json\n",
            "         Aspect                       SentimentText SentimentWord  \\\n",
            "0            가격                             가격이 착하고             2   \n",
            "1           디자인                           디자인이 예쁩니다             2   \n",
            "2            가격                                  싸고             1   \n",
            "3           디자인                            디자인이 예뻐요             2   \n",
            "4            가격                         가성비 가심비 입니다             3   \n",
            "...         ...                                 ...           ...   \n",
            "120211       품질                  바퀴도 크고 부드럽게 잘 움직여서             5   \n",
            "120212  사용성/편의성                         편하게 사용하겠어요.             2   \n",
            "120213      디자인  캐리어 찾을때도 비슷비슷한 모양들 많은중에 눈에 띌것같네요^^             7   \n",
            "120214       색상                            색상도 좋으네요             2   \n",
            "120215  사용성/편의성                4종류 용도에 맞게 사용하기 편리해요             5   \n",
            "\n",
            "       SentimentPolarity  \n",
            "0                      1  \n",
            "1                      1  \n",
            "2                      1  \n",
            "3                      1  \n",
            "4                      1  \n",
            "...                  ...  \n",
            "120211                 1  \n",
            "120212                 1  \n",
            "120213                 1  \n",
            "120214                 1  \n",
            "120215                 1  \n",
            "\n",
            "[120216 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "TRAIN_PATH = './DATA/Training/'\n",
        "# 여러 폴더 경로를 리스트로 저장\n",
        "folder_paths = os.listdir(TRAIN_PATH)\n",
        "\n",
        "# 빈 데이터프레임 리스트 생성\n",
        "dataframes = []\n",
        "\n",
        "# 각 폴더 내의 JSON 파일을 읽어와 데이터프레임으로 변환\n",
        "for folder_path in folder_paths:\n",
        "    FOLDER_PATH = TRAIN_PATH+folder_path\n",
        "    print(f\"Processing folder: {folder_path}\")\n",
        "    \n",
        "    # 폴더 내의 모든 JSON 파일 리스트\n",
        "    json_files = [file for file in os.listdir(FOLDER_PATH) if file.endswith('.json')]\n",
        "\n",
        "    for file in json_files:\n",
        "        file_path = os.path.join(FOLDER_PATH, file)\n",
        "        print(f\"Loading file: {file_path}\")\n",
        "        \n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "                \n",
        "                # 파일 내용 확인 및 데이터프레임으로 변환\n",
        "                if data:\n",
        "                    # Aspects만 추출\n",
        "                    for review in data:\n",
        "                        aspects = pd.json_normalize(review.get('Aspects'))\n",
        "                        dataframes.append(aspects)\n",
        "                else:\n",
        "                    print(f\"No data found in {file}\")\n",
        "                    \n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Error loading {file}: Invalid JSON\")\n",
        "\n",
        "# 데이터프레임 결합\n",
        "if dataframes:\n",
        "    final_dataframe = pd.concat(dataframes, ignore_index=True)\n",
        "    print(final_dataframe)\n",
        "else:\n",
        "    print(\"No valid dataframes to concatenate.\")\n",
        "final_dataframe.to_csv('./DATA/Train_Fashion_reivew.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing folder: VL_쇼핑몰_01.패션_1-1.여성의류\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(156).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(157).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(158).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(159).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(160).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(161).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(162).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(163).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(164).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(165).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(166).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(167).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(168).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(169).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(170).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(171).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(172).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(173).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(174).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-1.여성의류\\1-1.여성의류(175).json\n",
            "Processing folder: VL_쇼핑몰_01.패션_1-2.남성의류\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(81).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(82).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(83).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(84).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(85).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(86).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(87).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(88).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(89).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-2.남성의류\\1-2.남성의류(90).json\n",
            "Processing folder: VL_쇼핑몰_01.패션_1-3.패션슈즈\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(100).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(101).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(102).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(103).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(104).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(93).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(94).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(95).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(96).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(97).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(98).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-3.패션슈즈\\1-3.패션슈즈(99).json\n",
            "Processing folder: VL_쇼핑몰_01.패션_1-4.잡화\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(32).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(33).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(34).json\n",
            "Loading file: ./DATA/Validation/VL_쇼핑몰_01.패션_1-4.잡화\\1-4.잡화(35).json\n",
            "      Aspect         SentimentText SentimentWord SentimentPolarity\n",
            "0         기능          배도 편하게 눌러주고              3                 1\n",
            "1         길이         미디가 길어 편하다고..             3                 1\n",
            "2         색상     다섯가지 색상 모두 이쁘네요^^             4                 1\n",
            "3         기능             입자마자 시원하고             2                 1\n",
            "4         기능          배 부분도 잘 잡아주고             4                 1\n",
            "...      ...                   ...           ...               ...\n",
            "15755     품질                품질 좋아요             2                 1\n",
            "15756     품질        우선 박음질이 허접합니다.             3                -1\n",
            "15757     색상              색상은 이뻐요              2                 1\n",
            "15758     품질  검정백은 지퍼가 뻑뻑해서 잘 안닫혀요             5                -1\n",
            "15759     가격    50프로 적립 아니였음 반품이였음             4                 1\n",
            "\n",
            "[15760 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "Val_PATH = './DATA/Validation/'\n",
        "# 여러 폴더 경로를 리스트로 저장\n",
        "folder_paths = os.listdir(Val_PATH)\n",
        "\n",
        "# 빈 데이터프레임 리스트 생성\n",
        "dataframes = []\n",
        "\n",
        "# 각 폴더 내의 JSON 파일을 읽어와 데이터프레임으로 변환\n",
        "for folder_path in folder_paths:\n",
        "    FOLDER_PATH = Val_PATH+folder_path\n",
        "    print(f\"Processing folder: {folder_path}\")\n",
        "    \n",
        "    # 폴더 내의 모든 JSON 파일 리스트\n",
        "    json_files = [file for file in os.listdir(FOLDER_PATH) if file.endswith('.json')]\n",
        "\n",
        "    for file in json_files:\n",
        "        file_path = os.path.join(FOLDER_PATH, file)\n",
        "        print(f\"Loading file: {file_path}\")\n",
        "        \n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "                \n",
        "                # 파일 내용 확인 및 데이터프레임으로 변환\n",
        "                if data:\n",
        "                    # Aspects만 추출\n",
        "                    for review in data:\n",
        "                        aspects = pd.json_normalize(review.get('Aspects'))\n",
        "                        dataframes.append(aspects)\n",
        "                else:\n",
        "                    print(f\"No data found in {file}\")\n",
        "                    \n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Error loading {file}: Invalid JSON\")\n",
        "\n",
        "# 데이터프레임 결합\n",
        "if dataframes:\n",
        "    final_dataframe = pd.concat(dataframes, ignore_index=True)\n",
        "    print(final_dataframe)\n",
        "else:\n",
        "    print(\"No valid dataframes to concatenate.\")\n",
        "final_dataframe.to_csv('./DATA/Val_Fashion_reivew.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "class reviewClassifierModel(nn.Module):\n",
        "    def __init__(self, n_vocab, hidden_dim, embedding_dim, n_classes,\n",
        "                 n_layers, dropout=0.5, bidirectional=True) -> None:\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=n_vocab,         # num_embeddings = vocab이 들어감\n",
        "            embedding_dim=embedding_dim,\n",
        "            padding_idx=0\n",
        "        )\n",
        "        self.model = nn.LSTM(\n",
        "            input_size = embedding_dim,         # Input의 사이즈에 해당하는 수\n",
        "            hidden_size=hidden_dim,             # 은닉층의 사이즈에 해당하는 수\n",
        "            num_layers=n_layers,                # RNN의 은닉층 레이어 개수, default = 1\n",
        "            bidirectional=bidirectional,        # bidrectional True일시 양방향 RNN, default = False\n",
        "            dropout=dropout,                    # dropout 비율설정 기본값 0\n",
        "            batch_first=True,                   # True일 경우 Output 사이즈는 (batch, seq, feature) 기본값 False\n",
        "        )\n",
        "        if bidirectional:\n",
        "            self.classifier1 = nn.Linear(hidden_dim*2,n_classes)\n",
        "            self.classifier2 = nn.Linear(hidden_dim*2,1)\n",
        "        else:\n",
        "            self.classifier1 = nn.Linear(hidden_dim,n_classes)\n",
        "            self.classifier2 = nn.Linear(hidden_dim,1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,inputs):\n",
        "        embeddings = self.embedding(inputs)\n",
        "        output, _ = self.model(embeddings)\n",
        "        last_output = output[:, -1, :]\n",
        "        last_output = self.dropout(last_output)\n",
        "        classesd = self.classifier1(last_output)\n",
        "        logits = self.classifier2(last_output)\n",
        "        return classesd, logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # 데이터 로드함수\n",
        "# def load_data(csvfile):                                     # csv 파일 읽기\n",
        "#     reviewDF = pd.read_csv(csvfile, usecols=[1, 2, 4])      # 필요한 컬럼 추출\n",
        "#     trainDF = reviewDF.groupby(by='Aspect').apply(func=lambda x: x.sample(frac=0.8)).reset_index(drop=True)     # train ,test 분리\n",
        "#     testDF = reviewDF.drop(index=trainDF.index)\n",
        "#     return trainDF, testDF                                  # 리턴"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 데이터 인코딩 함수\n",
        "def data_encoding(DF):\n",
        "    labelCD = DF.Aspect.unique().tolist()                   # Aspect 컬럼의 유니크 값 리스트 \n",
        "    DF['Aspect'] = DF['Aspect'].map(lambda x: labelCD.index(x))         # 다중 분류 라벨링 인코딩\n",
        "    DF.loc[DF['SentimentPolarity'] == -1, 'SentimentPolarity'] = 0      # 2진 분류 인코딩\n",
        "    return DF, labelCD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 단어사전 만드는 함수\n",
        "def build_vocab(corpus, n_vocab, special_tokens):\n",
        "    counter = Counter()                                     # Counter 인스턴스 생성\n",
        "    for tokens in corpus:                                   # 입력받은 corpus로 카운터 모델 초기화\n",
        "        counter.update(tokens)\n",
        "    vocab = special_tokens.copy()                           \n",
        "    for token, count in counter.most_common(n_vocab):       # 상위 중복 언어 단어사전에 추가\n",
        "        vocab.append(token)\n",
        "    return vocab\n",
        "\n",
        "# 패딩함수\n",
        "def pad_sequences(sequences, max_length, pad_value):\n",
        "    result = list()\n",
        "    for sequence in sequences:                              \n",
        "        sequence = sequence[:max_length]                    # max_length 만큼 자르기\n",
        "        pad_length = max_length - len(sequence)             # max_length보다 단어가 적다면\n",
        "        padded_sequence = sequence + [pad_value] * pad_length   # 정해진 수 채우기\n",
        "        result.append(padded_sequence)\n",
        "    return np.asarray(result)\n",
        "\n",
        "# 자연어 인코딩 함수\n",
        "def encoding_ids(token_to_id, tokens, unk_id):\n",
        "    return [\n",
        "        [token_to_id.get(token, unk_id) for token in review] for review in tokens\n",
        "    ]   # 자연어 인코딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 학습함수\n",
        "def model_train(model, datasets, cl_criterion, bn_criterion, optimizer, device, interval):\n",
        "    model.train()\n",
        "    losses = []\n",
        "\n",
        "    for step, (input_ids, labels) in enumerate(datasets):\n",
        "        input_ids = input_ids.to(device)                    # 인풋데이터\n",
        "        cl_labels = labels[:, 0].to(device)                 # 라벨 다중분류\n",
        "        bn_labels = labels[:, 1].to(device).float()         # 라벨 2진분류  (float형)\n",
        "\n",
        "        # Forward pass\n",
        "        classesd, logits = model(input_ids)\n",
        "\n",
        "        # Calculate losses\n",
        "        loss_cl = cl_criterion(classesd, cl_labels)         # \n",
        "        loss_bn = bn_criterion(logits.squeeze(), bn_labels) # \n",
        "        loss = loss_cl + loss_bn\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % interval == 0:\n",
        "            print(f'Train Loss {step} : {np.mean(losses)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model_test(model, datasets, cl_criterion, bn_criterion, device):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    cl_score = []\n",
        "    bn_score = []\n",
        "\n",
        "    with torch.no_grad(): \n",
        "        for step, (input_ids, labels) in enumerate(datasets):\n",
        "            input_ids = input_ids.to(device)\n",
        "            cl_labels = labels[:, 0].to(device).long()\n",
        "            bn_labels = labels[:, 1].to(device).float() \n",
        "\n",
        "            # Forward pass\n",
        "            classesd, logits = model(input_ids)\n",
        "\n",
        "            # Calculate losses\n",
        "            loss_cl = cl_criterion(classesd, cl_labels)\n",
        "            loss_bn = bn_criterion(logits.squeeze(), bn_labels)\n",
        "            loss = loss_cl + loss_bn\n",
        "            losses.append(loss.item())\n",
        "            \n",
        "            # Calculate class accuracy\n",
        "            cl_predictions = torch.argmax(torch.softmax(classesd, dim=1), dim=1)  # 다중 클래스 예측\n",
        "            cl_score.extend(cl_predictions.eq(cl_labels).cpu().numpy())  # 정확도 계산\n",
        "            \n",
        "            # Calculate binary accuracy\n",
        "            bn_predictions = (torch.sigmoid(logits) > 0.5).int().squeeze()  # 이진 예측\n",
        "            bn_score.extend(bn_predictions.eq(bn_labels.int()).cpu().numpy())  # 정확도 계산\n",
        "        \n",
        "        # 정확도 계산\n",
        "        cl_accuracy = np.mean(cl_score)\n",
        "        bn_accuracy = np.mean(bn_score)\n",
        "        \n",
        "        print(f'Val Loss : {np.mean(losses)} , bn_score Val Accuracy : {bn_accuracy}, cl_score Val Accuracy : {cl_accuracy}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/100\n",
            "Train Loss 0 : 2.8981058597564697\n",
            "Train Loss 500 : 1.8992831238253627\n",
            "Train Loss 1000 : 1.5923539423918749\n",
            "Val Loss : 5.434236116970287 , bn_score Val Accuracy : 0.830932594644506, cl_score Val Accuracy : 0.06925207756232687\n",
            "Model saved at ./saved_model/review_classifier_epoch_1.pt\n",
            "2/100\n",
            "Train Loss 0 : 1.2398641109466553\n",
            "Train Loss 500 : 0.9614807129382136\n",
            "Train Loss 1000 : 0.9201367545973409\n",
            "Val Loss : 6.070040384460898 , bn_score Val Accuracy : 0.8713758079409049, cl_score Val Accuracy : 0.07128347183748845\n",
            "Model saved at ./saved_model/review_classifier_epoch_2.pt\n",
            "3/100\n",
            "Train Loss 0 : 0.6521004438400269\n",
            "Train Loss 500 : 0.7454994058537626\n",
            "Train Loss 1000 : 0.7380362789590399\n",
            "Val Loss : 6.275078641667085 , bn_score Val Accuracy : 0.8865189289012003, cl_score Val Accuracy : 0.0713758079409049\n",
            "Model saved at ./saved_model/review_classifier_epoch_3.pt\n",
            "4/100\n",
            "Train Loss 0 : 0.627572774887085\n",
            "Train Loss 500 : 0.6385126297107476\n",
            "Train Loss 1000 : 0.6359573106367986\n",
            "Val Loss : 6.328625833286958 , bn_score Val Accuracy : 0.8987072945521699, cl_score Val Accuracy : 0.08097876269621422\n",
            "Model saved at ./saved_model/review_classifier_epoch_4.pt\n",
            "5/100\n",
            "Train Loss 0 : 0.38232049345970154\n",
            "Train Loss 500 : 0.5561432880377342\n",
            "Train Loss 1000 : 0.5646276226440271\n",
            "Val Loss : 6.911016551186057 , bn_score Val Accuracy : 0.8993536472760849, cl_score Val Accuracy : 0.07396121883656509\n",
            "Model saved at ./saved_model/review_classifier_epoch_5.pt\n",
            "6/100\n",
            "Train Loss 0 : 0.4416311979293823\n",
            "Train Loss 500 : 0.515778471163647\n",
            "Train Loss 1000 : 0.5181319468713307\n",
            "Val Loss : 7.282127526227166 , bn_score Val Accuracy : 0.8983379501385041, cl_score Val Accuracy : 0.06703601108033241\n",
            "Model saved at ./saved_model/review_classifier_epoch_6.pt\n",
            "7/100\n",
            "Train Loss 0 : 0.49610525369644165\n",
            "Train Loss 500 : 0.46321596029751794\n",
            "Train Loss 1000 : 0.47720107061045985\n",
            "Val Loss : 7.45948146090788 , bn_score Val Accuracy : 0.9044321329639889, cl_score Val Accuracy : 0.07571560480147738\n",
            "Model saved at ./saved_model/review_classifier_epoch_7.pt\n",
            "8/100\n",
            "Train Loss 0 : 0.35268276929855347\n",
            "Train Loss 500 : 0.43432054444106516\n",
            "Train Loss 1000 : 0.44524493198711557\n",
            "Val Loss : 7.570520140143001 , bn_score Val Accuracy : 0.9060941828254848, cl_score Val Accuracy : 0.07285318559556787\n",
            "Model saved at ./saved_model/review_classifier_epoch_8.pt\n",
            "9/100\n",
            "Train Loss 0 : 0.3819774389266968\n",
            "Train Loss 500 : 0.4061776540890901\n",
            "Train Loss 1000 : 0.41475335158013205\n",
            "Val Loss : 7.565103438321282 , bn_score Val Accuracy : 0.9067405355493998, cl_score Val Accuracy : 0.06860572483841182\n",
            "Model saved at ./saved_model/review_classifier_epoch_9.pt\n",
            "10/100\n",
            "Train Loss 0 : 0.4406263828277588\n",
            "Train Loss 500 : 0.3808808160458734\n",
            "Train Loss 1000 : 0.39261208434443134\n",
            "Val Loss : 7.969054586747113 , bn_score Val Accuracy : 0.9054478301015697, cl_score Val Accuracy : 0.08254847645429363\n",
            "Model saved at ./saved_model/review_classifier_epoch_10.pt\n",
            "11/100\n",
            "Train Loss 0 : 0.2652354836463928\n",
            "Train Loss 500 : 0.3647077365698453\n",
            "Train Loss 1000 : 0.36901489870025445\n",
            "Val Loss : 8.101676876404706 , bn_score Val Accuracy : 0.9043397968605725, cl_score Val Accuracy : 0.08107109879963066\n",
            "Model saved at ./saved_model/review_classifier_epoch_11.pt\n",
            "12/100\n",
            "Train Loss 0 : 0.3291844129562378\n",
            "Train Loss 500 : 0.3454686353901427\n",
            "Train Loss 1000 : 0.3499979726918094\n",
            "Val Loss : 8.247459128323722 , bn_score Val Accuracy : 0.8990766389658357, cl_score Val Accuracy : 0.07119113573407203\n",
            "Model saved at ./saved_model/review_classifier_epoch_12.pt\n",
            "13/100\n",
            "Train Loss 0 : 0.3695144057273865\n",
            "Train Loss 500 : 0.32752546384008585\n",
            "Train Loss 1000 : 0.3330974050915682\n",
            "Val Loss : 8.399057623919319 , bn_score Val Accuracy : 0.9012003693444137, cl_score Val Accuracy : 0.07599261311172668\n",
            "Model saved at ./saved_model/review_classifier_epoch_13.pt\n",
            "14/100\n",
            "Train Loss 0 : 0.378717303276062\n",
            "Train Loss 500 : 0.3095310341098351\n",
            "Train Loss 1000 : 0.31605693162023485\n",
            "Val Loss : 8.355919750999002 , bn_score Val Accuracy : 0.8995383194829178, cl_score Val Accuracy : 0.07959372114496768\n",
            "Model saved at ./saved_model/review_classifier_epoch_14.pt\n",
            "15/100\n",
            "Train Loss 0 : 0.5417081117630005\n",
            "Train Loss 500 : 0.2962057165087697\n",
            "Train Loss 1000 : 0.30169473996722734\n",
            "Val Loss : 8.485908774768605 , bn_score Val Accuracy : 0.8992613111726685, cl_score Val Accuracy : 0.07192982456140351\n",
            "Model saved at ./saved_model/review_classifier_epoch_15.pt\n",
            "16/100\n",
            "Train Loss 0 : 0.3118504583835602\n",
            "Train Loss 500 : 0.2797626511541431\n",
            "Train Loss 1000 : 0.2849166904028122\n",
            "Val Loss : 8.436372002433329 , bn_score Val Accuracy : 0.9005540166204986, cl_score Val Accuracy : 0.07248384118190213\n",
            "Model saved at ./saved_model/review_classifier_epoch_16.pt\n",
            "17/100\n",
            "Train Loss 0 : 0.2069353312253952\n",
            "Train Loss 500 : 0.2702239779179206\n",
            "Train Loss 1000 : 0.2750479804945516\n",
            "Val Loss : 8.823544617260204 , bn_score Val Accuracy : 0.8998153277931671, cl_score Val Accuracy : 0.06860572483841182\n",
            "Model saved at ./saved_model/review_classifier_epoch_17.pt\n",
            "18/100\n",
            "Train Loss 0 : 0.2502564489841461\n",
            "Train Loss 500 : 0.2582886646875364\n",
            "Train Loss 1000 : 0.2650639755980714\n",
            "Val Loss : 9.083178767035989 , bn_score Val Accuracy : 0.8984302862419206, cl_score Val Accuracy : 0.06795937211449676\n",
            "Model saved at ./saved_model/review_classifier_epoch_18.pt\n",
            "19/100\n",
            "Train Loss 0 : 0.12760019302368164\n",
            "Train Loss 500 : 0.2504827493128781\n",
            "Train Loss 1000 : 0.2571472034089036\n",
            "Val Loss : 9.209547104555018 , bn_score Val Accuracy : 0.8997229916897507, cl_score Val Accuracy : 0.07830101569713759\n",
            "Model saved at ./saved_model/review_classifier_epoch_19.pt\n",
            "20/100\n",
            "Train Loss 0 : 0.22339697182178497\n",
            "Train Loss 500 : 0.24390625391534704\n",
            "Train Loss 1000 : 0.24964321088362168\n",
            "Val Loss : 9.234174383387845 , bn_score Val Accuracy : 0.9016620498614959, cl_score Val Accuracy : 0.07765466297322253\n",
            "Model saved at ./saved_model/review_classifier_epoch_20.pt\n",
            "21/100\n",
            "Train Loss 0 : 0.33081236481666565\n",
            "Train Loss 500 : 0.2372349094815121\n",
            "Train Loss 1000 : 0.24270820952616967\n",
            "Val Loss : 9.382159078822417 , bn_score Val Accuracy : 0.8979686057248384, cl_score Val Accuracy : 0.0677746999076639\n",
            "Model saved at ./saved_model/review_classifier_epoch_21.pt\n",
            "22/100\n",
            "Train Loss 0 : 0.10019198060035706\n",
            "Train Loss 500 : 0.2281460374102978\n",
            "Train Loss 1000 : 0.23338859728523545\n",
            "Val Loss : 9.441336292379043 , bn_score Val Accuracy : 0.8998153277931671, cl_score Val Accuracy : 0.06980609418282549\n",
            "Model saved at ./saved_model/review_classifier_epoch_22.pt\n",
            "23/100\n",
            "Train Loss 0 : 0.1551831215620041\n",
            "Train Loss 500 : 0.2155459117508696\n",
            "Train Loss 1000 : 0.22413858666003764\n",
            "Val Loss : 9.388865905649523 , bn_score Val Accuracy : 0.8987996306555863, cl_score Val Accuracy : 0.06749769159741459\n",
            "Model saved at ./saved_model/review_classifier_epoch_23.pt\n",
            "24/100\n",
            "Train Loss 0 : 0.23774480819702148\n",
            "Train Loss 500 : 0.2135532667522064\n",
            "Train Loss 1000 : 0.21953445114381426\n",
            "Val Loss : 9.587344935361077 , bn_score Val Accuracy : 0.8999076638965836, cl_score Val Accuracy : 0.06999076638965836\n",
            "Model saved at ./saved_model/review_classifier_epoch_24.pt\n",
            "25/100\n",
            "Train Loss 0 : 0.3345637321472168\n",
            "Train Loss 500 : 0.20680912489632886\n",
            "Train Loss 1000 : 0.21415241933488227\n",
            "Val Loss : 9.646109115376191 , bn_score Val Accuracy : 0.8997229916897507, cl_score Val Accuracy : 0.07248384118190213\n",
            "Model saved at ./saved_model/review_classifier_epoch_25.pt\n",
            "26/100\n",
            "Train Loss 0 : 0.18010270595550537\n",
            "Train Loss 500 : 0.2000415443466928\n",
            "Train Loss 1000 : 0.20688178330719412\n",
            "Val Loss : 9.969408728094661 , bn_score Val Accuracy : 0.9012927054478301, cl_score Val Accuracy : 0.06989843028624192\n",
            "Model saved at ./saved_model/review_classifier_epoch_26.pt\n",
            "27/100\n",
            "Train Loss 0 : 0.09552138298749924\n",
            "Train Loss 500 : 0.19872560067299358\n",
            "Train Loss 1000 : 0.20577420184498543\n",
            "Val Loss : 10.03717311410343 , bn_score Val Accuracy : 0.8976915974145891, cl_score Val Accuracy : 0.0703601108033241\n",
            "Model saved at ./saved_model/review_classifier_epoch_27.pt\n",
            "28/100\n",
            "Train Loss 0 : 0.2776048183441162\n",
            "Train Loss 500 : 0.19545155037900883\n",
            "Train Loss 1000 : 0.1984130248099893\n",
            "Val Loss : 10.00404941334444 , bn_score Val Accuracy : 0.8975069252077562, cl_score Val Accuracy : 0.07091412742382272\n",
            "Model saved at ./saved_model/review_classifier_epoch_28.pt\n",
            "29/100\n",
            "Train Loss 0 : 0.23879091441631317\n",
            "Train Loss 500 : 0.19226121013898573\n",
            "Train Loss 1000 : 0.19468952426603148\n",
            "Val Loss : 10.290348863601684 , bn_score Val Accuracy : 0.9001846722068328, cl_score Val Accuracy : 0.07968605724838412\n",
            "Model saved at ./saved_model/review_classifier_epoch_29.pt\n",
            "30/100\n",
            "Train Loss 0 : 0.19858351349830627\n",
            "Train Loss 500 : 0.1875685889296189\n",
            "Train Loss 1000 : 0.19148070247216778\n",
            "Val Loss : 11.023210946251364 , bn_score Val Accuracy : 0.8987996306555863, cl_score Val Accuracy : 0.07728531855955678\n",
            "Model saved at ./saved_model/review_classifier_epoch_30.pt\n",
            "31/100\n",
            "Train Loss 0 : 0.1136268824338913\n",
            "Train Loss 500 : 0.1835864899594508\n",
            "Train Loss 1000 : 0.19024504566719602\n",
            "Val Loss : 10.333307731852813 , bn_score Val Accuracy : 0.8999076638965836, cl_score Val Accuracy : 0.0713758079409049\n",
            "Model saved at ./saved_model/review_classifier_epoch_31.pt\n",
            "32/100\n",
            "Train Loss 0 : 0.3651908040046692\n",
            "Train Loss 500 : 0.17765913964686042\n",
            "Train Loss 1000 : 0.18572973917339827\n",
            "Val Loss : 10.498280558866613 , bn_score Val Accuracy : 0.8982456140350877, cl_score Val Accuracy : 0.06731301939058172\n",
            "Model saved at ./saved_model/review_classifier_epoch_32.pt\n",
            "33/100\n",
            "Train Loss 0 : 0.1418047994375229\n",
            "Train Loss 500 : 0.17476772864585272\n",
            "Train Loss 1000 : 0.1795360570611594\n",
            "Val Loss : 10.564983903660494 , bn_score Val Accuracy : 0.897045244690674, cl_score Val Accuracy : 0.07830101569713759\n",
            "Model saved at ./saved_model/review_classifier_epoch_33.pt\n",
            "34/100\n",
            "Train Loss 0 : 0.26149916648864746\n",
            "Train Loss 500 : 0.17452797452787203\n",
            "Train Loss 1000 : 0.1785341800077931\n",
            "Val Loss : 10.745709887672874 , bn_score Val Accuracy : 0.8971375807940905, cl_score Val Accuracy : 0.06823638042474607\n",
            "Model saved at ./saved_model/review_classifier_epoch_34.pt\n",
            "35/100\n",
            "Train Loss 0 : 0.08809629827737808\n",
            "Train Loss 500 : 0.16930457173722113\n",
            "Train Loss 1000 : 0.17447218154049984\n",
            "Val Loss : 11.030086831485525 , bn_score Val Accuracy : 0.8966759002770083, cl_score Val Accuracy : 0.07543859649122807\n",
            "Model saved at ./saved_model/review_classifier_epoch_35.pt\n",
            "36/100\n",
            "Train Loss 0 : 0.1608022153377533\n",
            "Train Loss 500 : 0.1641333376516601\n",
            "Train Loss 1000 : 0.17224096281120888\n",
            "Val Loss : 10.97303986268885 , bn_score Val Accuracy : 0.9015697137580794, cl_score Val Accuracy : 0.07636195752539243\n",
            "Model saved at ./saved_model/review_classifier_epoch_36.pt\n",
            "37/100\n",
            "Train Loss 0 : 0.280624121427536\n",
            "Train Loss 500 : 0.16252960164300695\n",
            "Train Loss 1000 : 0.16821787588030368\n",
            "Val Loss : 11.180996732150808 , bn_score Val Accuracy : 0.8990766389658357, cl_score Val Accuracy : 0.08217913204062789\n",
            "Model saved at ./saved_model/review_classifier_epoch_37.pt\n",
            "38/100\n",
            "Train Loss 0 : 0.1525077223777771\n",
            "Train Loss 500 : 0.1599754387152171\n",
            "Train Loss 1000 : 0.16711395582022306\n",
            "Val Loss : 11.253034597284653 , bn_score Val Accuracy : 0.8987996306555863, cl_score Val Accuracy : 0.0657433056325023\n",
            "Model saved at ./saved_model/review_classifier_epoch_38.pt\n",
            "39/100\n",
            "Train Loss 0 : 0.33504393696784973\n",
            "Train Loss 500 : 0.16051541926885793\n",
            "Train Loss 1000 : 0.1647827079841545\n",
            "Val Loss : 11.228862734401927 , bn_score Val Accuracy : 0.8935364727608495, cl_score Val Accuracy : 0.07377654662973222\n",
            "Model saved at ./saved_model/review_classifier_epoch_39.pt\n",
            "40/100\n",
            "Train Loss 0 : 0.1428844928741455\n",
            "Train Loss 500 : 0.15716334493708112\n",
            "Train Loss 1000 : 0.16296368670816366\n",
            "Val Loss : 11.49898335793439 , bn_score Val Accuracy : 0.8979686057248384, cl_score Val Accuracy : 0.06814404432132964\n",
            "Model saved at ./saved_model/review_classifier_epoch_40.pt\n",
            "41/100\n",
            "Train Loss 0 : 0.11637818813323975\n",
            "Train Loss 500 : 0.1586458599325009\n",
            "Train Loss 1000 : 0.16493839089694498\n",
            "Val Loss : 11.630641342611874 , bn_score Val Accuracy : 0.8990766389658357, cl_score Val Accuracy : 0.08088642659279778\n",
            "Model saved at ./saved_model/review_classifier_epoch_41.pt\n",
            "42/100\n",
            "Train Loss 0 : 0.16625303030014038\n",
            "Train Loss 500 : 0.15560297937152867\n",
            "Train Loss 1000 : 0.16086481023695204\n",
            "Val Loss : 11.099137266944437 , bn_score Val Accuracy : 0.8956602031394275, cl_score Val Accuracy : 0.0677746999076639\n",
            "Model saved at ./saved_model/review_classifier_epoch_42.pt\n",
            "43/100\n",
            "Train Loss 0 : 0.20895804464817047\n",
            "Train Loss 500 : 0.15288073368865007\n",
            "Train Loss 1000 : 0.16043182661790367\n",
            "Val Loss : 11.502027320861817 , bn_score Val Accuracy : 0.8976915974145891, cl_score Val Accuracy : 0.06740535549399815\n",
            "Model saved at ./saved_model/review_classifier_epoch_43.pt\n",
            "44/100\n",
            "Train Loss 0 : 0.060415539890527725\n",
            "Train Loss 500 : 0.1511841257829628\n",
            "Train Loss 1000 : 0.15612142040700347\n",
            "Val Loss : 11.431969179826623 , bn_score Val Accuracy : 0.8974145891043398, cl_score Val Accuracy : 0.0680517082179132\n",
            "Model saved at ./saved_model/review_classifier_epoch_44.pt\n",
            "45/100\n",
            "Train Loss 0 : 0.0776870921254158\n",
            "Train Loss 500 : 0.15423893754160453\n",
            "Train Loss 1000 : 0.154962621168694\n",
            "Val Loss : 11.68764578594881 , bn_score Val Accuracy : 0.8955678670360111, cl_score Val Accuracy : 0.07913204062788551\n",
            "Model saved at ./saved_model/review_classifier_epoch_45.pt\n",
            "46/100\n",
            "Train Loss 0 : 0.2717053294181824\n",
            "Train Loss 500 : 0.15524193478707307\n",
            "Train Loss 1000 : 0.15573006312546137\n",
            "Val Loss : 11.285067959392771 , bn_score Val Accuracy : 0.8969529085872576, cl_score Val Accuracy : 0.07322253000923361\n",
            "Model saved at ./saved_model/review_classifier_epoch_46.pt\n",
            "47/100\n",
            "Train Loss 0 : 0.059095025062561035\n",
            "Train Loss 500 : 0.1474045466319351\n",
            "Train Loss 1000 : 0.15573556038581735\n",
            "Val Loss : 11.384748618742998 , bn_score Val Accuracy : 0.8979686057248384, cl_score Val Accuracy : 0.06860572483841182\n",
            "Model saved at ./saved_model/review_classifier_epoch_47.pt\n",
            "48/100\n",
            "Train Loss 0 : 0.08281194418668747\n",
            "Train Loss 500 : 0.145864387769661\n",
            "Train Loss 1000 : 0.14907464410339322\n",
            "Val Loss : 11.820467766593485 , bn_score Val Accuracy : 0.9000923361034164, cl_score Val Accuracy : 0.07839335180055401\n",
            "Model saved at ./saved_model/review_classifier_epoch_48.pt\n",
            "49/100\n",
            "Train Loss 0 : 0.09833748638629913\n",
            "Train Loss 500 : 0.14035237897299008\n",
            "Train Loss 1000 : 0.1503131965856318\n",
            "Val Loss : 11.935350244185503 , bn_score Val Accuracy : 0.894921514312096, cl_score Val Accuracy : 0.07156048014773776\n",
            "Model saved at ./saved_model/review_classifier_epoch_49.pt\n",
            "50/100\n",
            "Train Loss 0 : 0.16971367597579956\n",
            "Train Loss 500 : 0.14518144242316783\n",
            "Train Loss 1000 : 0.1491316098377451\n",
            "Val Loss : 11.805005917829625 , bn_score Val Accuracy : 0.897045244690674, cl_score Val Accuracy : 0.07377654662973222\n",
            "Model saved at ./saved_model/review_classifier_epoch_50.pt\n",
            "51/100\n",
            "Train Loss 0 : 0.5699390769004822\n",
            "Train Loss 500 : 0.14741251917700923\n",
            "Train Loss 1000 : 0.14849648259154016\n",
            "Val Loss : 12.174115312800687 , bn_score Val Accuracy : 0.897876269621422, cl_score Val Accuracy : 0.07072945521698984\n",
            "Model saved at ./saved_model/review_classifier_epoch_51.pt\n",
            "52/100\n",
            "Train Loss 0 : 0.08201315999031067\n",
            "Train Loss 500 : 0.14264033257054118\n",
            "Train Loss 1000 : 0.14728279548560405\n",
            "Val Loss : 12.15073369250578 , bn_score Val Accuracy : 0.8976915974145891, cl_score Val Accuracy : 0.07710064635272391\n",
            "Model saved at ./saved_model/review_classifier_epoch_52.pt\n",
            "53/100\n",
            "Train Loss 0 : 0.1768423318862915\n",
            "Train Loss 500 : 0.14339851273852075\n",
            "Train Loss 1000 : 0.1443391911104902\n",
            "Val Loss : 12.329499441034654 , bn_score Val Accuracy : 0.8952908587257618, cl_score Val Accuracy : 0.07340720221606649\n",
            "Model saved at ./saved_model/review_classifier_epoch_53.pt\n",
            "54/100\n",
            "Train Loss 0 : 0.3004022538661957\n",
            "Train Loss 500 : 0.13884013612009213\n",
            "Train Loss 1000 : 0.14395302140919658\n",
            "Val Loss : 12.096736422707053 , bn_score Val Accuracy : 0.8987996306555863, cl_score Val Accuracy : 0.07903970452446907\n",
            "Model saved at ./saved_model/review_classifier_epoch_54.pt\n",
            "55/100\n",
            "Train Loss 0 : 0.15929533541202545\n",
            "Train Loss 500 : 0.1401960122326534\n",
            "Train Loss 1000 : 0.1431488568311209\n",
            "Val Loss : 11.962177024168128 , bn_score Val Accuracy : 0.8983379501385041, cl_score Val Accuracy : 0.06731301939058172\n",
            "Model saved at ./saved_model/review_classifier_epoch_55.pt\n",
            "56/100\n",
            "Train Loss 0 : 0.06763988733291626\n",
            "Train Loss 500 : 0.134909221203533\n",
            "Train Loss 1000 : 0.1420823444670939\n",
            "Val Loss : 11.981123719495885 , bn_score Val Accuracy : 0.897045244690674, cl_score Val Accuracy : 0.06740535549399815\n",
            "Model saved at ./saved_model/review_classifier_epoch_56.pt\n",
            "57/100\n",
            "Train Loss 0 : 0.18239185214042664\n",
            "Train Loss 500 : 0.13845524655844638\n",
            "Train Loss 1000 : 0.1419923208006374\n",
            "Val Loss : 12.57781873029821 , bn_score Val Accuracy : 0.8975069252077562, cl_score Val Accuracy : 0.06823638042474607\n",
            "Model saved at ./saved_model/review_classifier_epoch_57.pt\n",
            "58/100\n",
            "Train Loss 0 : 0.05870245769619942\n",
            "Train Loss 500 : 0.14061572577083062\n",
            "Train Loss 1000 : 0.1429210061748902\n",
            "Val Loss : 12.413090736725751 , bn_score Val Accuracy : 0.8987072945521699, cl_score Val Accuracy : 0.07626962142197599\n",
            "Model saved at ./saved_model/review_classifier_epoch_58.pt\n",
            "59/100\n",
            "Train Loss 0 : 0.20347067713737488\n",
            "Train Loss 500 : 0.13624338132557934\n",
            "Train Loss 1000 : 0.13663897492907026\n",
            "Val Loss : 12.163920214596917 , bn_score Val Accuracy : 0.8971375807940905, cl_score Val Accuracy : 0.07442289935364728\n",
            "Model saved at ./saved_model/review_classifier_epoch_59.pt\n",
            "60/100\n",
            "Train Loss 0 : 0.12535586953163147\n",
            "Train Loss 500 : 0.13610349689980228\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[112], line 92\u001b[0m\n\u001b[0;32m     89\u001b[0m         torch\u001b[38;5;241m.\u001b[39msave(classifier\u001b[38;5;241m.\u001b[39mstate_dict(), model_save_path)\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel saved at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_save_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 92\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[112], line 84\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 84\u001b[0m     \u001b[43mmodel_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcl_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbn_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mINTERVAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     model_test(classifier, test_loader, cl_criterion, bn_criterion, device)\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# 모델 저장 (에포크 번호 추가)\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[109], line 12\u001b[0m, in \u001b[0;36mmodel_train\u001b[1;34m(model, datasets, cl_criterion, bn_criterion, optimizer, device, interval)\u001b[0m\n\u001b[0;32m      9\u001b[0m bn_labels \u001b[38;5;241m=\u001b[39m labels[:, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()         \u001b[38;5;66;03m# 라벨 2진분류  (float형)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m classesd, logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Calculate losses\u001b[39;00m\n\u001b[0;32m     15\u001b[0m loss_cl \u001b[38;5;241m=\u001b[39m cl_criterion(classesd, cl_labels)         \u001b[38;5;66;03m# \u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\envs\\TEXT_018_230_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\envs\\TEXT_018_230_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[105], line 30\u001b[0m, in \u001b[0;36mreviewClassifierModel.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     28\u001b[0m output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(embeddings)\n\u001b[0;32m     29\u001b[0m last_output \u001b[38;5;241m=\u001b[39m output[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m---> 30\u001b[0m last_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m classesd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier1(last_output)\n\u001b[0;32m     32\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier2(last_output)\n",
            "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\envs\\TEXT_018_230_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\envs\\TEXT_018_230_38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\envs\\TEXT_018_230_38\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\envs\\TEXT_018_230_38\\lib\\site-packages\\torch\\nn\\functional.py:1295\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    N_VOCAB = 5000\n",
        "    MAX_LENGTH = 18\n",
        "    EPOCHS = 100\n",
        "    INTERVAL = 500\n",
        "    BATCH_SIZE = 64\n",
        "    LR = 0.001\n",
        "    special_tokens = ['<pad>', '<unk>']\n",
        "\n",
        "    raw_trainDF=pd.read_csv(r'C:\\Users\\MSI\\Desktop\\TORCH_NLP\\1010\\DATA\\Train_Fashion_reivew.csv', usecols=[1, 2, 4])\n",
        "    raw_testDF=pd.read_csv(r'C:\\Users\\MSI\\Desktop\\TORCH_NLP\\1010\\DATA\\Val_Fashion_reivew.csv', usecols=[1, 2, 4])\n",
        "\n",
        "    top10_train=raw_trainDF['Aspect'].value_counts().nlargest(n=9).index.tolist()\n",
        "    trainDF=raw_trainDF.drop(index=raw_trainDF[~raw_trainDF['Aspect'].isin(values=top10_train)].index).reset_index()\n",
        "\n",
        "    top10_test=raw_testDF['Aspect'].value_counts().nlargest(n=9).index.tolist()\n",
        "    testDF=raw_testDF.drop(index=raw_testDF[~raw_testDF['Aspect'].isin(values=top10_test)].index).reset_index()\n",
        "\n",
        "    trainDF, aspectCD = data_encoding(trainDF)\n",
        "    testDF, _ = data_encoding(testDF)\n",
        "\n",
        "    ############################################################################\n",
        "    punc=string.punctuation\n",
        "\n",
        "    for p in punc:\n",
        "        trainDF['SentimentText'] = trainDF['SentimentText'].str.replace(p, '')\n",
        "        testDF['SentimentText']=testDF['SentimentText'].str.replace(p,'')\n",
        "\n",
        "    m=re.compile('[^ ㄱ-ㅣ가-힣]+')\n",
        "\n",
        "    trainDF['SentimentText']=trainDF['SentimentText'].apply(lambda x: m.sub(' ', x))\n",
        "    testDF['SentimentText']=testDF['SentimentText'].apply(lambda x: m.sub(' ', x))\n",
        "\n",
        "    stop_word='./stopwords.txt'\n",
        "\n",
        "    with open(stop_word, 'r', encoding='utf-8') as f:\n",
        "        stop_words = [line.strip() for line in f]\n",
        "\n",
        "\n",
        "    tokenizer = Okt()\n",
        "    train_tokens = [[token for token in tokenizer.morphs(text) if token not in stop_words] for text in trainDF['SentimentText']]\n",
        "    val_tokens = [[token for token in tokenizer.morphs(text) if token not in stop_words] for text in testDF['SentimentText']]\n",
        "\n",
        "    ###################################################################################\n",
        "\n",
        "    vocab = build_vocab(train_tokens, N_VOCAB, special_tokens)\n",
        "    token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
        "    id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
        "\n",
        "    pad_id = token_to_id['<pad>']\n",
        "    unk_id = token_to_id['<unk>']\n",
        "    train_ids = encoding_ids(token_to_id, train_tokens, unk_id)\n",
        "    test_ids = encoding_ids(token_to_id, val_tokens, unk_id)\n",
        "    train_ids = pad_sequences(train_ids, MAX_LENGTH, pad_id)\n",
        "    test_ids = pad_sequences(test_ids, MAX_LENGTH, pad_id)\n",
        "\n",
        "    train_ids = torch.tensor(train_ids, dtype=torch.long)\n",
        "    test_ids = torch.tensor(test_ids, dtype=torch.long)\n",
        "\n",
        "    train_labels = torch.tensor(list(zip(trainDF['Aspect'].values, trainDF['SentimentPolarity'].values)), dtype=torch.long)\n",
        "    test_labels = torch.tensor(list(zip(testDF['Aspect'].values, testDF['SentimentPolarity'].values)), dtype=torch.float32)\n",
        "\n",
        "    train_dataset = TensorDataset(train_ids, train_labels)\n",
        "    test_dataset = TensorDataset(test_ids, test_labels)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    n_vocab = len(token_to_id)\n",
        "    hidden_dim = 64\n",
        "    embedding_dim = 128\n",
        "    n_layers = 2\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    classifier = reviewClassifierModel(\n",
        "        n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim, n_classes=len(aspectCD), n_layers=n_layers\n",
        "    ).to(device)\n",
        "\n",
        "    cl_criterion = nn.CrossEntropyLoss().to(device)\n",
        "    bn_criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "    optimizer = optim.RMSprop(classifier.parameters(), lr=LR)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f'{epoch+1}/{EPOCHS}')\n",
        "        model_train(classifier, train_loader, cl_criterion, bn_criterion, optimizer, device, INTERVAL)\n",
        "        model_test(classifier, test_loader, cl_criterion, bn_criterion, device)\n",
        "\n",
        "        # 모델 저장 (에포크 번호 추가)\n",
        "        model_save_path = f'./saved_model/review_classifier_epoch_{epoch + 1}.pt'  # 에포크 번호 포함\n",
        "        torch.save(classifier.state_dict(), model_save_path)\n",
        "        print(f'Model saved at {model_save_path}')\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['윈하', '는', '색', '은', '없지만']\n",
            "tensor([[0.0043, 0.0855, 0.2323, 0.6315, 0.0043, 0.0016, 0.0016, 0.0016, 0.0016,\n",
            "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
            "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
            "         0.0016, 0.0016, 0.0016, 0.0016, 0.0016]])\n",
            "Predicted label: 3\n"
          ]
        }
      ],
      "source": [
        "def predict(model, text, tokenizer, num_classes):\n",
        "    model.eval()\n",
        "    \n",
        "    # 1. 입력 텍스트를 토큰화 (Okt 사용)\n",
        "    tokens = tokenizer.morphs(text)\n",
        "    print(tokens)\n",
        "    \n",
        "    vocab = build_vocab(tokens, 5000, ['<pad>', '<unk>'])\n",
        "    token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
        "    id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
        "\n",
        "    # 2. 토큰을 ID로 변환 (예: token_to_id)\n",
        "    input_ids = [token_to_id.get(token, token_to_id[\"<unk>\"]) for token in tokens]\n",
        "    \n",
        "    # 3. 입력 길이에 맞게 패딩\n",
        "    max_length = 32  # 원하는 최대 길이\n",
        "    pad_id = token_to_id[\"<pad>\"]\n",
        "    input_ids = input_ids[:max_length] + [pad_id] * (max_length - len(input_ids))\n",
        "    \n",
        "    # 4. 입력 텐서 생성\n",
        "    input_tensor = torch.tensor([input_ids], dtype=torch.float32)  # 배치 차원 추가 및 float로 변환\n",
        "    \n",
        "    # 5. 모델에 입력하여 예측\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_tensor)\n",
        "    \n",
        "    # 6. 소프트맥스 함수로 확률 계산\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    print(probs)\n",
        "    \n",
        "    # 7. 최대 확률을 가진 클래스를 예측\n",
        "    prediction = torch.argmax(probs, dim=1)\n",
        "\n",
        "    # 8. 예측 결과 출력\n",
        "    print(f\"Predicted label: {prediction.item()}\")\n",
        "\n",
        "# 입력 예시\n",
        "num_classes = 9  # 예시: 분류할 클래스 수\n",
        "predict(reviewClassifierModel, \"텍스트\", tokenizer, num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "TEXT_018_230_38",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
