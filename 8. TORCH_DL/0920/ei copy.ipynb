{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모듈 로딩\n",
    "# Model 관련\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.optim as optim\n",
    "from torchmetrics.classification import F1Score,BinaryF1Score,BinaryAccuracy,BinaryConfusionMatrix\n",
    "import torch.optim.lr_scheduler as lr_scheduler \n",
    "\n",
    "\n",
    "# Data 및 시각화 관련\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class make_dataset(Dataset):\n",
    "    def __init__(self,featureDF,targetDF) -> None:\n",
    "        super().__init__()\n",
    "        # self.DF = DF\n",
    "        self.targetDF = targetDF\n",
    "        self.featureDF = featureDF\n",
    "        self.n_rows = featureDF.shape[0]\n",
    "        self.n_features = featureDF.shape[1]\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'        \n",
    "    def __len__(self):\n",
    "        return self.n_rows\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 텐서화\n",
    "        featureTS = torch.FloatTensor(self.featureDF.iloc[index].values).to(self.device)\n",
    "        targetTS = torch.FloatTensor(self.targetDF.iloc[index].values).to(self.device)\n",
    "\n",
    "        # 피쳐와 타겟 반환\n",
    "        return featureTS, targetTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dynamic_bcf_Model(nn.Module):\n",
    "    # 모델구조 구성 및 인스턴스 생성 메서드\n",
    "    def __init__(self,in_in,* in_out):\n",
    "        super().__init__()\n",
    "        self.in_layer = nn.Linear(in_in,in_out[0])\n",
    "        self.h_layer = nn.ModuleList([nn.Linear(in_out[i],in_out[i+1]) for i in range(len(in_out)-1)])\n",
    "        self.out_layer = nn.Linear(in_out[-1],1)\n",
    "\n",
    "    # 순방향 학습 진행 메서드\n",
    "    def forward(self,x):\n",
    "        y=self.in_layer(x)      # f1w1 + f2w2 + f3w3 + b 결과 10\n",
    "        y=F.relu(y)             # 0 <= y\n",
    "        for layer in self.h_layer:\n",
    "            y=layer(y)\n",
    "            y=F.relu(y)\n",
    "        return F.sigmoid(self.out_layer(y))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posts</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>know intj tool use interaction people excuse a...</td>\n",
       "      <td>INTJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rap music ehh opp yeah know valid well know fa...</td>\n",
       "      <td>INTJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>preferably p hd low except wew lad video p min...</td>\n",
       "      <td>INTJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>drink like wish could drink red wine give head...</td>\n",
       "      <td>INTJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>space program ah bad deal meing freelance max ...</td>\n",
       "      <td>INTJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106062</th>\n",
       "      <td>stay frustrate world life want take long nap w...</td>\n",
       "      <td>INFP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106063</th>\n",
       "      <td>fizzle around time mention sure mistake thing ...</td>\n",
       "      <td>INFP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106064</th>\n",
       "      <td>schedule modify hey w intp strong wing underst...</td>\n",
       "      <td>INFP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106065</th>\n",
       "      <td>enfj since january busy schedule able spend li...</td>\n",
       "      <td>INFP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106066</th>\n",
       "      <td>feel like men good problem tell parent want te...</td>\n",
       "      <td>INFP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106067 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    posts  type\n",
       "0       know intj tool use interaction people excuse a...  INTJ\n",
       "1       rap music ehh opp yeah know valid well know fa...  INTJ\n",
       "2       preferably p hd low except wew lad video p min...  INTJ\n",
       "3       drink like wish could drink red wine give head...  INTJ\n",
       "4       space program ah bad deal meing freelance max ...  INTJ\n",
       "...                                                   ...   ...\n",
       "106062  stay frustrate world life want take long nap w...  INFP\n",
       "106063  fizzle around time mention sure mistake thing ...  INFP\n",
       "106064  schedule modify hey w intp strong wing underst...  INFP\n",
       "106065  enfj since january busy schedule able spend li...  INFP\n",
       "106066  feel like men good problem tell parent want te...  INFP\n",
       "\n",
       "[106067 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbti_df=pd.read_csv('../data/MBTI.csv')\n",
    "mbti_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "ENFJ    60\n",
       "ENFP    60\n",
       "ENTJ    60\n",
       "ENTP    60\n",
       "ESFJ    60\n",
       "ESFP    60\n",
       "ESTJ    60\n",
       "ESTP    60\n",
       "INFJ    60\n",
       "INFP    60\n",
       "INTJ    60\n",
       "INTP    60\n",
       "ISFJ    60\n",
       "ISFP    60\n",
       "ISTJ    60\n",
       "ISTP    60\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df = mbti_df.groupby('type', group_keys=False).apply(lambda x: x.sample(n=60, random_state=1))\n",
    "sampled_df['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df=sampled_df['posts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53640     True\n",
       "54635     True\n",
       "53847     True\n",
       "53691     True\n",
       "54304     True\n",
       "         ...  \n",
       "50725    False\n",
       "52006    False\n",
       "50163    False\n",
       "52683    False\n",
       "52551    False\n",
       "Name: type, Length: 960, dtype: bool"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_df=sampled_df['type'].str.contains('E')\n",
    "target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#E=0, I=1\n",
    "target_df=np.array(target_df.astype('int8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#문자열 데이터를 벡터화\n",
    "vectorizer = TfidfVectorizer(max_features=1200,min_df=5)\n",
    "vectorizer = vectorizer.fit(sampled_df['posts'].to_numpy().reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vds = []\n",
    "vts = []\n",
    "for i,e in enumerate(feature_df.to_numpy()):\n",
    "    dataStr = e.split(' ')\n",
    "    npStr = np.array(dataStr).reshape(-1)\n",
    "    for v in vectorizer.transform(npStr).toarray():\n",
    "        vds.append(v)\n",
    "        vts.append(target_df[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df=pd.DataFrame(vds)\n",
    "target_df=pd.DataFrame(vts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480864, 1200) 2\n",
      "(480864, 1) 2\n"
     ]
    }
   ],
   "source": [
    "print(feature_df.shape,feature_df.ndim)\n",
    "print(target_df.shape,target_df.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(feature_df,target_df,stratify=target_df, test_size=0.2, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train,y_train,stratify=y_train, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dynamic_bcf_Model(\n",
      "  (in_layer): Linear(in_features=1200, out_features=900, bias=True)\n",
      "  (h_layer): ModuleList(\n",
      "    (0): Linear(in_features=900, out_features=800, bias=True)\n",
      "    (1): Linear(in_features=800, out_features=700, bias=True)\n",
      "    (2): Linear(in_features=700, out_features=600, bias=True)\n",
      "    (3): Linear(in_features=600, out_features=500, bias=True)\n",
      "    (4): Linear(in_features=500, out_features=400, bias=True)\n",
      "    (5): Linear(in_features=400, out_features=300, bias=True)\n",
      "    (6): Linear(in_features=300, out_features=200, bias=True)\n",
      "    (7): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (8): Linear(in_features=100, out_features=80, bias=True)\n",
      "    (9): Linear(in_features=80, out_features=75, bias=True)\n",
      "    (10): Linear(in_features=75, out_features=50, bias=True)\n",
      "    (11): Linear(in_features=50, out_features=25, bias=True)\n",
      "    (12): Linear(in_features=25, out_features=15, bias=True)\n",
      "    (13): Linear(in_features=15, out_features=10, bias=True)\n",
      "    (14): Linear(in_features=10, out_features=5, bias=True)\n",
      "  )\n",
      "  (out_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model=dynamic_bcf_Model(1200,900,800,700,600,500,400,300,200,100,80,75,50,25,15,10,5)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "615.504"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[0]/500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([615, 1200]) torch.Size([615, 1]) tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "train_ds=make_dataset(x_train,y_train)\n",
    "val_ds=make_dataset(x_val,y_val)\n",
    "test_ds=make_dataset(x_test,y_test)\n",
    "\n",
    "train_dl=DataLoader(train_ds,batch_size=x_train.shape[0]//500)\n",
    "\n",
    "for feature,target in train_dl:\n",
    "    print(feature.shape,target.shape, feature,target)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "scheduler=lr_scheduler.ReduceLROnPlateau(optimizer,mode='max',patience=10,verbose=True)\n",
    "\n",
    "binary_loss=nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1000]\n",
      "- Train Loss : 0.6931500256418468 Score : 0.4893268052927272\n",
      "- Val Loss : 0.6931465268135071 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 0 scheduler.patience: 100\n",
      "[2/1000]\n",
      "- Train Loss : 0.693149288971267 Score : 0.4879889404940272\n",
      "- Val Loss : 0.6931466460227966 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 1 scheduler.patience: 100\n",
      "[3/1000]\n",
      "- Train Loss : 0.693149188440479 Score : 0.4879889404940272\n",
      "- Val Loss : 0.693146824836731 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 2 scheduler.patience: 100\n",
      "[4/1000]\n",
      "- Train Loss : 0.6931491048036221 Score : 0.4879172710839384\n",
      "- Val Loss : 0.6931469440460205 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 3 scheduler.patience: 100\n",
      "[5/1000]\n",
      "- Train Loss : 0.6931490240220776 Score : 0.4879172710839384\n",
      "- Val Loss : 0.6931470036506653 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 4 scheduler.patience: 100\n",
      "[6/1000]\n",
      "- Train Loss : 0.6931489704849715 Score : 0.4879172710839384\n",
      "- Val Loss : 0.6931472420692444 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 5 scheduler.patience: 100\n",
      "[7/1000]\n",
      "- Train Loss : 0.6931489233723181 Score : 0.48923780734905464\n",
      "- Val Loss : 0.6931472420692444 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 6 scheduler.patience: 100\n",
      "[8/1000]\n",
      "- Train Loss : 0.6931488918449112 Score : 0.48923780734905464\n",
      "- Val Loss : 0.6931473016738892 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 7 scheduler.patience: 100\n",
      "[9/1000]\n",
      "- Train Loss : 0.6931488676937279 Score : 0.48923780734905464\n",
      "- Val Loss : 0.6931474208831787 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 8 scheduler.patience: 100\n",
      "[10/1000]\n",
      "- Train Loss : 0.6931488324782091 Score : 0.4879646062137124\n",
      "- Val Loss : 0.6931474208831787 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 9 scheduler.patience: 100\n",
      "[11/1000]\n",
      "- Train Loss : 0.6931488002369741 Score : 0.4879646062137124\n",
      "- Val Loss : 0.6931475400924683 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 10 scheduler.patience: 100\n",
      "[12/1000]\n",
      "- Train Loss : 0.6931487697803094 Score : 0.4879646062137124\n",
      "- Val Loss : 0.6931475400924683 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 11 scheduler.patience: 100\n",
      "[13/1000]\n",
      "- Train Loss : 0.6931487380149598 Score : 0.4879646062137124\n",
      "- Val Loss : 0.693147599697113 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 12 scheduler.patience: 100\n",
      "[14/1000]\n",
      "- Train Loss : 0.693148724690169 Score : 0.4892938350013154\n",
      "- Val Loss : 0.6931477189064026 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 13 scheduler.patience: 100\n",
      "[15/1000]\n",
      "- Train Loss : 0.6931486991113294 Score : 0.4892938350013154\n",
      "- Val Loss : 0.6931476593017578 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 14 scheduler.patience: 100\n",
      "[16/1000]\n",
      "- Train Loss : 0.6931486863813953 Score : 0.4879445421719503\n",
      "- Val Loss : 0.6931476593017578 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 15 scheduler.patience: 100\n",
      "[17/1000]\n",
      "- Train Loss : 0.6931486500951345 Score : 0.4879445421719503\n",
      "- Val Loss : 0.6931474804878235 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 16 scheduler.patience: 100\n",
      "[18/1000]\n",
      "- Train Loss : 0.6931487261178251 Score : 0.4879445421719503\n",
      "- Val Loss : 0.6931477785110474 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 17 scheduler.patience: 100\n",
      "[19/1000]\n",
      "- Train Loss : 0.6931487340889053 Score : 0.4879445421719503\n",
      "- Val Loss : 0.6931477189064026 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 18 scheduler.patience: 100\n",
      "[20/1000]\n",
      "- Train Loss : 0.6931486625871258 Score : 0.4879445421719503\n",
      "- Val Loss : 0.6931477189064026 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 19 scheduler.patience: 100\n",
      "[21/1000]\n",
      "- Train Loss : 0.6931486598507849 Score : 0.4879445421719503\n",
      "- Val Loss : 0.6931478381156921 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 20 scheduler.patience: 100\n",
      "[22/1000]\n",
      "- Train Loss : 0.6931486487864497 Score : 0.4879445421719503\n",
      "- Val Loss : 0.6931477785110474 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 21 scheduler.patience: 100\n",
      "[23/1000]\n",
      "- Train Loss : 0.6931486371272576 Score : 0.4879445421719503\n",
      "- Val Loss : 0.6931480169296265 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 22 scheduler.patience: 100\n",
      "[24/1000]\n",
      "- Train Loss : 0.6931486310597189 Score : 0.4879445421719503\n",
      "- Val Loss : 0.6931478381156921 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 23 scheduler.patience: 100\n",
      "[25/1000]\n",
      "- Train Loss : 0.6931486214230398 Score : 0.48922973359654287\n",
      "- Val Loss : 0.6931479573249817 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 24 scheduler.patience: 100\n",
      "[26/1000]\n",
      "- Train Loss : 0.6931486186866989 Score : 0.48922973359654287\n",
      "- Val Loss : 0.6931479573249817 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 25 scheduler.patience: 100\n",
      "[27/1000]\n",
      "- Train Loss : 0.6931486110725327 Score : 0.48922973359654287\n",
      "- Val Loss : 0.6931479573249817 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 26 scheduler.patience: 100\n",
      "[28/1000]\n",
      "- Train Loss : 0.6931485994133407 Score : 0.48789186879784285\n",
      "- Val Loss : 0.6931479573249817 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 27 scheduler.patience: 100\n",
      "[29/1000]\n",
      "- Train Loss : 0.6931485942975728 Score : 0.48789186879784285\n",
      "- Val Loss : 0.6931480169296265 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 28 scheduler.patience: 100\n",
      "[30/1000]\n",
      "- Train Loss : 0.6931485821624954 Score : 0.48789186879784285\n",
      "- Val Loss : 0.6931480169296265 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 29 scheduler.patience: 100\n",
      "[31/1000]\n",
      "- Train Loss : 0.693148576213928 Score : 0.48789186879784285\n",
      "- Val Loss : 0.6931480169296265 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 30 scheduler.patience: 100\n",
      "[32/1000]\n",
      "- Train Loss : 0.6931485649116501 Score : 0.48789186879784285\n",
      "- Val Loss : 0.693148136138916 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 31 scheduler.patience: 100\n",
      "[33/1000]\n",
      "- Train Loss : 0.6931485544421716 Score : 0.48789186879784285\n",
      "- Val Loss : 0.6931480765342712 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 32 scheduler.patience: 100\n",
      "[34/1000]\n",
      "- Train Loss : 0.6931485488505182 Score : 0.48789186879784285\n",
      "- Val Loss : 0.6931480169296265 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 33 scheduler.patience: 100\n",
      "[35/1000]\n",
      "- Train Loss : 0.693148535406756 Score : 0.48789186879784285\n",
      "- Val Loss : 0.693148136138916 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 34 scheduler.patience: 100\n",
      "[36/1000]\n",
      "- Train Loss : 0.6931485313617303 Score : 0.48789186879784285\n",
      "- Val Loss : 0.6931483745574951 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 35 scheduler.patience: 100\n",
      "[37/1000]\n",
      "- Train Loss : 0.6931485207732804 Score : 0.48789476003951415\n",
      "- Val Loss : 0.6931482553482056 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 36 scheduler.patience: 100\n",
      "[38/1000]\n",
      "- Train Loss : 0.6931485124452861 Score : 0.48789476003951415\n",
      "- Val Loss : 0.6931482553482056 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 37 scheduler.patience: 100\n",
      "[39/1000]\n",
      "- Train Loss : 0.6931484987635812 Score : 0.48789476003951415\n",
      "- Val Loss : 0.6931482553482056 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 38 scheduler.patience: 100\n",
      "[40/1000]\n",
      "- Train Loss : 0.6931484879371886 Score : 0.48789476003951415\n",
      "- Val Loss : 0.6931483745574951 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 39 scheduler.patience: 100\n",
      "[41/1000]\n",
      "- Train Loss : 0.6931484835352488 Score : 0.48789476003951415\n",
      "- Val Loss : 0.6931483745574951 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 40 scheduler.patience: 100\n",
      "[42/1000]\n",
      "- Train Loss : 0.6931485105417445 Score : 0.48789476003951415\n",
      "- Val Loss : 0.6931483745574951 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 41 scheduler.patience: 100\n",
      "[43/1000]\n",
      "- Train Loss : 0.6931484560528677 Score : 0.48651454179824705\n",
      "- Val Loss : 0.6931484341621399 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 42 scheduler.patience: 100\n",
      "[44/1000]\n",
      "- Train Loss : 0.6931484453454465 Score : 0.4878204638610581\n",
      "- Val Loss : 0.6931484341621399 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 43 scheduler.patience: 100\n",
      "[45/1000]\n",
      "- Train Loss : 0.6931484240495754 Score : 0.4878204638610581\n",
      "- Val Loss : 0.6931486129760742 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 44 scheduler.patience: 100\n",
      "[46/1000]\n",
      "- Train Loss : 0.6931484064418161 Score : 0.4878204638610581\n",
      "- Val Loss : 0.6931484341621399 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 45 scheduler.patience: 100\n",
      "[47/1000]\n",
      "- Train Loss : 0.693148379316349 Score : 0.4878204638610581\n",
      "- Val Loss : 0.6931485533714294 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 46 scheduler.patience: 100\n",
      "[48/1000]\n",
      "- Train Loss : 0.6931483561169364 Score : 0.4878204638610581\n",
      "- Val Loss : 0.6931485533714294 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 47 scheduler.patience: 100\n",
      "[49/1000]\n",
      "- Train Loss : 0.6931483298242687 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487321853638 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 48 scheduler.patience: 100\n",
      "[50/1000]\n",
      "- Train Loss : 0.6931483150718217 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 49 scheduler.patience: 100\n",
      "[51/1000]\n",
      "- Train Loss : 0.6931483106698818 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487321853638 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 50 scheduler.patience: 100\n",
      "[52/1000]\n",
      "- Train Loss : 0.6931483046023431 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 51 scheduler.patience: 100\n",
      "[53/1000]\n",
      "- Train Loss : 0.6931483060299993 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 52 scheduler.patience: 100\n",
      "[54/1000]\n",
      "- Train Loss : 0.693148306624856 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 53 scheduler.patience: 100\n",
      "[55/1000]\n",
      "- Train Loss : 0.6931483067438274 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 54 scheduler.patience: 100\n",
      "[56/1000]\n",
      "- Train Loss : 0.693148306624856 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 55 scheduler.patience: 100\n",
      "[57/1000]\n",
      "- Train Loss : 0.6931483071007415 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 56 scheduler.patience: 100\n",
      "[58/1000]\n",
      "- Train Loss : 0.6931483074576555 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 57 scheduler.patience: 100\n",
      "[59/1000]\n",
      "- Train Loss : 0.6931483074576555 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 58 scheduler.patience: 100\n",
      "[60/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 59 scheduler.patience: 100\n",
      "[61/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 60 scheduler.patience: 100\n",
      "[62/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 61 scheduler.patience: 100\n",
      "[63/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 62 scheduler.patience: 100\n",
      "[64/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 63 scheduler.patience: 100\n",
      "[65/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 64 scheduler.patience: 100\n",
      "[66/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 65 scheduler.patience: 100\n",
      "[67/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 66 scheduler.patience: 100\n",
      "[68/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 67 scheduler.patience: 100\n",
      "[69/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 68 scheduler.patience: 100\n",
      "[70/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 69 scheduler.patience: 100\n",
      "[71/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 70 scheduler.patience: 100\n",
      "[72/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 71 scheduler.patience: 100\n",
      "[73/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 72 scheduler.patience: 100\n",
      "[74/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 73 scheduler.patience: 100\n",
      "[75/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 74 scheduler.patience: 100\n",
      "[76/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 75 scheduler.patience: 100\n",
      "[77/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 76 scheduler.patience: 100\n",
      "[78/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 77 scheduler.patience: 100\n",
      "[79/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 78 scheduler.patience: 100\n",
      "[80/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 79 scheduler.patience: 100\n",
      "[81/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 80 scheduler.patience: 100\n",
      "[82/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 81 scheduler.patience: 100\n",
      "[83/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 82 scheduler.patience: 100\n",
      "[84/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 83 scheduler.patience: 100\n",
      "[85/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 84 scheduler.patience: 100\n",
      "[86/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 85 scheduler.patience: 100\n",
      "[87/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 86 scheduler.patience: 100\n",
      "[88/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 87 scheduler.patience: 100\n",
      "[89/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 88 scheduler.patience: 100\n",
      "[90/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 89 scheduler.patience: 100\n",
      "[91/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 90 scheduler.patience: 100\n",
      "[92/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 91 scheduler.patience: 100\n",
      "[93/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 92 scheduler.patience: 100\n",
      "[94/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 93 scheduler.patience: 100\n",
      "[95/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 94 scheduler.patience: 100\n",
      "[96/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 95 scheduler.patience: 100\n",
      "[97/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 96 scheduler.patience: 100\n",
      "[98/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 97 scheduler.patience: 100\n",
      "[99/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 98 scheduler.patience: 100\n",
      "[100/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 99 scheduler.patience: 100\n",
      "[101/1000]\n",
      "- Train Loss : 0.6931483069817701 Score : 0.4891583286597581\n",
      "- Val Loss : 0.6931487917900085 Score : 0.6674922108650208\n",
      "scheduler.num_bad_epochs: 100 scheduler.patience: 100\n",
      "100 EPOCH 성능 개선 없어서 조기 종료\n"
     ]
    }
   ],
   "source": [
    "loss_history=[[],[]]\n",
    "score_history=[[],[]]\n",
    "\n",
    "EPOCH=100\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    model.train()\n",
    "\n",
    "    total_loss=0\n",
    "    total_score=0\n",
    "\n",
    "    for feature, target in train_dl:\n",
    "        #학습 진행\n",
    "        pre_y=model(feature)\n",
    "\n",
    "        #손실 계산\n",
    "        loss=binary_loss(pre_y,target)\n",
    "        total_loss+=loss.item()\n",
    "\n",
    "        #성능평가\n",
    "        score=BinaryF1Score()(pre_y,target)\n",
    "        total_score+=score.item()\n",
    "\n",
    "        #최적화 진행\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 에포크 당 검증\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #검증데이터셋\n",
    "        val_feature=val_ds[:][0]\n",
    "        val_target=val_ds[:][1]\n",
    "\n",
    "        #평가\n",
    "        pre_val=model(val_feature)\n",
    "\n",
    "        #손실\n",
    "        loss_val=binary_loss(pre_val,val_target)\n",
    "        \n",
    "        #성능평가\n",
    "        score_val=BinaryF1Score()(pre_val,val_target)\n",
    "\n",
    "    # 손실값과 성능값 저장\n",
    "    loss_history[0].append(total_loss/len(train_dl))\n",
    "    score_history[0].append(total_score/len(train_dl))\n",
    "\n",
    "    loss_history[1].append(loss_val)\n",
    "    score_history[1].append(score_val)\n",
    "\n",
    "    print(f'[{epoch+1}/{EPOCH}]\\n- Train Loss : {loss_history[0][-1]} Score : {score_history[0][-1]}')\n",
    "    print(f'- Val Loss : {loss_history[1][-1]} Score : {score_history[1][-1]}')\n",
    "\n",
    "    #최적화 스케줄러 인스턴스 업데이트\n",
    "    scheduler.step(loss_val)\n",
    "    print(f'scheduler.num_bad_epochs: {scheduler.num_bad_epochs}',end=' ')\n",
    "    print(f'scheduler.patience: {scheduler.patience}')\n",
    "\n",
    "    #손실 감소(성능 개선) 안 되는 경우 조기 종료\n",
    "    if scheduler.num_bad_epochs >= scheduler.patience:\n",
    "        print(f'{scheduler.patience} EPOCH 성능 개선 없어서 조기 종료')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#학습 결과 확인(학습과 검증의 Loss, 성능지표 변화 확인) w. 시각화\n",
    "\n",
    "th=len(score_history[1])\n",
    "fg,axes=plt.subplots(1,2,figsize=(10,5),sharey=True)\n",
    "axes[0].plot(range(1,th+1),loss_history[0][:th],label='Train')\n",
    "axes[0].plot(range(1,th+1),loss_history[1][:th],label='Val')\n",
    "axes[0].set_ylim([0.5,0.8])\n",
    "axes[0].grid()\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(range(1,th+1),score_history[0][:th],label='Train')\n",
    "axes[1].plot(range(1,th+1),score_history[1][:th],label='Val')\n",
    "axes[1].grid()\n",
    "\n",
    "axes[1].legend\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
