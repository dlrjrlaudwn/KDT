{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[IRIS - Regression]\n",
    "- feature: 3개 (Sepal_Length, Sepal_Width, Petal_Width)\n",
    "- target: 1개 (Petal_Length)\n",
    "\n",
    "[IRIS - Binary Classification]\n",
    "- feature: 4개 (Sepal_Length, Sepal_Width, Petal_Width, Petal_Length)\n",
    "- target: 1개 (Petal_Length)\n",
    "- class: 품종 3개 (두 개의 품종만 선택_ Setosa or other)\n",
    "\n",
    "[IRIS - MultiClassification]\n",
    "- feature: 4개 (Sepal_Length, Sepal_Width, Petal_Width, Petal_Length)\n",
    "- target: 1개 (Petal_Length)\n",
    "- class: 품종 3개\n",
    "\n",
    "- - -\n",
    "\n",
    "[MNIST-Digit]\n",
    "- feature: 64개\n",
    "- target: 1ro\n",
    "- class: 0-9 (10개)\n",
    "\n",
    "[MNIST-Fashion]\n",
    "- feature: 784개\n",
    "- target: 1개\n",
    "- class: 0-9 (10개)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[DNN 기반 다중 분류 모델 구현]\n",
    "- 데이터셋"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] 모듈 로딩 및 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모듈 로딩\n",
    "# Model 관련\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.optim as optim\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "from torchmetrics.classification import BinaryConfusionMatrix\n",
    "from torchinfo import summary\n",
    "\n",
    "# Data 및 시각화 관련\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch  v.2.4.1\n",
      "pandas v.2.0.3\n"
     ]
    }
   ],
   "source": [
    "#활용 패키지 버전 체크\n",
    "print(f'torch  v.{torch.__version__}')\n",
    "print(f'pandas v.{pd.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal.length  sepal.width  petal.length  petal.width variety\n",
       "0           5.1          3.5           1.4          0.2  Setosa\n",
       "1           4.9          3.0           1.4          0.2  Setosa\n",
       "2           4.7          3.2           1.3          0.2  Setosa\n",
       "3           4.6          3.1           1.5          0.2  Setosa\n",
       "4           5.0          3.6           1.4          0.2  Setosa"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터 로딩\n",
    "data_file='../data/iris.csv'\n",
    "\n",
    "#csv -> DataFrame\n",
    "iris_df=pd.read_csv(data_file)\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal.length  sepal.width  petal.length  petal.width  variety\n",
       "0             5.1          3.5           1.4          0.2        0\n",
       "1             4.9          3.0           1.4          0.2        0\n",
       "2             4.7          3.2           1.3          0.2        0\n",
       "3             4.6          3.1           1.5          0.2        0\n",
       "4             5.0          3.6           1.4          0.2        0\n",
       "..            ...          ...           ...          ...      ...\n",
       "145           6.7          3.0           5.2          2.3        2\n",
       "146           6.3          2.5           5.0          1.9        2\n",
       "147           6.5          3.0           5.2          2.0        2\n",
       "148           6.2          3.4           5.4          2.3        2\n",
       "149           5.9          3.0           5.1          1.8        2\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=dict(zip(iris_df['variety'].unique().tolist(),range(3)))\n",
    "iris_df['variety']=iris_df['variety'].replace(labels)\n",
    "\n",
    "iris_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - - \n",
    "[2] 모델 클래스 설계 및 정의\n",
    "- 클래스 목적: iris 데이터 학습 및 추론\n",
    "- 클래스 이름: iris_mcf_model\n",
    "- 부모 클래스: nn.Module\n",
    "- 매개변수: 각 층별 입출력 개수 고정 => 필요 X\n",
    "- 클래스 속성: feature_df, target_df, n_rows, n_features\n",
    "- 클래스 기능: _ _ init _ _( ) <- 모델 구조 설정, forward( ) <- 순방향 학습(오버라이딩(상속 관계에서만 가능) 필요)\n",
    "- 클래스 구조\n",
    "    - 입력층:  4개 입력 (feature 개수) 10개 출력 (=퍼셉트론 10개) \n",
    "    - 은닉층: 10개 입력                 5개 출력 (=퍼셉트론 30개)\n",
    "    - 출력층:  5개 입력                 3개 출력 (다중분류의 결과)\n",
    "\n",
    "- 활성화함수\n",
    "    - 클래스 형태 (대문자로 시작)\n",
    "        - nn.MSELoss, nn.ReLU 등\n",
    "        - _ _ init _ _( ) 메서드에서 사용됨\n",
    "        - 하나의 층처럼 사용 가능\n",
    "    - 함수 형태 (소문자로 시작)\n",
    "        - torch.nn.functional.relu 등\n",
    "        - forward 메서드에 사용됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iris_mcf_model(nn.Module):\n",
    "\n",
    "    #모델 구조 구성 및 인스턴스 생성 메서드\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_layer=nn.Linear(4,10)\n",
    "        self.hidden_layer=nn.Linear(10,5)\n",
    "        self.out_layer=nn.Linear(5,3)\n",
    "\n",
    "    #순방향 학습 진행 메서드\n",
    "    def forward(self,input_data):\n",
    "\n",
    "        #입력층\n",
    "        y=self.in_layer(input_data)     \n",
    "        y=F.relu(y)                     # y 값: 0 이상\n",
    "\n",
    "        #은닉층\n",
    "        y=self.hidden_layer(y)\n",
    "        y=F.relu(y)\n",
    "\n",
    "        #출력층_다중분류니까 손실함수 CrossEntropyLoss가 내부에서 Softmax 진행\n",
    "        return self.out_layer(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris_mcf_model(\n",
      "  (in_layer): Linear(in_features=4, out_features=10, bias=True)\n",
      "  (hidden_layer): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (out_layer): Linear(in_features=5, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#모델 인스턴스 생성_확인용\n",
    "model=iris_mcf_model()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "iris_mcf_model                           [5, 3]                    --\n",
       "├─Linear: 1-1                            [5, 10]                   50\n",
       "├─Linear: 1-2                            [5, 5]                    55\n",
       "├─Linear: 1-3                            [5, 3]                    18\n",
       "==========================================================================================\n",
       "Total params: 123\n",
       "Trainable params: 123\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model,input_size=(5,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "[3] 데이터셋 클래스 설계 및 정의\n",
    "- 데이터셋: iris.csv\n",
    "- feature: 4개\n",
    "- target: 1개\n",
    "- 클래스 이름: iris_ds\n",
    "- 부모 클래스: utils.data.Dataset\n",
    "- 클래스 속성(필드): feature_df,target_df,n_rows,n_features\n",
    "- 필수 메서드: \n",
    "    - _ _ init _ _ (self): 데이터셋 저장 및 전처리, 개발자가 필요한 속성 설정\n",
    "    - _ _ len _ _ (self): 데이터 개수 반환\n",
    "    - _ _ getItem _ _ (self): 특정 인덱스의 feature/target 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iris_dataset(Dataset):\n",
    "    def __init__(self,feature_df,target_df):\n",
    "        self.feature_df=feature_df\n",
    "        self.target_df=target_df\n",
    "\n",
    "        self.n_rows=feature_df.shape[0]\n",
    "        self.n_features=feature_df.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_rows\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        #tensor화\n",
    "        feature_ts=torch.FloatTensor(self.feature_df.iloc[index].values)\n",
    "        target_ts=torch.FloatTensor(self.target_df.iloc[index].values)\n",
    "\n",
    "        #feature/target 반환\n",
    "        return feature_ts,target_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4]) torch.Size([1, 1]) tensor([[5.1000, 3.5000, 1.4000, 0.2000]]) tensor([[0.]])\n"
     ]
    }
   ],
   "source": [
    "#데이터셋 인스턴스 생성\n",
    "\n",
    "feature_df=iris_df[iris_df.columns[:-1]]    #2D (150,3)\n",
    "target_df=iris_df[iris_df.columns[-1:]]     #2D (150,1)\n",
    "\n",
    "iris_ds=iris_dataset(feature_df,target_df)\n",
    "\n",
    "iris_dl=DataLoader(iris_ds)\n",
    "for feature,label in iris_dl:\n",
    "    print(feature.shape,label.shape, feature,label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "[4] 학습 준비\n",
    "- 학습 횟수: EPOCH          #회독 횟수\n",
    "- 배치 크기: BATCH_SIZE     #한번에 학습할 데이터 양\n",
    "- 위치 지정: DEVICE         #텐서 저장 및 실행 위치 (GPU or CPU)\n",
    "- 학습률(LR)\n",
    "    - 가중치와 절편 업데이트 시 경사하강법으로 업데이트 간격 설정\n",
    "    - 0.001 ~ 0.1 사이 값 주로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_CNT: 15\n"
     ]
    }
   ],
   "source": [
    "#학습 진행 관련 설정\n",
    "\n",
    "EPOCH=1000         #처음에 1로 설정해서 잘 돌아가는지 확인하고 올리기\n",
    "BATCH_SIZE=10\n",
    "BATCH_CNT=iris_df.shape[0]//BATCH_SIZE\n",
    "DEVICE='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "LR=0.001\n",
    "\n",
    "print(f'BATCH_CNT: {BATCH_CNT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 인스턴스: 모델, 데이터셋, 최적화, (손실함수, 성능지표)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (84, 4), x_test: (38, 4), x_val: (28, 4)\n",
      "y_train: (84, 1), y_test: (38, 1), y_val: (28, 1)\n",
      "<class 'pandas.core.frame.DataFrame'>, <class 'pandas.core.frame.DataFrame'>, <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#모델 인스턴스\n",
    "model=iris_mcf_model()\n",
    "\n",
    "#데이터셋 인스턴스\n",
    "x_train,x_test,y_train,y_test=train_test_split(feature_df,target_df,random_state=1)\n",
    "x_train,x_val,y_train,y_val=train_test_split(x_train,y_train,random_state=1)\n",
    "print(f'x_train: {x_train.shape}, x_test: {x_test.shape}, x_val: {x_val.shape}')\n",
    "print(f'y_train: {y_train.shape}, y_test: {y_test.shape}, y_val: {y_val.shape}')\n",
    "print(f'{type(x_train)}, {type(x_test)}, {type(x_val)}')\n",
    "\n",
    "#iris_ds=iris_dataset(feature_df,target_df)\n",
    "\n",
    "train_ds=iris_dataset(x_train,y_train)\n",
    "val_ds=iris_dataset(x_val,y_val)\n",
    "test_ds=iris_dataset(x_test,y_test)\n",
    "\n",
    "#최적화 인스턴스\n",
    "iris_dl=DataLoader(train_ds,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최적화, 손실함수 인스턴스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#최적화 인스턴스: model.parameters() 전달\n",
    "optimizer=optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "#손실함수 인스턴스: 다중분류 => CrossEntropyLoss 사용\n",
    "#                             예측값을 선형식 결과 값으로 전달 => AF 처리 X\n",
    "multi_loss=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "[5] 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNT : 9\n",
      "[0/1000]\n",
      "- Train Loss : 1.1150950855678983 Score : 0.173298925989204\n",
      "- Val Loss : 1.0973923206329346 Score : 0.18803419172763824\n",
      "[1/1000]\n",
      "- Train Loss : 1.1081095006730821 Score : 0.173298925989204\n",
      "- Val Loss : 1.098044991493225 Score : 0.18803419172763824\n",
      "[2/1000]\n",
      "- Train Loss : 1.0997542672687106 Score : 0.173298925989204\n",
      "- Val Loss : 1.096265435218811 Score : 0.18803419172763824\n",
      "[3/1000]\n",
      "- Train Loss : 1.0908319685194228 Score : 0.173298925989204\n",
      "- Val Loss : 1.0955742597579956 Score : 0.18803419172763824\n",
      "[4/1000]\n",
      "- Train Loss : 1.0849571625391643 Score : 0.19579988304111692\n",
      "- Val Loss : 1.086546778678894 Score : 0.2716049253940582\n",
      "[5/1000]\n",
      "- Train Loss : 1.073509203063117 Score : 0.22111654529968897\n",
      "- Val Loss : 1.075994849205017 Score : 0.319088339805603\n",
      "[6/1000]\n",
      "- Train Loss : 1.065152022573683 Score : 0.3580995202064514\n",
      "- Val Loss : 1.0687397718429565 Score : 0.319088339805603\n",
      "[7/1000]\n",
      "- Train Loss : 1.0602888266245525 Score : 0.32022875050703686\n",
      "- Val Loss : 1.059598684310913 Score : 0.319088339805603\n",
      "[8/1000]\n",
      "- Train Loss : 1.054573072327508 Score : 0.3327914939986335\n",
      "- Val Loss : 1.0516541004180908 Score : 0.319088339805603\n",
      "[9/1000]\n",
      "- Train Loss : 1.0484558873706393 Score : 0.379333022567961\n",
      "- Val Loss : 1.0432090759277344 Score : 0.319088339805603\n",
      "[10/1000]\n",
      "- Train Loss : 1.041509124967787 Score : 0.3868526650799645\n",
      "- Val Loss : 1.0328162908554077 Score : 0.319088339805603\n",
      "[11/1000]\n",
      "- Train Loss : 1.0349855687883165 Score : 0.3868526650799645\n",
      "- Val Loss : 1.0226993560791016 Score : 0.319088339805603\n",
      "[12/1000]\n",
      "- Train Loss : 1.0273488693767123 Score : 0.3957191076543596\n",
      "- Val Loss : 1.0115641355514526 Score : 0.36350876092910767\n",
      "[13/1000]\n",
      "- Train Loss : 1.0189971923828125 Score : 0.41210518611801994\n",
      "- Val Loss : 0.9990007281303406 Score : 0.36350876092910767\n",
      "[14/1000]\n",
      "- Train Loss : 1.0100155141618516 Score : 0.4236278235912323\n",
      "- Val Loss : 0.9851496815681458 Score : 0.4055555462837219\n",
      "[15/1000]\n",
      "- Train Loss : 1.0006295376353793 Score : 0.4236278235912323\n",
      "- Val Loss : 0.9706124663352966 Score : 0.6003221273422241\n",
      "[16/1000]\n",
      "- Train Loss : 0.9904796812269423 Score : 0.6535380449559953\n",
      "- Val Loss : 0.955169141292572 Score : 0.9484702348709106\n",
      "[17/1000]\n",
      "- Train Loss : 0.9795182281070285 Score : 0.8409278127882216\n",
      "- Val Loss : 0.9384435415267944 Score : 0.9484702348709106\n",
      "[18/1000]\n",
      "- Train Loss : 0.9676879313257005 Score : 0.8409278127882216\n",
      "- Val Loss : 0.9203260540962219 Score : 0.9484702348709106\n",
      "[19/1000]\n",
      "- Train Loss : 0.95529572168986 Score : 0.8409278127882216\n",
      "- Val Loss : 0.9012840986251831 Score : 0.9484702348709106\n",
      "[20/1000]\n",
      "- Train Loss : 0.9423481623331705 Score : 0.8409278127882216\n",
      "- Val Loss : 0.8816583752632141 Score : 0.8888405561447144\n",
      "[21/1000]\n",
      "- Train Loss : 0.9287249909506904 Score : 0.7607690824402703\n",
      "- Val Loss : 0.8614111542701721 Score : 0.8170090913772583\n",
      "[22/1000]\n",
      "- Train Loss : 0.9145417080985175 Score : 0.5988349119822184\n",
      "- Val Loss : 0.8401889801025391 Score : 0.8170090913772583\n",
      "[23/1000]\n",
      "- Train Loss : 0.900188128153483 Score : 0.5511570870876312\n",
      "- Val Loss : 0.818992555141449 Score : 0.726248025894165\n",
      "[24/1000]\n",
      "- Train Loss : 0.8855014244715372 Score : 0.5155309836069742\n",
      "- Val Loss : 0.7971097230911255 Score : 0.6045548915863037\n",
      "[25/1000]\n",
      "- Train Loss : 0.8708234230677286 Score : 0.5167655514346229\n",
      "- Val Loss : 0.7755808234214783 Score : 0.6045548915863037\n",
      "[26/1000]\n",
      "- Train Loss : 0.8555407921473185 Score : 0.5167655514346229\n",
      "- Val Loss : 0.7532859444618225 Score : 0.6045548915863037\n",
      "[27/1000]\n",
      "- Train Loss : 0.8407190905676948 Score : 0.5209181871679094\n",
      "- Val Loss : 0.7316508889198303 Score : 0.6045548915863037\n",
      "[28/1000]\n",
      "- Train Loss : 0.8251566025945876 Score : 0.5209181871679094\n",
      "- Val Loss : 0.7090330123901367 Score : 0.6045548915863037\n",
      "[29/1000]\n",
      "- Train Loss : 0.8102181090248955 Score : 0.5209181871679094\n",
      "- Val Loss : 0.6879686117172241 Score : 0.6045548915863037\n",
      "[30/1000]\n",
      "- Train Loss : 0.7948627207014296 Score : 0.5209181871679094\n",
      "- Val Loss : 0.6675336956977844 Score : 0.6045548915863037\n",
      "[31/1000]\n",
      "- Train Loss : 0.7792987492349412 Score : 0.5209181871679094\n",
      "- Val Loss : 0.6480299830436707 Score : 0.6045548915863037\n",
      "[32/1000]\n",
      "- Train Loss : 0.7636663516362509 Score : 0.5257602598932054\n",
      "- Val Loss : 0.6288822293281555 Score : 0.6045548915863037\n",
      "[33/1000]\n",
      "- Train Loss : 0.748743646674686 Score : 0.5257602598932054\n",
      "- Val Loss : 0.6099266409873962 Score : 0.6018518209457397\n",
      "[34/1000]\n",
      "- Train Loss : 0.7336299618085226 Score : 0.5257602598932054\n",
      "- Val Loss : 0.5907443761825562 Score : 0.6018518209457397\n",
      "[35/1000]\n",
      "- Train Loss : 0.719139403767056 Score : 0.5257602598932054\n",
      "- Val Loss : 0.5716856122016907 Score : 0.6018518209457397\n",
      "[36/1000]\n",
      "- Train Loss : 0.7048645416895548 Score : 0.5257602598932054\n",
      "- Val Loss : 0.5527885556221008 Score : 0.6018518209457397\n",
      "[37/1000]\n",
      "- Train Loss : 0.6915943622589111 Score : 0.5257602598932054\n",
      "- Val Loss : 0.5359250903129578 Score : 0.6018518209457397\n",
      "[38/1000]\n",
      "- Train Loss : 0.6785489122072855 Score : 0.5257602598932054\n",
      "- Val Loss : 0.5198553204536438 Score : 0.6018518209457397\n",
      "[39/1000]\n",
      "- Train Loss : 0.6655913988749186 Score : 0.5257602598932054\n",
      "- Val Loss : 0.5036963820457458 Score : 0.6018518209457397\n",
      "[40/1000]\n",
      "- Train Loss : 0.653926690419515 Score : 0.5155309836069742\n",
      "- Val Loss : 0.48934605717658997 Score : 0.6018518209457397\n",
      "[41/1000]\n",
      "- Train Loss : 0.6417719390657213 Score : 0.5257602598932054\n",
      "- Val Loss : 0.47496670484542847 Score : 0.6018518209457397\n",
      "[42/1000]\n",
      "- Train Loss : 0.6315127544932895 Score : 0.5257602598932054\n",
      "- Val Loss : 0.4629537761211395 Score : 0.6018518209457397\n",
      "[43/1000]\n",
      "- Train Loss : 0.6196200980080498 Score : 0.5167655514346229\n",
      "- Val Loss : 0.44972971081733704 Score : 0.6018518209457397\n",
      "[44/1000]\n",
      "- Train Loss : 0.6097426182693906 Score : 0.5259954200850593\n",
      "- Val Loss : 0.4377900958061218 Score : 0.6018518209457397\n",
      "[45/1000]\n",
      "- Train Loss : 0.5998942454655966 Score : 0.5155309836069742\n",
      "- Val Loss : 0.42670971155166626 Score : 0.6018518209457397\n",
      "[46/1000]\n",
      "- Train Loss : 0.5900122457080417 Score : 0.5400459733274248\n",
      "- Val Loss : 0.4163489043712616 Score : 0.6018518209457397\n",
      "[47/1000]\n",
      "- Train Loss : 0.5812760094801585 Score : 0.5400459733274248\n",
      "- Val Loss : 0.4074031412601471 Score : 0.6018518209457397\n",
      "[48/1000]\n",
      "- Train Loss : 0.5719514555401273 Score : 0.5298166970411936\n",
      "- Val Loss : 0.39810705184936523 Score : 0.6018518209457397\n",
      "[49/1000]\n",
      "- Train Loss : 0.5633655488491058 Score : 0.5606808894210391\n",
      "- Val Loss : 0.38934770226478577 Score : 0.6018518209457397\n",
      "[50/1000]\n",
      "- Train Loss : 0.555215984582901 Score : 0.5606808894210391\n",
      "- Val Loss : 0.38143786787986755 Score : 0.6018518209457397\n",
      "[51/1000]\n",
      "- Train Loss : 0.5471225414011214 Score : 0.6079472104708353\n",
      "- Val Loss : 0.37400293350219727 Score : 0.6018518209457397\n",
      "[52/1000]\n",
      "- Train Loss : 0.5391941037442949 Score : 0.6220565570725335\n",
      "- Val Loss : 0.36687010526657104 Score : 0.726248025894165\n",
      "[53/1000]\n",
      "- Train Loss : 0.5314757095442878 Score : 0.6220565570725335\n",
      "- Val Loss : 0.35999923944473267 Score : 0.8170090913772583\n",
      "[54/1000]\n",
      "- Train Loss : 0.5239838560422262 Score : 0.6577307515674167\n",
      "- Val Loss : 0.35322973132133484 Score : 0.8170090913772583\n",
      "[55/1000]\n",
      "- Train Loss : 0.5168243249257406 Score : 0.6577307515674167\n",
      "- Val Loss : 0.3467862010002136 Score : 0.8170090913772583\n",
      "[56/1000]\n",
      "- Train Loss : 0.5096684098243713 Score : 0.6577307515674167\n",
      "- Val Loss : 0.34028753638267517 Score : 0.8170090913772583\n",
      "[57/1000]\n",
      "- Train Loss : 0.5030773844983842 Score : 0.6825713449054294\n",
      "- Val Loss : 0.33437612652778625 Score : 0.8170090913772583\n",
      "[58/1000]\n",
      "- Train Loss : 0.4962966740131378 Score : 0.711377931965722\n",
      "- Val Loss : 0.32819852232933044 Score : 0.8170090913772583\n",
      "[59/1000]\n",
      "- Train Loss : 0.4902111490567525 Score : 0.711377931965722\n",
      "- Val Loss : 0.32298389077186584 Score : 0.8170090913772583\n",
      "[60/1000]\n",
      "- Train Loss : 0.4839865333504147 Score : 0.711377931965722\n",
      "- Val Loss : 0.3178260028362274 Score : 0.8170090913772583\n",
      "[61/1000]\n",
      "- Train Loss : 0.47783316837416756 Score : 0.7401845190260146\n",
      "- Val Loss : 0.3126428425312042 Score : 0.8170090913772583\n",
      "[62/1000]\n",
      "- Train Loss : 0.47205503119362724 Score : 0.7810855905214945\n",
      "- Val Loss : 0.3079509437084198 Score : 0.8888405561447144\n",
      "[63/1000]\n",
      "- Train Loss : 0.466486437453164 Score : 0.797546492682563\n",
      "- Val Loss : 0.30396413803100586 Score : 0.8888405561447144\n",
      "[64/1000]\n",
      "- Train Loss : 0.4606616795063019 Score : 0.797546492682563\n",
      "- Val Loss : 0.29944562911987305 Score : 0.8888405561447144\n",
      "[65/1000]\n",
      "- Train Loss : 0.45512593785921734 Score : 0.8253242704603407\n",
      "- Val Loss : 0.2948218286037445 Score : 0.8888405561447144\n",
      "[66/1000]\n",
      "- Train Loss : 0.45013097259733414 Score : 0.8397275739245944\n",
      "- Val Loss : 0.29099151492118835 Score : 0.8888405561447144\n",
      "[67/1000]\n",
      "- Train Loss : 0.44489356213145786 Score : 0.8397275739245944\n",
      "- Val Loss : 0.28708580136299133 Score : 0.8888405561447144\n",
      "[68/1000]\n",
      "- Train Loss : 0.439367456568612 Score : 0.8397275739245944\n",
      "- Val Loss : 0.2829502522945404 Score : 0.8888405561447144\n",
      "[69/1000]\n",
      "- Train Loss : 0.43508510788281757 Score : 0.889110287030538\n",
      "- Val Loss : 0.28002145886421204 Score : 0.8888405561447144\n",
      "[70/1000]\n",
      "- Train Loss : 0.42995298239919877 Score : 0.889110287030538\n",
      "- Val Loss : 0.276838093996048 Score : 0.8888405561447144\n",
      "[71/1000]\n",
      "- Train Loss : 0.4248519076241387 Score : 0.9035884009467231\n",
      "- Val Loss : 0.2729547917842865 Score : 0.8888405561447144\n",
      "[72/1000]\n",
      "- Train Loss : 0.42030947075949776 Score : 0.9035884009467231\n",
      "- Val Loss : 0.2692825198173523 Score : 0.8888405561447144\n",
      "[73/1000]\n",
      "- Train Loss : 0.41583242350154453 Score : 0.9035884009467231\n",
      "- Val Loss : 0.2657286822795868 Score : 0.8888405561447144\n",
      "[74/1000]\n",
      "- Train Loss : 0.4114001890023549 Score : 0.9035884009467231\n",
      "- Val Loss : 0.2621583640575409 Score : 0.8888405561447144\n",
      "[75/1000]\n",
      "- Train Loss : 0.4072817431555854 Score : 0.9035884009467231\n",
      "- Val Loss : 0.25885066390037537 Score : 0.8888405561447144\n",
      "[76/1000]\n",
      "- Train Loss : 0.40317536062664455 Score : 0.9035884009467231\n",
      "- Val Loss : 0.2557462751865387 Score : 0.9484702348709106\n",
      "[77/1000]\n",
      "- Train Loss : 0.3990824520587921 Score : 0.9035884009467231\n",
      "- Val Loss : 0.2527768313884735 Score : 0.9484702348709106\n",
      "[78/1000]\n",
      "- Train Loss : 0.3950485156642066 Score : 0.9035884009467231\n",
      "- Val Loss : 0.24989834427833557 Score : 0.9484702348709106\n",
      "[79/1000]\n",
      "- Train Loss : 0.39114657706684536 Score : 0.9151110251744589\n",
      "- Val Loss : 0.2471430003643036 Score : 0.9484702348709106\n",
      "[80/1000]\n",
      "- Train Loss : 0.38731108440293205 Score : 0.9240175551838345\n",
      "- Val Loss : 0.2444925308227539 Score : 0.9484702348709106\n",
      "[81/1000]\n",
      "- Train Loss : 0.3833857046233283 Score : 0.9240175551838345\n",
      "- Val Loss : 0.24173057079315186 Score : 0.9484702348709106\n",
      "[82/1000]\n",
      "- Train Loss : 0.37980738282203674 Score : 0.9240175551838345\n",
      "- Val Loss : 0.23908087611198425 Score : 0.9484702348709106\n",
      "[83/1000]\n",
      "- Train Loss : 0.376158293750551 Score : 0.9240175551838345\n",
      "- Val Loss : 0.23640604317188263 Score : 0.9484702348709106\n",
      "[84/1000]\n",
      "- Train Loss : 0.372713843981425 Score : 0.9240175551838345\n",
      "- Val Loss : 0.23382331430912018 Score : 0.9484702348709106\n",
      "[85/1000]\n",
      "- Train Loss : 0.3692924910121494 Score : 0.9240175551838345\n",
      "- Val Loss : 0.2313384860754013 Score : 0.9484702348709106\n",
      "[86/1000]\n",
      "- Train Loss : 0.3659380012088352 Score : 0.9240175551838345\n",
      "- Val Loss : 0.228970006108284 Score : 0.9484702348709106\n",
      "[87/1000]\n",
      "- Train Loss : 0.36259253488646614 Score : 0.9240175551838345\n",
      "- Val Loss : 0.22668898105621338 Score : 0.9484702348709106\n",
      "[88/1000]\n",
      "- Train Loss : 0.3593376874923706 Score : 0.9240175551838345\n",
      "- Val Loss : 0.22448496520519257 Score : 0.9484702348709106\n",
      "[89/1000]\n",
      "- Train Loss : 0.3561469316482544 Score : 0.9240175551838345\n",
      "- Val Loss : 0.2223450392484665 Score : 0.9484702348709106\n",
      "[90/1000]\n",
      "- Train Loss : 0.35287102063496906 Score : 0.9240175551838345\n",
      "- Val Loss : 0.2202042192220688 Score : 0.9484702348709106\n",
      "[91/1000]\n",
      "- Train Loss : 0.3498682710859511 Score : 0.9240175551838345\n",
      "- Val Loss : 0.2180711328983307 Score : 0.9484702348709106\n",
      "[92/1000]\n",
      "- Train Loss : 0.34679357541932 Score : 0.9240175551838345\n",
      "- Val Loss : 0.21594129502773285 Score : 0.9484702348709106\n",
      "[93/1000]\n",
      "- Train Loss : 0.34405190745989483 Score : 0.9240175551838345\n",
      "- Val Loss : 0.2138110250234604 Score : 0.9484702348709106\n",
      "[94/1000]\n",
      "- Train Loss : 0.3410179300440682 Score : 0.9240175551838345\n",
      "- Val Loss : 0.21181701123714447 Score : 0.9484702348709106\n",
      "[95/1000]\n",
      "- Train Loss : 0.3381764954990811 Score : 0.9240175551838345\n",
      "- Val Loss : 0.20989680290222168 Score : 0.9484702348709106\n",
      "[96/1000]\n",
      "- Train Loss : 0.3354575004842546 Score : 0.9240175551838345\n",
      "- Val Loss : 0.20797650516033173 Score : 0.9484702348709106\n",
      "[97/1000]\n",
      "- Train Loss : 0.3327331311172909 Score : 0.9342739648289151\n",
      "- Val Loss : 0.20610597729682922 Score : 0.9484702348709106\n",
      "[98/1000]\n",
      "- Train Loss : 0.33001670406924355 Score : 0.9342739648289151\n",
      "- Val Loss : 0.20432575047016144 Score : 0.9484702348709106\n",
      "[99/1000]\n",
      "- Train Loss : 0.32734926376077866 Score : 0.9342739648289151\n",
      "- Val Loss : 0.20261426270008087 Score : 0.9484702348709106\n",
      "[100/1000]\n",
      "- Train Loss : 0.32473375068770516 Score : 0.9342739648289151\n",
      "- Val Loss : 0.2009410709142685 Score : 0.9484702348709106\n",
      "[101/1000]\n",
      "- Train Loss : 0.3221546659866969 Score : 0.9342739648289151\n",
      "- Val Loss : 0.19928088784217834 Score : 0.9484702348709106\n",
      "[102/1000]\n",
      "- Train Loss : 0.3196059763431549 Score : 0.9342739648289151\n",
      "- Val Loss : 0.19750788807868958 Score : 0.9484702348709106\n",
      "[103/1000]\n",
      "- Train Loss : 0.3170919401778115 Score : 0.9342739648289151\n",
      "- Val Loss : 0.19575490057468414 Score : 0.9484702348709106\n",
      "[104/1000]\n",
      "- Train Loss : 0.3145192282067405 Score : 0.9506600565380521\n",
      "- Val Loss : 0.19393467903137207 Score : 0.9484702348709106\n",
      "[105/1000]\n",
      "- Train Loss : 0.3122091293334961 Score : 0.9342739648289151\n",
      "- Val Loss : 0.19221165776252747 Score : 0.9484702348709106\n",
      "[106/1000]\n",
      "- Train Loss : 0.3098018434312608 Score : 0.9506600565380521\n",
      "- Val Loss : 0.1905186027288437 Score : 0.9484702348709106\n",
      "[107/1000]\n",
      "- Train Loss : 0.3073907378647063 Score : 0.9593180550469292\n",
      "- Val Loss : 0.18879462778568268 Score : 0.9484702348709106\n",
      "[108/1000]\n",
      "- Train Loss : 0.30512810746828717 Score : 0.9593180550469292\n",
      "- Val Loss : 0.1871330291032791 Score : 0.9484702348709106\n",
      "[109/1000]\n",
      "- Train Loss : 0.3028101060125563 Score : 0.9593180550469292\n",
      "- Val Loss : 0.18544557690620422 Score : 0.9484702348709106\n",
      "[110/1000]\n",
      "- Train Loss : 0.30075638161765206 Score : 0.9593180550469292\n",
      "- Val Loss : 0.1839110404253006 Score : 0.9484702348709106\n",
      "[111/1000]\n",
      "- Train Loss : 0.29842911495102775 Score : 0.9593180550469292\n",
      "- Val Loss : 0.1823122799396515 Score : 0.9484702348709106\n",
      "[112/1000]\n",
      "- Train Loss : 0.2962974574830797 Score : 0.9593180550469292\n",
      "- Val Loss : 0.18075837194919586 Score : 0.9484702348709106\n",
      "[113/1000]\n",
      "- Train Loss : 0.2942495197057724 Score : 0.9593180550469292\n",
      "- Val Loss : 0.1792670637369156 Score : 0.9484702348709106\n",
      "[114/1000]\n",
      "- Train Loss : 0.2921586301591661 Score : 0.9593180550469292\n",
      "- Val Loss : 0.17781023681163788 Score : 0.9484702348709106\n",
      "[115/1000]\n",
      "- Train Loss : 0.2900885161426332 Score : 0.9593180550469292\n",
      "- Val Loss : 0.17637749016284943 Score : 0.9484702348709106\n",
      "[116/1000]\n",
      "- Train Loss : 0.28806296322080827 Score : 0.9593180550469292\n",
      "- Val Loss : 0.17497070133686066 Score : 0.9484702348709106\n",
      "[117/1000]\n",
      "- Train Loss : 0.2860705140564177 Score : 0.9593180550469292\n",
      "- Val Loss : 0.1735890656709671 Score : 0.9484702348709106\n",
      "[118/1000]\n",
      "- Train Loss : 0.28409822781880695 Score : 0.9593180550469292\n",
      "- Val Loss : 0.17222778499126434 Score : 0.9484702348709106\n",
      "[119/1000]\n",
      "- Train Loss : 0.2821472187836965 Score : 0.9593180550469292\n",
      "- Val Loss : 0.1708831787109375 Score : 0.9484702348709106\n",
      "[120/1000]\n",
      "- Train Loss : 0.28022192749712205 Score : 0.9593180550469292\n",
      "- Val Loss : 0.16955389082431793 Score : 0.9484702348709106\n",
      "[121/1000]\n",
      "- Train Loss : 0.2783237198988597 Score : 0.9593180550469292\n",
      "- Val Loss : 0.16823969781398773 Score : 0.9484702348709106\n",
      "[122/1000]\n",
      "- Train Loss : 0.2764505296945572 Score : 0.9593180550469292\n",
      "- Val Loss : 0.16694076359272003 Score : 0.9484702348709106\n",
      "[123/1000]\n",
      "- Train Loss : 0.2746003054910236 Score : 0.9593180550469292\n",
      "- Val Loss : 0.16565649211406708 Score : 0.9484702348709106\n",
      "[124/1000]\n",
      "- Train Loss : 0.2727733817365434 Score : 0.9593180550469292\n",
      "- Val Loss : 0.16438667476177216 Score : 0.9484702348709106\n",
      "[125/1000]\n",
      "- Train Loss : 0.2709702253341675 Score : 0.9593180550469292\n",
      "- Val Loss : 0.16313132643699646 Score : 0.9484702348709106\n",
      "[126/1000]\n",
      "- Train Loss : 0.269190380970637 Score : 0.9593180550469292\n",
      "- Val Loss : 0.1618904322385788 Score : 0.9484702348709106\n",
      "[127/1000]\n",
      "- Train Loss : 0.26743309365378487 Score : 0.9593180550469292\n",
      "- Val Loss : 0.16066396236419678 Score : 0.9484702348709106\n",
      "[128/1000]\n",
      "- Train Loss : 0.26569799251026577 Score : 0.9593180550469292\n",
      "- Val Loss : 0.15945176780223846 Score : 0.9484702348709106\n",
      "[129/1000]\n",
      "- Train Loss : 0.26398487389087677 Score : 0.9593180550469292\n",
      "- Val Loss : 0.1582537144422531 Score : 0.9484702348709106\n",
      "[130/1000]\n",
      "- Train Loss : 0.26229342818260193 Score : 0.9593180550469292\n",
      "- Val Loss : 0.15706981718540192 Score : 0.9484702348709106\n",
      "[131/1000]\n",
      "- Train Loss : 0.2606232911348343 Score : 0.9593180550469292\n",
      "- Val Loss : 0.15589988231658936 Score : 0.9484702348709106\n",
      "[132/1000]\n",
      "- Train Loss : 0.258974078628752 Score : 0.9593180550469292\n",
      "- Val Loss : 0.15474382042884827 Score : 0.9484702348709106\n",
      "[133/1000]\n",
      "- Train Loss : 0.257345500919554 Score : 0.9593180550469292\n",
      "- Val Loss : 0.1536014825105667 Score : 0.9484702348709106\n",
      "[134/1000]\n",
      "- Train Loss : 0.25560515622297925 Score : 0.9593180550469292\n",
      "- Val Loss : 0.1524507999420166 Score : 0.9484702348709106\n",
      "[135/1000]\n",
      "- Train Loss : 0.2542170435190201 Score : 0.9593180550469292\n",
      "- Val Loss : 0.1513345092535019 Score : 0.9484702348709106\n",
      "[136/1000]\n",
      "- Train Loss : 0.2525526483853658 Score : 0.9593180550469292\n",
      "- Val Loss : 0.1502303034067154 Score : 0.9484702348709106\n",
      "[137/1000]\n",
      "- Train Loss : 0.2509610288672977 Score : 0.9593180550469292\n",
      "- Val Loss : 0.14914235472679138 Score : 0.9484702348709106\n",
      "[138/1000]\n",
      "- Train Loss : 0.24964423643218148 Score : 0.9593180550469292\n",
      "- Val Loss : 0.1480848491191864 Score : 0.9484702348709106\n",
      "[139/1000]\n",
      "- Train Loss : 0.2480147745874193 Score : 0.9593180550469292\n",
      "- Val Loss : 0.1470421999692917 Score : 0.9484702348709106\n",
      "[140/1000]\n",
      "- Train Loss : 0.24647698303063711 Score : 0.9593180550469292\n",
      "- Val Loss : 0.1460130214691162 Score : 0.9484702348709106\n",
      "[141/1000]\n",
      "- Train Loss : 0.24509035547574362 Score : 0.9593180550469292\n",
      "- Val Loss : 0.144999161362648 Score : 0.9484702348709106\n",
      "[142/1000]\n",
      "- Train Loss : 0.2436822454134623 Score : 0.9593180550469292\n",
      "- Val Loss : 0.14399829506874084 Score : 0.9484702348709106\n",
      "[143/1000]\n",
      "- Train Loss : 0.24213018516699472 Score : 0.9593180550469292\n",
      "- Val Loss : 0.1430295705795288 Score : 0.9484702348709106\n",
      "[144/1000]\n",
      "- Train Loss : 0.2409093760781818 Score : 0.9593180550469292\n",
      "- Val Loss : 0.14205633103847504 Score : 0.9484702348709106\n",
      "[145/1000]\n",
      "- Train Loss : 0.2394141819741991 Score : 0.9593180550469292\n",
      "- Val Loss : 0.1411198079586029 Score : 0.9484702348709106\n",
      "[146/1000]\n",
      "- Train Loss : 0.23816687034236061 Score : 0.9593180550469292\n",
      "- Val Loss : 0.1401834636926651 Score : 0.9484702348709106\n",
      "[147/1000]\n",
      "- Train Loss : 0.23671097060044607 Score : 0.9593180550469292\n",
      "- Val Loss : 0.13928398489952087 Score : 0.9484702348709106\n",
      "[148/1000]\n",
      "- Train Loss : 0.23539782729413775 Score : 0.9593180550469292\n",
      "- Val Loss : 0.13840189576148987 Score : 0.9484702348709106\n",
      "[149/1000]\n",
      "- Train Loss : 0.23414309322834015 Score : 0.9593180550469292\n",
      "- Val Loss : 0.13751603662967682 Score : 0.9484702348709106\n",
      "[150/1000]\n",
      "- Train Loss : 0.23285559647613102 Score : 0.9593180550469292\n",
      "- Val Loss : 0.13664618134498596 Score : 0.9484702348709106\n",
      "[151/1000]\n",
      "- Train Loss : 0.23157205018732283 Score : 0.9593180550469292\n",
      "- Val Loss : 0.13579696416854858 Score : 0.9484702348709106\n",
      "[152/1000]\n",
      "- Train Loss : 0.23032081458303663 Score : 0.9593180550469292\n",
      "- Val Loss : 0.13495732843875885 Score : 0.9484702348709106\n",
      "[153/1000]\n",
      "- Train Loss : 0.2290740559498469 Score : 0.9593180550469292\n",
      "- Val Loss : 0.13413257896900177 Score : 0.9484702348709106\n",
      "[154/1000]\n",
      "- Train Loss : 0.22785086101955837 Score : 0.9593180550469292\n",
      "- Val Loss : 0.1333175152540207 Score : 0.9484702348709106\n",
      "[155/1000]\n",
      "- Train Loss : 0.22664573126369053 Score : 0.9593180550469292\n",
      "- Val Loss : 0.13250580430030823 Score : 0.9484702348709106\n",
      "[156/1000]\n",
      "- Train Loss : 0.2254367189274894 Score : 0.9593180550469292\n",
      "- Val Loss : 0.13170865178108215 Score : 0.9484702348709106\n",
      "[157/1000]\n",
      "- Train Loss : 0.22425059643056658 Score : 0.9593180550469292\n",
      "- Val Loss : 0.13092146813869476 Score : 0.9484702348709106\n",
      "[158/1000]\n",
      "- Train Loss : 0.22308864030573103 Score : 0.9593180550469292\n",
      "- Val Loss : 0.13013629615306854 Score : 0.9484702348709106\n",
      "[159/1000]\n",
      "- Train Loss : 0.2219200829664866 Score : 0.9593180550469292\n",
      "- Val Loss : 0.1293645203113556 Score : 0.9484702348709106\n",
      "[160/1000]\n",
      "- Train Loss : 0.22077583107683393 Score : 0.9593180550469292\n",
      "- Val Loss : 0.12860170006752014 Score : 0.9484702348709106\n",
      "[161/1000]\n",
      "- Train Loss : 0.21965354846583474 Score : 0.9593180550469292\n",
      "- Val Loss : 0.12784060835838318 Score : 0.9484702348709106\n",
      "[162/1000]\n",
      "- Train Loss : 0.21852360996935102 Score : 0.9593180550469292\n",
      "- Val Loss : 0.12709304690361023 Score : 0.9484702348709106\n",
      "[163/1000]\n",
      "- Train Loss : 0.217417867647277 Score : 0.9593180550469292\n",
      "- Val Loss : 0.12635450065135956 Score : 0.9484702348709106\n",
      "[164/1000]\n",
      "- Train Loss : 0.21633382141590118 Score : 0.9593180550469292\n",
      "- Val Loss : 0.12561742961406708 Score : 0.9484702348709106\n",
      "[165/1000]\n",
      "- Train Loss : 0.215242733558019 Score : 0.9593180550469292\n",
      "- Val Loss : 0.12489373236894608 Score : 0.9484702348709106\n",
      "[166/1000]\n",
      "- Train Loss : 0.2141724775234858 Score : 0.9593180550469292\n",
      "- Val Loss : 0.12417931109666824 Score : 0.9484702348709106\n",
      "[167/1000]\n",
      "- Train Loss : 0.21312376691235435 Score : 0.9593180550469292\n",
      "- Val Loss : 0.12346629053354263 Score : 0.9484702348709106\n",
      "[168/1000]\n",
      "- Train Loss : 0.2120708872874578 Score : 0.9593180550469292\n",
      "- Val Loss : 0.12276563793420792 Score : 0.9484702348709106\n",
      "[169/1000]\n",
      "- Train Loss : 0.2110365778207779 Score : 0.9593180550469292\n",
      "- Val Loss : 0.12207384407520294 Score : 0.9484702348709106\n",
      "[170/1000]\n",
      "- Train Loss : 0.21002299586931863 Score : 0.9593180550469292\n",
      "- Val Loss : 0.1213836744427681 Score : 0.9484702348709106\n",
      "[171/1000]\n",
      "- Train Loss : 0.20900275144312117 Score : 0.9593180550469292\n",
      "- Val Loss : 0.1207062378525734 Score : 0.9484702348709106\n",
      "[172/1000]\n",
      "- Train Loss : 0.20800163514084286 Score : 0.9593180550469292\n",
      "- Val Loss : 0.12003742903470993 Score : 0.9484702348709106\n",
      "[173/1000]\n",
      "- Train Loss : 0.2070240577061971 Score : 0.9593180550469292\n",
      "- Val Loss : 0.11936908960342407 Score : 0.9484702348709106\n",
      "[174/1000]\n",
      "- Train Loss : 0.20603720678223503 Score : 0.9593180550469292\n",
      "- Val Loss : 0.1187131404876709 Score : 0.9484702348709106\n",
      "[175/1000]\n",
      "- Train Loss : 0.2050745975640085 Score : 0.9593180550469292\n",
      "- Val Loss : 0.11806093901395798 Score : 0.9484702348709106\n",
      "[176/1000]\n",
      "- Train Loss : 0.20411151730351978 Score : 0.9593180550469292\n",
      "- Val Loss : 0.11742021888494492 Score : 0.9484702348709106\n",
      "[177/1000]\n",
      "- Train Loss : 0.20316726797156864 Score : 0.9593180550469292\n",
      "- Val Loss : 0.1167866662144661 Score : 0.9484702348709106\n",
      "[178/1000]\n",
      "- Train Loss : 0.20223956058422723 Score : 0.9593180550469292\n",
      "- Val Loss : 0.1161537915468216 Score : 0.9484702348709106\n",
      "[179/1000]\n",
      "- Train Loss : 0.2013055466943317 Score : 0.9593180550469292\n",
      "- Val Loss : 0.11553299427032471 Score : 0.9484702348709106\n",
      "[180/1000]\n",
      "- Train Loss : 0.2003950054446856 Score : 0.9593180550469292\n",
      "- Val Loss : 0.11491533368825912 Score : 0.9484702348709106\n",
      "[181/1000]\n",
      "- Train Loss : 0.19948288218842614 Score : 0.9593180550469292\n",
      "- Val Loss : 0.11430878192186356 Score : 0.9484702348709106\n",
      "[182/1000]\n",
      "- Train Loss : 0.19858841763602364 Score : 0.9593180550469292\n",
      "- Val Loss : 0.11370913684368134 Score : 0.9484702348709106\n",
      "[183/1000]\n",
      "- Train Loss : 0.1977108915646871 Score : 0.9593180550469292\n",
      "- Val Loss : 0.11312579363584518 Score : 0.9484702348709106\n",
      "[184/1000]\n",
      "- Train Loss : 0.1968245580792427 Score : 0.9593180550469292\n",
      "- Val Loss : 0.11254911869764328 Score : 0.9484702348709106\n",
      "[185/1000]\n",
      "- Train Loss : 0.19596116493145624 Score : 0.9593180550469292\n",
      "- Val Loss : 0.11197976022958755 Score : 0.9484702348709106\n",
      "[186/1000]\n",
      "- Train Loss : 0.195098538365629 Score : 0.9593180550469292\n",
      "- Val Loss : 0.11141461133956909 Score : 0.9484702348709106\n",
      "[187/1000]\n",
      "- Train Loss : 0.19425124178330103 Score : 0.9593180550469292\n",
      "- Val Loss : 0.11085677891969681 Score : 0.9484702348709106\n",
      "[188/1000]\n",
      "- Train Loss : 0.19335701730516222 Score : 0.9593180550469292\n",
      "- Val Loss : 0.11028017103672028 Score : 0.9484702348709106\n",
      "[189/1000]\n",
      "- Train Loss : 0.1925880586107572 Score : 0.9593180550469292\n",
      "- Val Loss : 0.10972286015748978 Score : 0.9484702348709106\n",
      "[190/1000]\n",
      "- Train Loss : 0.19177523420916665 Score : 0.9593180550469292\n",
      "- Val Loss : 0.10918822139501572 Score : 0.9484702348709106\n",
      "[191/1000]\n",
      "- Train Loss : 0.1909726137916247 Score : 0.9593180550469292\n",
      "- Val Loss : 0.10865737497806549 Score : 0.9484702348709106\n",
      "[192/1000]\n",
      "- Train Loss : 0.19014395111136967 Score : 0.9593180550469292\n",
      "- Val Loss : 0.1081329733133316 Score : 0.9484702348709106\n",
      "[193/1000]\n",
      "- Train Loss : 0.18937242527802786 Score : 0.9593180550469292\n",
      "- Val Loss : 0.10761569440364838 Score : 0.9484702348709106\n",
      "[194/1000]\n",
      "- Train Loss : 0.18858618206448025 Score : 0.9593180550469292\n",
      "- Val Loss : 0.10710929334163666 Score : 0.9484702348709106\n",
      "[195/1000]\n",
      "- Train Loss : 0.18774395601616967 Score : 0.9593180550469292\n",
      "- Val Loss : 0.10659042745828629 Score : 0.9484702348709106\n",
      "[196/1000]\n",
      "- Train Loss : 0.1870981471406089 Score : 0.9593180550469292\n",
      "- Val Loss : 0.10609991103410721 Score : 0.9484702348709106\n",
      "[197/1000]\n",
      "- Train Loss : 0.18632077260149849 Score : 0.9593180550469292\n",
      "- Val Loss : 0.10562276095151901 Score : 0.9484702348709106\n",
      "[198/1000]\n",
      "- Train Loss : 0.18546615623765522 Score : 0.9593180550469292\n",
      "- Val Loss : 0.10511904954910278 Score : 0.9484702348709106\n",
      "[199/1000]\n",
      "- Train Loss : 0.18476265751653248 Score : 0.9593180550469292\n",
      "- Val Loss : 0.1046358272433281 Score : 0.9484702348709106\n",
      "[200/1000]\n",
      "- Train Loss : 0.18407494492001003 Score : 0.9593180550469292\n",
      "- Val Loss : 0.10415951162576675 Score : 0.9484702348709106\n",
      "[201/1000]\n",
      "- Train Loss : 0.1833456183473269 Score : 0.9593180550469292\n",
      "- Val Loss : 0.10369408130645752 Score : 0.9484702348709106\n",
      "[202/1000]\n",
      "- Train Loss : 0.1826030065615972 Score : 0.9593180550469292\n",
      "- Val Loss : 0.1032324880361557 Score : 0.9484702348709106\n",
      "[203/1000]\n",
      "- Train Loss : 0.1818964026040501 Score : 0.9593180550469292\n",
      "- Val Loss : 0.10277669876813889 Score : 0.9484702348709106\n",
      "[204/1000]\n",
      "- Train Loss : 0.18119751579231685 Score : 0.9593180550469292\n",
      "- Val Loss : 0.1023266389966011 Score : 0.9484702348709106\n",
      "[205/1000]\n",
      "- Train Loss : 0.1804943217171563 Score : 0.9593180550469292\n",
      "- Val Loss : 0.10188174247741699 Score : 0.9484702348709106\n",
      "[206/1000]\n",
      "- Train Loss : 0.17981071852975422 Score : 0.9593180550469292\n",
      "- Val Loss : 0.10144149512052536 Score : 0.9484702348709106\n",
      "[207/1000]\n",
      "- Train Loss : 0.17912058201101091 Score : 0.9593180550469292\n",
      "- Val Loss : 0.10100562870502472 Score : 0.9484702348709106\n",
      "[208/1000]\n",
      "- Train Loss : 0.1784494130147828 Score : 0.9593180550469292\n",
      "- Val Loss : 0.10057373344898224 Score : 0.9484702348709106\n",
      "[209/1000]\n",
      "- Train Loss : 0.17777263621489206 Score : 0.9593180550469292\n",
      "- Val Loss : 0.10014598816633224 Score : 0.9484702348709106\n",
      "[210/1000]\n",
      "- Train Loss : 0.17711512909995186 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0997220128774643 Score : 0.9484702348709106\n",
      "[211/1000]\n",
      "- Train Loss : 0.17645166565974554 Score : 0.9593180550469292\n",
      "- Val Loss : 0.09930216521024704 Score : 0.9484702348709106\n",
      "[212/1000]\n",
      "- Train Loss : 0.17580706543392605 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0988859012722969 Score : 0.9484702348709106\n",
      "[213/1000]\n",
      "- Train Loss : 0.17516175905863443 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0984729453921318 Score : 0.9484702348709106\n",
      "[214/1000]\n",
      "- Train Loss : 0.17451550563176474 Score : 0.9593180550469292\n",
      "- Val Loss : 0.09806405752897263 Score : 0.9484702348709106\n",
      "[215/1000]\n",
      "- Train Loss : 0.17389158573415545 Score : 0.9593180550469292\n",
      "- Val Loss : 0.09765885770320892 Score : 0.9484702348709106\n",
      "[216/1000]\n",
      "- Train Loss : 0.17326133532656562 Score : 0.9593180550469292\n",
      "- Val Loss : 0.09725804626941681 Score : 0.9484702348709106\n",
      "[217/1000]\n",
      "- Train Loss : 0.17264713760879305 Score : 0.9593180550469292\n",
      "- Val Loss : 0.09686049073934555 Score : 0.9484702348709106\n",
      "[218/1000]\n",
      "- Train Loss : 0.17202679150634342 Score : 0.9593180550469292\n",
      "- Val Loss : 0.09646717458963394 Score : 0.9484702348709106\n",
      "[219/1000]\n",
      "- Train Loss : 0.1714247763156891 Score : 0.9593180550469292\n",
      "- Val Loss : 0.09607703983783722 Score : 0.9484702348709106\n",
      "[220/1000]\n",
      "- Train Loss : 0.17081703162855572 Score : 0.9593180550469292\n",
      "- Val Loss : 0.09569109976291656 Score : 0.9484702348709106\n",
      "[221/1000]\n",
      "- Train Loss : 0.1702268487877316 Score : 0.9593180550469292\n",
      "- Val Loss : 0.09530825167894363 Score : 0.9484702348709106\n",
      "[222/1000]\n",
      "- Train Loss : 0.16963544405168957 Score : 0.9593180550469292\n",
      "- Val Loss : 0.09492846578359604 Score : 0.9484702348709106\n",
      "[223/1000]\n",
      "- Train Loss : 0.16904268744919035 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0945529192686081 Score : 0.9484702348709106\n",
      "[224/1000]\n",
      "- Train Loss : 0.1684709439675013 Score : 0.9593180550469292\n",
      "- Val Loss : 0.09418042749166489 Score : 0.9484702348709106\n",
      "[225/1000]\n",
      "- Train Loss : 0.16789267626073626 Score : 0.9593180550469292\n",
      "- Val Loss : 0.09381213039159775 Score : 0.9484702348709106\n",
      "[226/1000]\n",
      "- Train Loss : 0.16732958207527796 Score : 0.9593180550469292\n",
      "- Val Loss : 0.09344667941331863 Score : 0.9484702348709106\n",
      "[227/1000]\n",
      "- Train Loss : 0.16675999760627747 Score : 0.9593180550469292\n",
      "- Val Loss : 0.09308534115552902 Score : 0.9484702348709106\n",
      "[228/1000]\n",
      "- Train Loss : 0.1662077084183693 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0927266925573349 Score : 0.9484702348709106\n",
      "[229/1000]\n",
      "- Train Loss : 0.16565452764431635 Score : 0.9593180550469292\n",
      "- Val Loss : 0.09237086772918701 Score : 0.9484702348709106\n",
      "[230/1000]\n",
      "- Train Loss : 0.1650996067457729 Score : 0.9593180550469292\n",
      "- Val Loss : 0.09201931208372116 Score : 0.9484702348709106\n",
      "[231/1000]\n",
      "- Train Loss : 0.16456446962224114 Score : 0.9593180550469292\n",
      "- Val Loss : 0.09167031943798065 Score : 0.9484702348709106\n",
      "[232/1000]\n",
      "- Train Loss : 0.16402280579010645 Score : 0.9593180550469292\n",
      "- Val Loss : 0.09132544696331024 Score : 0.9484702348709106\n",
      "[233/1000]\n",
      "- Train Loss : 0.16349555138084623 Score : 0.9593180550469292\n",
      "- Val Loss : 0.09098292887210846 Score : 0.9484702348709106\n",
      "[234/1000]\n",
      "- Train Loss : 0.16296185718642342 Score : 0.9593180550469292\n",
      "- Val Loss : 0.090644471347332 Score : 0.9484702348709106\n",
      "[235/1000]\n",
      "- Train Loss : 0.16244453771246803 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0903083011507988 Score : 0.9484702348709106\n",
      "[236/1000]\n",
      "- Train Loss : 0.16192609320084253 Score : 0.9593180550469292\n",
      "- Val Loss : 0.08997470885515213 Score : 0.9484702348709106\n",
      "[237/1000]\n",
      "- Train Loss : 0.16140619168678919 Score : 0.9593180550469292\n",
      "- Val Loss : 0.08964542299509048 Score : 0.9484702348709106\n",
      "[238/1000]\n",
      "- Train Loss : 0.16090467241075304 Score : 0.9593180550469292\n",
      "- Val Loss : 0.08931832015514374 Score : 0.9484702348709106\n",
      "[239/1000]\n",
      "- Train Loss : 0.16039688802427715 Score : 0.9593180550469292\n",
      "- Val Loss : 0.08899521082639694 Score : 0.9484702348709106\n",
      "[240/1000]\n",
      "- Train Loss : 0.1599019608563847 Score : 0.9593180550469292\n",
      "- Val Loss : 0.08867417275905609 Score : 0.9484702348709106\n",
      "[241/1000]\n",
      "- Train Loss : 0.15940579026937485 Score : 0.9593180550469292\n",
      "- Val Loss : 0.08835555613040924 Score : 0.9484702348709106\n",
      "[242/1000]\n",
      "- Train Loss : 0.15890899880064857 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0880412682890892 Score : 0.9484702348709106\n",
      "[243/1000]\n",
      "- Train Loss : 0.1584302360812823 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0877288207411766 Score : 0.9484702348709106\n",
      "[244/1000]\n",
      "- Train Loss : 0.15794429928064346 Score : 0.9593180550469292\n",
      "- Val Loss : 0.08742028474807739 Score : 0.9484702348709106\n",
      "[245/1000]\n",
      "- Train Loss : 0.15747137616078058 Score : 0.9593180550469292\n",
      "- Val Loss : 0.08711346238851547 Score : 0.9484702348709106\n",
      "[246/1000]\n",
      "- Train Loss : 0.15699730234013665 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0868089571595192 Score : 0.9484702348709106\n",
      "[247/1000]\n",
      "- Train Loss : 0.15652216556999418 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0865086168050766 Score : 0.9484702348709106\n",
      "[248/1000]\n",
      "- Train Loss : 0.1560637661152416 Score : 0.9593180550469292\n",
      "- Val Loss : 0.08620978891849518 Score : 0.9484702348709106\n",
      "[249/1000]\n",
      "- Train Loss : 0.15560439063443077 Score : 0.9593180550469292\n",
      "- Val Loss : 0.08591317385435104 Score : 0.9484702348709106\n",
      "[250/1000]\n",
      "- Train Loss : 0.15514038420385784 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0856207087635994 Score : 0.9484702348709106\n",
      "[251/1000]\n",
      "- Train Loss : 0.15469474097092947 Score : 0.9593180550469292\n",
      "- Val Loss : 0.08532978594303131 Score : 0.9484702348709106\n",
      "[252/1000]\n",
      "- Train Loss : 0.15424115707476935 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0850425735116005 Score : 0.9484702348709106\n",
      "[253/1000]\n",
      "- Train Loss : 0.15380146520005333 Score : 0.9593180550469292\n",
      "- Val Loss : 0.08475670963525772 Score : 0.9484702348709106\n",
      "[254/1000]\n",
      "- Train Loss : 0.15336071037583882 Score : 0.9593180550469292\n",
      "- Val Loss : 0.08447297662496567 Score : 0.9484702348709106\n",
      "[255/1000]\n",
      "- Train Loss : 0.15291673441727957 Score : 0.9593180550469292\n",
      "- Val Loss : 0.08419348299503326 Score : 0.9484702348709106\n",
      "[256/1000]\n",
      "- Train Loss : 0.15248923914300072 Score : 0.9593180550469292\n",
      "- Val Loss : 0.08391518145799637 Score : 0.9484702348709106\n",
      "[257/1000]\n",
      "- Train Loss : 0.15206072479486465 Score : 0.9593180550469292\n",
      "- Val Loss : 0.08363871276378632 Score : 0.9484702348709106\n",
      "[258/1000]\n",
      "- Train Loss : 0.1516292691230774 Score : 0.9593180550469292\n",
      "- Val Loss : 0.08336635679006577 Score : 0.9484702348709106\n",
      "[259/1000]\n",
      "- Train Loss : 0.15121306892898348 Score : 0.9593180550469292\n",
      "- Val Loss : 0.08309520035982132 Score : 0.9484702348709106\n",
      "[260/1000]\n",
      "- Train Loss : 0.15079394893513787 Score : 0.9593180550469292\n",
      "- Val Loss : 0.08282583206892014 Score : 0.9484702348709106\n",
      "[261/1000]\n",
      "- Train Loss : 0.1503725548585256 Score : 0.9593180550469292\n",
      "- Val Loss : 0.08256063610315323 Score : 0.9484702348709106\n",
      "[262/1000]\n",
      "- Train Loss : 0.14996845606300566 Score : 0.9593180550469292\n",
      "- Val Loss : 0.08229629695415497 Score : 0.9484702348709106\n",
      "[263/1000]\n",
      "- Train Loss : 0.14956100036700568 Score : 0.9593180550469292\n",
      "- Val Loss : 0.08203380554914474 Score : 0.9484702348709106\n",
      "[264/1000]\n",
      "- Train Loss : 0.14914888971381718 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0817754939198494 Score : 0.9484702348709106\n",
      "[265/1000]\n",
      "- Train Loss : 0.1487562366657787 Score : 0.9593180550469292\n",
      "- Val Loss : 0.08151747286319733 Score : 0.9484702348709106\n",
      "[266/1000]\n",
      "- Train Loss : 0.1483521411816279 Score : 0.9593180550469292\n",
      "- Val Loss : 0.08126328140497208 Score : 0.9484702348709106\n",
      "[267/1000]\n",
      "- Train Loss : 0.14796482109361225 Score : 0.9593180550469292\n",
      "- Val Loss : 0.08100950717926025 Score : 0.9484702348709106\n",
      "[268/1000]\n",
      "- Train Loss : 0.14756815301047432 Score : 0.9593180550469292\n",
      "- Val Loss : 0.08075917512178421 Score : 0.9484702348709106\n",
      "[269/1000]\n",
      "- Train Loss : 0.14718081967698204 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0805114358663559 Score : 0.9484702348709106\n",
      "[270/1000]\n",
      "- Train Loss : 0.14680270105600357 Score : 0.9593180550469292\n",
      "- Val Loss : 0.08026355504989624 Score : 0.9484702348709106\n",
      "[271/1000]\n",
      "- Train Loss : 0.14641502913501528 Score : 0.9593180550469292\n",
      "- Val Loss : 0.08001946657896042 Score : 0.9484702348709106\n",
      "[272/1000]\n",
      "- Train Loss : 0.14603576809167862 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07977808266878128 Score : 0.9484702348709106\n",
      "[273/1000]\n",
      "- Train Loss : 0.14566883362001842 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07953635603189468 Score : 0.9484702348709106\n",
      "[274/1000]\n",
      "- Train Loss : 0.145290436844031 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07929807156324387 Score : 0.9484702348709106\n",
      "[275/1000]\n",
      "- Train Loss : 0.14492117613554 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07906240969896317 Score : 0.9484702348709106\n",
      "[276/1000]\n",
      "- Train Loss : 0.144557386636734 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07882855832576752 Score : 0.9484702348709106\n",
      "[277/1000]\n",
      "- Train Loss : 0.1441991792784797 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07859466224908829 Score : 0.9484702348709106\n",
      "[278/1000]\n",
      "- Train Loss : 0.1438329741358757 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07836432009935379 Score : 0.9484702348709106\n",
      "[279/1000]\n",
      "- Train Loss : 0.14347610539860195 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07813649624586105 Score : 0.9484702348709106\n",
      "[280/1000]\n",
      "- Train Loss : 0.14312420785427094 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07791037857532501 Score : 0.9484702348709106\n",
      "[281/1000]\n",
      "- Train Loss : 0.14277748266855875 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07768408209085464 Score : 0.9484702348709106\n",
      "[282/1000]\n",
      "- Train Loss : 0.1424229856994417 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07746146619319916 Score : 0.9484702348709106\n",
      "[283/1000]\n",
      "- Train Loss : 0.1420770146780544 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07724123448133469 Score : 0.9484702348709106\n",
      "[284/1000]\n",
      "- Train Loss : 0.14173602023058468 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07702253758907318 Score : 0.9484702348709106\n",
      "[285/1000]\n",
      "- Train Loss : 0.14140215184953478 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07680339366197586 Score : 0.9484702348709106\n",
      "[286/1000]\n",
      "- Train Loss : 0.1410575724310345 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07658790796995163 Score : 0.9484702348709106\n",
      "[287/1000]\n",
      "- Train Loss : 0.14072274913390478 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07637479156255722 Score : 0.9484702348709106\n",
      "[288/1000]\n",
      "- Train Loss : 0.14039727259013388 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07616110146045685 Score : 0.9484702348709106\n",
      "[289/1000]\n",
      "- Train Loss : 0.14006272703409195 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07595083862543106 Score : 0.9484702348709106\n",
      "[290/1000]\n",
      "- Train Loss : 0.13973486382100317 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07574283331632614 Score : 0.9484702348709106\n",
      "[291/1000]\n",
      "- Train Loss : 0.1394141929017173 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07553597539663315 Score : 0.9484702348709106\n",
      "[292/1000]\n",
      "- Train Loss : 0.13909700637062392 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07532832771539688 Score : 0.9484702348709106\n",
      "[293/1000]\n",
      "- Train Loss : 0.1387705546286371 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07512479275465012 Score : 0.9484702348709106\n",
      "[294/1000]\n",
      "- Train Loss : 0.13845369882053798 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07492364197969437 Score : 0.9484702348709106\n",
      "[295/1000]\n",
      "- Train Loss : 0.13814320415258408 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07472336292266846 Score : 0.9484702348709106\n",
      "[296/1000]\n",
      "- Train Loss : 0.13783282993568313 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07452402263879776 Score : 0.9484702348709106\n",
      "[297/1000]\n",
      "- Train Loss : 0.13752254098653793 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07432635128498077 Score : 0.9484702348709106\n",
      "[298/1000]\n",
      "- Train Loss : 0.13721544212765163 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07413043826818466 Score : 0.9484702348709106\n",
      "[299/1000]\n",
      "- Train Loss : 0.13691095428334343 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0739361122250557 Score : 0.9484702348709106\n",
      "[300/1000]\n",
      "- Train Loss : 0.13660850665635532 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07374335080385208 Score : 0.9484702348709106\n",
      "[301/1000]\n",
      "- Train Loss : 0.13630958729320103 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0735519751906395 Score : 0.9484702348709106\n",
      "[302/1000]\n",
      "- Train Loss : 0.13601231740580666 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0733620747923851 Score : 0.9484702348709106\n",
      "[303/1000]\n",
      "- Train Loss : 0.13571644036306274 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07317379862070084 Score : 0.9484702348709106\n",
      "[304/1000]\n",
      "- Train Loss : 0.13542412138647503 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07298692315816879 Score : 0.9484702348709106\n",
      "[305/1000]\n",
      "- Train Loss : 0.1351336133148935 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07280147075653076 Score : 0.9484702348709106\n",
      "[306/1000]\n",
      "- Train Loss : 0.13484451961186197 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0726175457239151 Score : 0.9484702348709106\n",
      "[307/1000]\n",
      "- Train Loss : 0.1345588755276468 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07243501394987106 Score : 0.9484702348709106\n",
      "[308/1000]\n",
      "- Train Loss : 0.13427440490987566 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07225388288497925 Score : 0.9484702348709106\n",
      "[309/1000]\n",
      "- Train Loss : 0.1339930006199413 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07207412272691727 Score : 0.9484702348709106\n",
      "[310/1000]\n",
      "- Train Loss : 0.13371315225958824 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0718957707285881 Score : 0.9484702348709106\n",
      "[311/1000]\n",
      "- Train Loss : 0.13343523856666353 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07171889394521713 Score : 0.9484702348709106\n",
      "[312/1000]\n",
      "- Train Loss : 0.1331633138987753 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07154116779565811 Score : 0.9484702348709106\n",
      "[313/1000]\n",
      "- Train Loss : 0.13288502974642646 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07136671990156174 Score : 0.9484702348709106\n",
      "[314/1000]\n",
      "- Train Loss : 0.13261266632212532 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07119434326887131 Score : 0.9484702348709106\n",
      "[315/1000]\n",
      "- Train Loss : 0.13234470163782439 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07102300971746445 Score : 0.9484702348709106\n",
      "[316/1000]\n",
      "- Train Loss : 0.13207821796337763 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07085270434617996 Score : 0.9484702348709106\n",
      "[317/1000]\n",
      "- Train Loss : 0.1318125468161371 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07068373262882233 Score : 0.9484702348709106\n",
      "[318/1000]\n",
      "- Train Loss : 0.13154813481701744 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07051622122526169 Score : 0.9484702348709106\n",
      "[319/1000]\n",
      "- Train Loss : 0.13128706937034926 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07034994661808014 Score : 0.9484702348709106\n",
      "[320/1000]\n",
      "- Train Loss : 0.1310276264945666 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07018483430147171 Score : 0.9484702348709106\n",
      "[321/1000]\n",
      "- Train Loss : 0.13076974120404986 Score : 0.9593180550469292\n",
      "- Val Loss : 0.07002106308937073 Score : 0.9484702348709106\n",
      "[322/1000]\n",
      "- Train Loss : 0.13051368254754278 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0698585957288742 Score : 0.9484702348709106\n",
      "[323/1000]\n",
      "- Train Loss : 0.1302596496211158 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0696972981095314 Score : 0.9484702348709106\n",
      "[324/1000]\n",
      "- Train Loss : 0.13001061148113674 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06953506916761398 Score : 0.9484702348709106\n",
      "[325/1000]\n",
      "- Train Loss : 0.12975532023443115 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0693761333823204 Score : 0.9484702348709106\n",
      "[326/1000]\n",
      "- Train Loss : 0.1295065792898337 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06921888887882233 Score : 0.9484702348709106\n",
      "[327/1000]\n",
      "- Train Loss : 0.1292617085079352 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06906235218048096 Score : 0.9484702348709106\n",
      "[328/1000]\n",
      "- Train Loss : 0.12901760679152277 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06890670955181122 Score : 0.9484702348709106\n",
      "[329/1000]\n",
      "- Train Loss : 0.12877409408489862 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06875229626893997 Score : 0.9484702348709106\n",
      "[330/1000]\n",
      "- Train Loss : 0.12853215096725357 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06859923899173737 Score : 0.9484702348709106\n",
      "[331/1000]\n",
      "- Train Loss : 0.12829234906368786 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06844732910394669 Score : 0.9484702348709106\n",
      "[332/1000]\n",
      "- Train Loss : 0.1280544681681527 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06829655915498734 Score : 0.9484702348709106\n",
      "[333/1000]\n",
      "- Train Loss : 0.1278212885889742 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06814470142126083 Score : 0.9484702348709106\n",
      "[334/1000]\n",
      "- Train Loss : 0.12758176608218086 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06799604743719101 Score : 0.9484702348709106\n",
      "[335/1000]\n",
      "- Train Loss : 0.1273484941985872 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06784903258085251 Score : 0.9484702348709106\n",
      "[336/1000]\n",
      "- Train Loss : 0.1271189364294211 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06770265102386475 Score : 0.9484702348709106\n",
      "[337/1000]\n",
      "- Train Loss : 0.12689010426402092 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06755698472261429 Score : 0.9484702348709106\n",
      "[338/1000]\n",
      "- Train Loss : 0.12666171996129882 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06741250306367874 Score : 0.9484702348709106\n",
      "[339/1000]\n",
      "- Train Loss : 0.12643475416633818 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06726926565170288 Score : 0.9484702348709106\n",
      "[340/1000]\n",
      "- Train Loss : 0.12620973214507103 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06712712347507477 Score : 0.9484702348709106\n",
      "[341/1000]\n",
      "- Train Loss : 0.12598653882741928 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06698597967624664 Score : 0.9484702348709106\n",
      "[342/1000]\n",
      "- Train Loss : 0.1257682608233558 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06684355437755585 Score : 0.9484702348709106\n",
      "[343/1000]\n",
      "- Train Loss : 0.12554268331991303 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06670451909303665 Score : 0.9484702348709106\n",
      "[344/1000]\n",
      "- Train Loss : 0.1253235824406147 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0665670782327652 Score : 0.9484702348709106\n",
      "[345/1000]\n",
      "- Train Loss : 0.12510830867621633 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06643008440732956 Score : 0.9484702348709106\n",
      "[346/1000]\n",
      "- Train Loss : 0.12489361274573538 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0662936121225357 Score : 0.9484702348709106\n",
      "[347/1000]\n",
      "- Train Loss : 0.12467872144447432 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06615828722715378 Score : 0.9484702348709106\n",
      "[348/1000]\n",
      "- Train Loss : 0.1244659957786401 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06602394580841064 Score : 0.9484702348709106\n",
      "[349/1000]\n",
      "- Train Loss : 0.12425795073310535 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06588844209909439 Score : 0.9484702348709106\n",
      "[350/1000]\n",
      "- Train Loss : 0.12404289717475574 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06575623154640198 Score : 0.9484702348709106\n",
      "[351/1000]\n",
      "- Train Loss : 0.12383414142661625 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06562557071447372 Score : 0.9484702348709106\n",
      "[352/1000]\n",
      "- Train Loss : 0.12362899175948566 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06549528241157532 Score : 0.9484702348709106\n",
      "[353/1000]\n",
      "- Train Loss : 0.12342443234390682 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06536547839641571 Score : 0.9484702348709106\n",
      "[354/1000]\n",
      "- Train Loss : 0.123220002071725 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06523668766021729 Score : 0.9484702348709106\n",
      "[355/1000]\n",
      "- Train Loss : 0.1230166537894143 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0651090145111084 Score : 0.9484702348709106\n",
      "[356/1000]\n",
      "- Train Loss : 0.12281466275453568 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06498237699270248 Score : 0.9484702348709106\n",
      "[357/1000]\n",
      "- Train Loss : 0.12261851504445076 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0648542121052742 Score : 0.9484702348709106\n",
      "[358/1000]\n",
      "- Train Loss : 0.12241488115655051 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06472919136285782 Score : 0.9484702348709106\n",
      "[359/1000]\n",
      "- Train Loss : 0.12221694489320119 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06460573524236679 Score : 0.9484702348709106\n",
      "[360/1000]\n",
      "- Train Loss : 0.122022471908066 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06448265165090561 Score : 0.9484702348709106\n",
      "[361/1000]\n",
      "- Train Loss : 0.12182929780748156 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0643598809838295 Score : 0.9484702348709106\n",
      "[362/1000]\n",
      "- Train Loss : 0.12163561003075705 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06423801928758621 Score : 0.9484702348709106\n",
      "[363/1000]\n",
      "- Train Loss : 0.1214425659014119 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06411737948656082 Score : 0.9484702348709106\n",
      "[364/1000]\n",
      "- Train Loss : 0.12125453932417764 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06399543583393097 Score : 0.9484702348709106\n",
      "[365/1000]\n",
      "- Train Loss : 0.12106003653672007 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0638766810297966 Score : 0.9484702348709106\n",
      "[366/1000]\n",
      "- Train Loss : 0.12087148966060744 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06375934183597565 Score : 0.9484702348709106\n",
      "[367/1000]\n",
      "- Train Loss : 0.12068608692950672 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06364215910434723 Score : 0.9484702348709106\n",
      "[368/1000]\n",
      "- Train Loss : 0.12050110267268287 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06352542340755463 Score : 0.9484702348709106\n",
      "[369/1000]\n",
      "- Train Loss : 0.1203193027112219 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06340716779232025 Score : 0.9484702348709106\n",
      "[370/1000]\n",
      "- Train Loss : 0.12013045408659512 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06329235434532166 Score : 0.9484702348709106\n",
      "[371/1000]\n",
      "- Train Loss : 0.11994755930370754 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0631791278719902 Score : 0.9484702348709106\n",
      "[372/1000]\n",
      "- Train Loss : 0.1197682912978861 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0630660206079483 Score : 0.9484702348709106\n",
      "[373/1000]\n",
      "- Train Loss : 0.1195894508726067 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06295454502105713 Score : 0.9484702348709106\n",
      "[374/1000]\n",
      "- Train Loss : 0.11941047757863998 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06284432113170624 Score : 0.9484702348709106\n",
      "[375/1000]\n",
      "- Train Loss : 0.11923548082510631 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06273125857114792 Score : 0.9484702348709106\n",
      "[376/1000]\n",
      "- Train Loss : 0.11905400082468987 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06262385845184326 Score : 0.9484702348709106\n",
      "[377/1000]\n",
      "- Train Loss : 0.1188783273100853 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06251832842826843 Score : 0.9484702348709106\n",
      "[378/1000]\n",
      "- Train Loss : 0.11870591135488616 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06241210550069809 Score : 0.9484702348709106\n",
      "[379/1000]\n",
      "- Train Loss : 0.11853383605678876 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06230555847287178 Score : 0.9484702348709106\n",
      "[380/1000]\n",
      "- Train Loss : 0.11836470580763286 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06219581142067909 Score : 0.9484702348709106\n",
      "[381/1000]\n",
      "- Train Loss : 0.11818860760993427 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06209191307425499 Score : 0.9484702348709106\n",
      "[382/1000]\n",
      "- Train Loss : 0.1180181900660197 Score : 0.9593180550469292\n",
      "- Val Loss : 0.061990056186914444 Score : 0.9484702348709106\n",
      "[383/1000]\n",
      "- Train Loss : 0.11785118737154537 Score : 0.9593180550469292\n",
      "- Val Loss : 0.061887580901384354 Score : 0.9484702348709106\n",
      "[384/1000]\n",
      "- Train Loss : 0.11768464785483149 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06178462877869606 Score : 0.9484702348709106\n",
      "[385/1000]\n",
      "- Train Loss : 0.11752090520328945 Score : 0.9593180550469292\n",
      "- Val Loss : 0.061678461730480194 Score : 0.9484702348709106\n",
      "[386/1000]\n",
      "- Train Loss : 0.11735018425517613 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06157802417874336 Score : 0.9484702348709106\n",
      "[387/1000]\n",
      "- Train Loss : 0.11718536002768411 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06147948279976845 Score : 0.9484702348709106\n",
      "[388/1000]\n",
      "- Train Loss : 0.11702352265516917 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06138032674789429 Score : 0.9484702348709106\n",
      "[389/1000]\n",
      "- Train Loss : 0.11686201766133308 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06128089874982834 Score : 0.9484702348709106\n",
      "[390/1000]\n",
      "- Train Loss : 0.11670344322919846 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06117817386984825 Score : 0.9484702348709106\n",
      "[391/1000]\n",
      "- Train Loss : 0.11653794886337386 Score : 0.9593180550469292\n",
      "- Val Loss : 0.061081189662218094 Score : 0.9484702348709106\n",
      "[392/1000]\n",
      "- Train Loss : 0.11637795633739895 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06098616123199463 Score : 0.9484702348709106\n",
      "[393/1000]\n",
      "- Train Loss : 0.11622119529379739 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06089046224951744 Score : 0.9484702348709106\n",
      "[394/1000]\n",
      "- Train Loss : 0.11606484403212865 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06079430133104324 Score : 0.9484702348709106\n",
      "[395/1000]\n",
      "- Train Loss : 0.11591123913725217 Score : 0.9593180550469292\n",
      "- Val Loss : 0.060694847255945206 Score : 0.9484702348709106\n",
      "[396/1000]\n",
      "- Train Loss : 0.11575068078107303 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06060102954506874 Score : 0.9484702348709106\n",
      "[397/1000]\n",
      "- Train Loss : 0.1155955046415329 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06050919368863106 Score : 0.9484702348709106\n",
      "[398/1000]\n",
      "- Train Loss : 0.11544350658853848 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06041673570871353 Score : 0.9484702348709106\n",
      "[399/1000]\n",
      "- Train Loss : 0.11529193777177069 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06032375618815422 Score : 0.9484702348709106\n",
      "[400/1000]\n",
      "- Train Loss : 0.11514313229256207 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06022743508219719 Score : 0.9484702348709106\n",
      "[401/1000]\n",
      "- Train Loss : 0.1149873389965958 Score : 0.9593180550469292\n",
      "- Val Loss : 0.06013665348291397 Score : 0.9484702348709106\n",
      "[402/1000]\n",
      "- Train Loss : 0.11483681946992874 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0600479356944561 Score : 0.9484702348709106\n",
      "[403/1000]\n",
      "- Train Loss : 0.11468945278061761 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05995851382613182 Score : 0.9484702348709106\n",
      "[404/1000]\n",
      "- Train Loss : 0.11454245323936145 Score : 0.9593180550469292\n",
      "- Val Loss : 0.059868600219488144 Score : 0.9484702348709106\n",
      "[405/1000]\n",
      "- Train Loss : 0.11439817398786545 Score : 0.9593180550469292\n",
      "- Val Loss : 0.059775300323963165 Score : 0.9484702348709106\n",
      "[406/1000]\n",
      "- Train Loss : 0.11424695327877998 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05968749150633812 Score : 0.9484702348709106\n",
      "[407/1000]\n",
      "- Train Loss : 0.11410091527634197 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05960164591670036 Score : 0.9484702348709106\n",
      "[408/1000]\n",
      "- Train Loss : 0.11395794111821386 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05951515585184097 Score : 0.9484702348709106\n",
      "[409/1000]\n",
      "- Train Loss : 0.11381531962090069 Score : 0.9593180550469292\n",
      "- Val Loss : 0.059428129345178604 Score : 0.9484702348709106\n",
      "[410/1000]\n",
      "- Train Loss : 0.11367540558179219 Score : 0.9593180550469292\n",
      "- Val Loss : 0.059337738901376724 Score : 0.9484702348709106\n",
      "[411/1000]\n",
      "- Train Loss : 0.11352860968973902 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0592527873814106 Score : 0.9484702348709106\n",
      "[412/1000]\n",
      "- Train Loss : 0.11338683631685045 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05916978791356087 Score : 0.9484702348709106\n",
      "[413/1000]\n",
      "- Train Loss : 0.11324810153908199 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05908610299229622 Score : 0.9484702348709106\n",
      "[414/1000]\n",
      "- Train Loss : 0.11310976164208518 Score : 0.9593180550469292\n",
      "- Val Loss : 0.059001822024583817 Score : 0.9484702348709106\n",
      "[415/1000]\n",
      "- Train Loss : 0.11297399716244803 Score : 0.9593180550469292\n",
      "- Val Loss : 0.058914266526699066 Score : 0.9484702348709106\n",
      "[416/1000]\n",
      "- Train Loss : 0.11283141705724928 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05883197858929634 Score : 0.9484702348709106\n",
      "[417/1000]\n",
      "- Train Loss : 0.11269375681877136 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0587516687810421 Score : 0.9484702348709106\n",
      "[418/1000]\n",
      "- Train Loss : 0.11255910164780086 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05867068096995354 Score : 0.9484702348709106\n",
      "[419/1000]\n",
      "- Train Loss : 0.1124276150431898 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05858535319566727 Score : 0.9484702348709106\n",
      "[420/1000]\n",
      "- Train Loss : 0.11228875526123577 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05850478261709213 Score : 0.9484702348709106\n",
      "[421/1000]\n",
      "- Train Loss : 0.11215411747495334 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05842633172869682 Score : 0.9484702348709106\n",
      "[422/1000]\n",
      "- Train Loss : 0.11202230511440171 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05834745615720749 Score : 0.9484702348709106\n",
      "[423/1000]\n",
      "- Train Loss : 0.11189107265737322 Score : 0.9593180550469292\n",
      "- Val Loss : 0.058268092572689056 Score : 0.9484702348709106\n",
      "[424/1000]\n",
      "- Train Loss : 0.11176249467664295 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05818527936935425 Score : 0.9484702348709106\n",
      "[425/1000]\n",
      "- Train Loss : 0.11162718675202793 Score : 0.9593180550469292\n",
      "- Val Loss : 0.058107640594244 Score : 0.9484702348709106\n",
      "[426/1000]\n",
      "- Train Loss : 0.11149660580688053 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05803189426660538 Score : 0.9484702348709106\n",
      "[427/1000]\n",
      "- Train Loss : 0.11136881882945697 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05795552581548691 Score : 0.9484702348709106\n",
      "[428/1000]\n",
      "- Train Loss : 0.11124142714672619 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0578785240650177 Score : 0.9484702348709106\n",
      "[429/1000]\n",
      "- Train Loss : 0.1111164999504884 Score : 0.9593180550469292\n",
      "- Val Loss : 0.057798177003860474 Score : 0.9484702348709106\n",
      "[430/1000]\n",
      "- Train Loss : 0.11098491731617185 Score : 0.9593180550469292\n",
      "- Val Loss : 0.057722967118024826 Score : 0.9484702348709106\n",
      "[431/1000]\n",
      "- Train Loss : 0.11085798591375351 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05764969438314438 Score : 0.9484702348709106\n",
      "[432/1000]\n",
      "- Train Loss : 0.11073389442430602 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05757574364542961 Score : 0.9484702348709106\n",
      "[433/1000]\n",
      "- Train Loss : 0.11061009516318639 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05750115588307381 Score : 0.9484702348709106\n",
      "[434/1000]\n",
      "- Train Loss : 0.1104887645277712 Score : 0.9593180550469292\n",
      "- Val Loss : 0.057423196732997894 Score : 0.9484702348709106\n",
      "[435/1000]\n",
      "- Train Loss : 0.11036081198188993 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05735030770301819 Score : 0.9484702348709106\n",
      "[436/1000]\n",
      "- Train Loss : 0.11023749638762739 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05727934464812279 Score : 0.9484702348709106\n",
      "[437/1000]\n",
      "- Train Loss : 0.11011684499680996 Score : 0.9593180550469292\n",
      "- Val Loss : 0.057207729667425156 Score : 0.9484702348709106\n",
      "[438/1000]\n",
      "- Train Loss : 0.1099965949025419 Score : 0.9593180550469292\n",
      "- Val Loss : 0.057135481387376785 Score : 0.9484702348709106\n",
      "[439/1000]\n",
      "- Train Loss : 0.10987876479824384 Score : 0.9593180550469292\n",
      "- Val Loss : 0.057059790939092636 Score : 0.9484702348709106\n",
      "[440/1000]\n",
      "- Train Loss : 0.10975429457094935 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0569891557097435 Score : 0.9484702348709106\n",
      "[441/1000]\n",
      "- Train Loss : 0.10963432345953253 Score : 0.9593180550469292\n",
      "- Val Loss : 0.056920427829027176 Score : 0.9484702348709106\n",
      "[442/1000]\n",
      "- Train Loss : 0.10951710161235598 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05685100331902504 Score : 0.9484702348709106\n",
      "[443/1000]\n",
      "- Train Loss : 0.10940015833410952 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05678095296025276 Score : 0.9484702348709106\n",
      "[444/1000]\n",
      "- Train Loss : 0.10928291289342774 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05671120062470436 Score : 0.9484702348709106\n",
      "[445/1000]\n",
      "- Train Loss : 0.10916866217222479 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05663850158452988 Score : 0.9484702348709106\n",
      "[446/1000]\n",
      "- Train Loss : 0.10904833157029417 Score : 0.9593180550469292\n",
      "- Val Loss : 0.056570734828710556 Score : 0.9484702348709106\n",
      "[447/1000]\n",
      "- Train Loss : 0.108932516641087 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05650455877184868 Score : 0.9484702348709106\n",
      "[448/1000]\n",
      "- Train Loss : 0.10881925084524685 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05643763393163681 Score : 0.9484702348709106\n",
      "[449/1000]\n",
      "- Train Loss : 0.10870881771875752 Score : 0.9593180550469292\n",
      "- Val Loss : 0.056366387754678726 Score : 0.9484702348709106\n",
      "[450/1000]\n",
      "- Train Loss : 0.10859135393467215 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05629964917898178 Score : 0.9484702348709106\n",
      "[451/1000]\n",
      "- Train Loss : 0.10847768435875575 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05623489245772362 Score : 0.9484702348709106\n",
      "[452/1000]\n",
      "- Train Loss : 0.10836654322014914 Score : 0.9593180550469292\n",
      "- Val Loss : 0.056169699877500534 Score : 0.9484702348709106\n",
      "[453/1000]\n",
      "- Train Loss : 0.10825591762032774 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0561039075255394 Score : 0.9484702348709106\n",
      "[454/1000]\n",
      "- Train Loss : 0.10814506560564041 Score : 0.9593180550469292\n",
      "- Val Loss : 0.056038372218608856 Score : 0.9484702348709106\n",
      "[455/1000]\n",
      "- Train Loss : 0.10803708537585205 Score : 0.9593180550469292\n",
      "- Val Loss : 0.055969756096601486 Score : 0.9484702348709106\n",
      "[456/1000]\n",
      "- Train Loss : 0.10792308963007397 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0559060238301754 Score : 0.9484702348709106\n",
      "[457/1000]\n",
      "- Train Loss : 0.10781345951060455 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05584380775690079 Score : 0.9484702348709106\n",
      "[458/1000]\n",
      "- Train Loss : 0.10770628187391493 Score : 0.9593180550469292\n",
      "- Val Loss : 0.055780865252017975 Score : 0.9484702348709106\n",
      "[459/1000]\n",
      "- Train Loss : 0.10759917439685927 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05571731552481651 Score : 0.9484702348709106\n",
      "[460/1000]\n",
      "- Train Loss : 0.10749443454874887 Score : 0.9593180550469292\n",
      "- Val Loss : 0.055650364607572556 Score : 0.9484702348709106\n",
      "[461/1000]\n",
      "- Train Loss : 0.10738325884772672 Score : 0.9593180550469292\n",
      "- Val Loss : 0.055588312447071075 Score : 0.9484702348709106\n",
      "[462/1000]\n",
      "- Train Loss : 0.10727629789875613 Score : 0.9593180550469292\n",
      "- Val Loss : 0.055528100579977036 Score : 0.9484702348709106\n",
      "[463/1000]\n",
      "- Train Loss : 0.1071719183690018 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05546717718243599 Score : 0.9484702348709106\n",
      "[464/1000]\n",
      "- Train Loss : 0.10706781182024214 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05540557578206062 Score : 0.9484702348709106\n",
      "[465/1000]\n",
      "- Train Loss : 0.1069659400317404 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05534052103757858 Score : 0.9484702348709106\n",
      "[466/1000]\n",
      "- Train Loss : 0.10685768661399682 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05528026074171066 Score : 0.9484702348709106\n",
      "[467/1000]\n",
      "- Train Loss : 0.10675350638727348 Score : 0.9593180550469292\n",
      "- Val Loss : 0.055221833288669586 Score : 0.9484702348709106\n",
      "[468/1000]\n",
      "- Train Loss : 0.10665185232129362 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05516277998685837 Score : 0.9484702348709106\n",
      "[469/1000]\n",
      "- Train Loss : 0.1065504892418782 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05510302260518074 Score : 0.9484702348709106\n",
      "[470/1000]\n",
      "- Train Loss : 0.1064513787213299 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05503972992300987 Score : 0.9484702348709106\n",
      "[471/1000]\n",
      "- Train Loss : 0.10634592154787646 Score : 0.9593180550469292\n",
      "- Val Loss : 0.054981254041194916 Score : 0.9484702348709106\n",
      "[472/1000]\n",
      "- Train Loss : 0.1062444272554583 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0549246184527874 Score : 0.9484702348709106\n",
      "[473/1000]\n",
      "- Train Loss : 0.10614543718596299 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05486733466386795 Score : 0.9484702348709106\n",
      "[474/1000]\n",
      "- Train Loss : 0.10604674327704641 Score : 0.9593180550469292\n",
      "- Val Loss : 0.054809264838695526 Score : 0.9484702348709106\n",
      "[475/1000]\n",
      "- Train Loss : 0.10594766897459824 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05475139245390892 Score : 0.9484702348709106\n",
      "[476/1000]\n",
      "- Train Loss : 0.10585133648580974 Score : 0.9593180550469292\n",
      "- Val Loss : 0.054690513759851456 Score : 0.9484702348709106\n",
      "[477/1000]\n",
      "- Train Loss : 0.10574910293022792 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05463435873389244 Score : 0.9484702348709106\n",
      "[478/1000]\n",
      "- Train Loss : 0.10565099244316419 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05457976832985878 Score : 0.9484702348709106\n",
      "[479/1000]\n",
      "- Train Loss : 0.1055551307896773 Score : 0.9593180550469292\n",
      "- Val Loss : 0.054524410516023636 Score : 0.9484702348709106\n",
      "[480/1000]\n",
      "- Train Loss : 0.10550107041166888 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05444568768143654 Score : 0.9484702348709106\n",
      "[481/1000]\n",
      "- Train Loss : 0.10536167936192618 Score : 0.9593180550469292\n",
      "- Val Loss : 0.054386209696531296 Score : 0.9484702348709106\n",
      "[482/1000]\n",
      "- Train Loss : 0.10525032836529943 Score : 0.9593180550469292\n",
      "- Val Loss : 0.054347384721040726 Score : 0.9484702348709106\n",
      "[483/1000]\n",
      "- Train Loss : 0.10520840229259597 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05427740141749382 Score : 0.9484702348709106\n",
      "[484/1000]\n",
      "- Train Loss : 0.10507601809998353 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05421968922019005 Score : 0.9484702348709106\n",
      "[485/1000]\n",
      "- Train Loss : 0.10501119535830286 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05415734276175499 Score : 0.9484702348709106\n",
      "[486/1000]\n",
      "- Train Loss : 0.10492207006447846 Score : 0.9593180550469292\n",
      "- Val Loss : 0.054087333381175995 Score : 0.9484702348709106\n",
      "[487/1000]\n",
      "- Train Loss : 0.10478027309808466 Score : 0.9593180550469292\n",
      "- Val Loss : 0.054045867174863815 Score : 0.9484702348709106\n",
      "[488/1000]\n",
      "- Train Loss : 0.10472573505507575 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05399373546242714 Score : 0.9484702348709106\n",
      "[489/1000]\n",
      "- Train Loss : 0.10464770471056302 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0539240762591362 Score : 0.9484702348709106\n",
      "[490/1000]\n",
      "- Train Loss : 0.10454975668754843 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05385890603065491 Score : 0.9484702348709106\n",
      "[491/1000]\n",
      "- Train Loss : 0.10445023079713185 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05380282551050186 Score : 0.9484702348709106\n",
      "[492/1000]\n",
      "- Train Loss : 0.10435751018424828 Score : 0.9593180550469292\n",
      "- Val Loss : 0.053750474005937576 Score : 0.9484702348709106\n",
      "[493/1000]\n",
      "- Train Loss : 0.10426988618241416 Score : 0.9593180550469292\n",
      "- Val Loss : 0.053697071969509125 Score : 0.9484702348709106\n",
      "[494/1000]\n",
      "- Train Loss : 0.10418281662795278 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05364192649722099 Score : 0.9484702348709106\n",
      "[495/1000]\n",
      "- Train Loss : 0.10409457029567824 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05358652397990227 Score : 0.9484702348709106\n",
      "[496/1000]\n",
      "- Train Loss : 0.10400576620466179 Score : 0.9593180550469292\n",
      "- Val Loss : 0.053531963378190994 Score : 0.9484702348709106\n",
      "[497/1000]\n",
      "- Train Loss : 0.1039175848580069 Score : 0.9593180550469292\n",
      "- Val Loss : 0.053478263318538666 Score : 0.9484702348709106\n",
      "[498/1000]\n",
      "- Train Loss : 0.10383022307521766 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0534251406788826 Score : 0.9484702348709106\n",
      "[499/1000]\n",
      "- Train Loss : 0.1037460770457983 Score : 0.9593180550469292\n",
      "- Val Loss : 0.053368840366601944 Score : 0.9484702348709106\n",
      "[500/1000]\n",
      "- Train Loss : 0.1036560804479652 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05331654101610184 Score : 0.9484702348709106\n",
      "[501/1000]\n",
      "- Train Loss : 0.10356972263091141 Score : 0.9593180550469292\n",
      "- Val Loss : 0.053265757858753204 Score : 0.9484702348709106\n",
      "[502/1000]\n",
      "- Train Loss : 0.10348543566134241 Score : 0.9593180550469292\n",
      "- Val Loss : 0.053214382380247116 Score : 0.9484702348709106\n",
      "[503/1000]\n",
      "- Train Loss : 0.10340118242634667 Score : 0.9593180550469292\n",
      "- Val Loss : 0.053162477910518646 Score : 0.9484702348709106\n",
      "[504/1000]\n",
      "- Train Loss : 0.10333618356121911 Score : 0.9593180550469292\n",
      "- Val Loss : 0.053097449243068695 Score : 0.9484702348709106\n",
      "[505/1000]\n",
      "- Train Loss : 0.10323725909822518 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05303562059998512 Score : 0.9484702348709106\n",
      "[506/1000]\n",
      "- Train Loss : 0.10313246937261687 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05299796909093857 Score : 0.9484702348709106\n",
      "[507/1000]\n",
      "- Train Loss : 0.10307386000123289 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05294594168663025 Score : 0.9484702348709106\n",
      "[508/1000]\n",
      "- Train Loss : 0.10298662156694466 Score : 0.9593180550469292\n",
      "- Val Loss : 0.052886657416820526 Score : 0.9484702348709106\n",
      "[509/1000]\n",
      "- Train Loss : 0.10288777326544125 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05284629017114639 Score : 0.9484702348709106\n",
      "[510/1000]\n",
      "- Train Loss : 0.10283139699863063 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05278925970196724 Score : 0.9484702348709106\n",
      "[511/1000]\n",
      "- Train Loss : 0.10273886492682828 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05273134633898735 Score : 0.9484702348709106\n",
      "[512/1000]\n",
      "- Train Loss : 0.10265942269729243 Score : 0.9593180550469292\n",
      "- Val Loss : 0.052681516855955124 Score : 0.9484702348709106\n",
      "[513/1000]\n",
      "- Train Loss : 0.10256911462379827 Score : 0.9593180550469292\n",
      "- Val Loss : 0.052632350474596024 Score : 0.9484702348709106\n",
      "[514/1000]\n",
      "- Train Loss : 0.10249753068718645 Score : 0.9593180550469292\n",
      "- Val Loss : 0.052585143595933914 Score : 0.9484702348709106\n",
      "[515/1000]\n",
      "- Train Loss : 0.10242980242603356 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05252230912446976 Score : 0.9484702348709106\n",
      "[516/1000]\n",
      "- Train Loss : 0.10232528588838047 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05247610807418823 Score : 0.9484702348709106\n",
      "[517/1000]\n",
      "- Train Loss : 0.10225199825233883 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05243608355522156 Score : 0.9484702348709106\n",
      "[518/1000]\n",
      "- Train Loss : 0.10219002246028847 Score : 0.9593180550469292\n",
      "- Val Loss : 0.052377160638570786 Score : 0.9484702348709106\n",
      "[519/1000]\n",
      "- Train Loss : 0.10208987672295836 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05233106017112732 Score : 0.9484702348709106\n",
      "[520/1000]\n",
      "- Train Loss : 0.1020186139891545 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05229046568274498 Score : 0.9484702348709106\n",
      "[521/1000]\n",
      "- Train Loss : 0.10195882307986419 Score : 0.9593180550469292\n",
      "- Val Loss : 0.052228812128305435 Score : 0.9484702348709106\n",
      "[522/1000]\n",
      "- Train Loss : 0.10187397462626298 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05217236280441284 Score : 0.9484702348709106\n",
      "[523/1000]\n",
      "- Train Loss : 0.10178878634340233 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05212453380227089 Score : 0.9484702348709106\n",
      "[524/1000]\n",
      "- Train Loss : 0.10171043707264794 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05207971855998039 Score : 0.9484702348709106\n",
      "[525/1000]\n",
      "- Train Loss : 0.10163647101985084 Score : 0.9593180550469292\n",
      "- Val Loss : 0.052033741027116776 Score : 0.9484702348709106\n",
      "[526/1000]\n",
      "- Train Loss : 0.10156271089282301 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05198611691594124 Score : 0.9484702348709106\n",
      "[527/1000]\n",
      "- Train Loss : 0.10148766719632679 Score : 0.9593180550469292\n",
      "- Val Loss : 0.051938291639089584 Score : 0.9484702348709106\n",
      "[528/1000]\n",
      "- Train Loss : 0.10141211127241452 Score : 0.9593180550469292\n",
      "- Val Loss : 0.051891226321458817 Score : 0.9484702348709106\n",
      "[529/1000]\n",
      "- Train Loss : 0.10133711062371731 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05184495076537132 Score : 0.9484702348709106\n",
      "[530/1000]\n",
      "- Train Loss : 0.10126288628412618 Score : 0.9593180550469292\n",
      "- Val Loss : 0.051799070090055466 Score : 0.9484702348709106\n",
      "[531/1000]\n",
      "- Train Loss : 0.10118921659886837 Score : 0.9593180550469292\n",
      "- Val Loss : 0.051753412932157516 Score : 0.9484702348709106\n",
      "[532/1000]\n",
      "- Train Loss : 0.10111585466398133 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05170789733529091 Score : 0.9484702348709106\n",
      "[533/1000]\n",
      "- Train Loss : 0.10104271873003906 Score : 0.9593180550469292\n",
      "- Val Loss : 0.051662687212228775 Score : 0.9484702348709106\n",
      "[534/1000]\n",
      "- Train Loss : 0.100969893236955 Score : 0.9593180550469292\n",
      "- Val Loss : 0.051617804914712906 Score : 0.9484702348709106\n",
      "[535/1000]\n",
      "- Train Loss : 0.10089742806222704 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05157320946455002 Score : 0.9484702348709106\n",
      "[536/1000]\n",
      "- Train Loss : 0.10082768566078609 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05152575299143791 Score : 0.9484702348709106\n",
      "[537/1000]\n",
      "- Train Loss : 0.10075236587888664 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05148213356733322 Score : 0.9484702348709106\n",
      "[538/1000]\n",
      "- Train Loss : 0.10068052406940195 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0514397956430912 Score : 0.9484702348709106\n",
      "[539/1000]\n",
      "- Train Loss : 0.10061053538488017 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05139685422182083 Score : 0.9484702348709106\n",
      "[540/1000]\n",
      "- Train Loss : 0.10054061168597804 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05135335773229599 Score : 0.9484702348709106\n",
      "[541/1000]\n",
      "- Train Loss : 0.1004702200492223 Score : 0.9593180550469292\n",
      "- Val Loss : 0.051309969276189804 Score : 0.9484702348709106\n",
      "[542/1000]\n",
      "- Train Loss : 0.10039978184633785 Score : 0.9593180550469292\n",
      "- Val Loss : 0.051267109811306 Score : 0.9484702348709106\n",
      "[543/1000]\n",
      "- Train Loss : 0.10032978095114231 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05122470483183861 Score : 0.9484702348709106\n",
      "[544/1000]\n",
      "- Train Loss : 0.10026263900929028 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05117950960993767 Score : 0.9484702348709106\n",
      "[545/1000]\n",
      "- Train Loss : 0.10019005400439103 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05113792419433594 Score : 0.9484702348709106\n",
      "[546/1000]\n",
      "- Train Loss : 0.10012084142201477 Score : 0.9593180550469292\n",
      "- Val Loss : 0.051097605377435684 Score : 0.9484702348709106\n",
      "[547/1000]\n",
      "- Train Loss : 0.10005336772236559 Score : 0.9593180550469292\n",
      "- Val Loss : 0.051056694239377975 Score : 0.9484702348709106\n",
      "[548/1000]\n",
      "- Train Loss : 0.09998595238559776 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05101520195603371 Score : 0.9484702348709106\n",
      "[549/1000]\n",
      "- Train Loss : 0.09991808525390095 Score : 0.9593180550469292\n",
      "- Val Loss : 0.050973810255527496 Score : 0.9484702348709106\n",
      "[550/1000]\n",
      "- Train Loss : 0.09985021584563786 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05093294382095337 Score : 0.9484702348709106\n",
      "[551/1000]\n",
      "- Train Loss : 0.09978271772464116 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05089247226715088 Score : 0.9484702348709106\n",
      "[552/1000]\n",
      "- Train Loss : 0.09971572686400679 Score : 0.9593180550469292\n",
      "- Val Loss : 0.050852250307798386 Score : 0.9484702348709106\n",
      "[553/1000]\n",
      "- Train Loss : 0.09965136150519054 Score : 0.9593180550469292\n",
      "- Val Loss : 0.050809141248464584 Score : 0.9484702348709106\n",
      "[554/1000]\n",
      "- Train Loss : 0.09958155929214424 Score : 0.9593180550469292\n",
      "- Val Loss : 0.050769660621881485 Score : 0.9484702348709106\n",
      "[555/1000]\n",
      "- Train Loss : 0.09951502974662516 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05073140189051628 Score : 0.9484702348709106\n",
      "[556/1000]\n",
      "- Train Loss : 0.09945028647780418 Score : 0.9593180550469292\n",
      "- Val Loss : 0.050692614167928696 Score : 0.9484702348709106\n",
      "[557/1000]\n",
      "- Train Loss : 0.09938552458253172 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05065322294831276 Score : 0.9484702348709106\n",
      "[558/1000]\n",
      "- Train Loss : 0.09932035269836585 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05061391741037369 Score : 0.9484702348709106\n",
      "[559/1000]\n",
      "- Train Loss : 0.0992551398360067 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05057501420378685 Score : 0.9484702348709106\n",
      "[560/1000]\n",
      "- Train Loss : 0.09919027218388186 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05053656920790672 Score : 0.9484702348709106\n",
      "[561/1000]\n",
      "- Train Loss : 0.09912594531973203 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0504983626306057 Score : 0.9484702348709106\n",
      "[562/1000]\n",
      "- Train Loss : 0.09906420836018191 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05045720189809799 Score : 0.9484702348709106\n",
      "[563/1000]\n",
      "- Train Loss : 0.09899701985220115 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05041968822479248 Score : 0.9484702348709106\n",
      "[564/1000]\n",
      "- Train Loss : 0.09893303902612792 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05038336291909218 Score : 0.9484702348709106\n",
      "[565/1000]\n",
      "- Train Loss : 0.09887082771294647 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05034652352333069 Score : 0.9484702348709106\n",
      "[566/1000]\n",
      "- Train Loss : 0.09880866317285432 Score : 0.9593180550469292\n",
      "- Val Loss : 0.050309114158153534 Score : 0.9484702348709106\n",
      "[567/1000]\n",
      "- Train Loss : 0.0987460296601057 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05027167871594429 Score : 0.9484702348709106\n",
      "[568/1000]\n",
      "- Train Loss : 0.09868333178261916 Score : 0.9593180550469292\n",
      "- Val Loss : 0.050234705209732056 Score : 0.9484702348709106\n",
      "[569/1000]\n",
      "- Train Loss : 0.09862099339564641 Score : 0.9593180550469292\n",
      "- Val Loss : 0.050198107957839966 Score : 0.9484702348709106\n",
      "[570/1000]\n",
      "- Train Loss : 0.09855907410383224 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05016179010272026 Score : 0.9484702348709106\n",
      "[571/1000]\n",
      "- Train Loss : 0.09849977700246705 Score : 0.9593180550469292\n",
      "- Val Loss : 0.050122518092393875 Score : 0.9484702348709106\n",
      "[572/1000]\n",
      "- Train Loss : 0.09843510679072803 Score : 0.9593180550469292\n",
      "- Val Loss : 0.05008680000901222 Score : 0.9484702348709106\n",
      "[573/1000]\n",
      "- Train Loss : 0.09837357389430205 Score : 0.9593180550469292\n",
      "- Val Loss : 0.050052352249622345 Score : 0.9484702348709106\n",
      "[574/1000]\n",
      "- Train Loss : 0.09831373662584358 Score : 0.9593180550469292\n",
      "- Val Loss : 0.050017349421978 Score : 0.9484702348709106\n",
      "[575/1000]\n",
      "- Train Loss : 0.09825395047664642 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04998169466853142 Score : 0.9484702348709106\n",
      "[576/1000]\n",
      "- Train Loss : 0.09819372764064206 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04994606226682663 Score : 0.9484702348709106\n",
      "[577/1000]\n",
      "- Train Loss : 0.09813340774012937 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04991082474589348 Score : 0.9484702348709106\n",
      "[578/1000]\n",
      "- Train Loss : 0.09807340593801604 Score : 0.9593180550469292\n",
      "- Val Loss : 0.049876026809215546 Score : 0.9484702348709106\n",
      "[579/1000]\n",
      "- Train Loss : 0.09801386607189973 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04984142631292343 Score : 0.9484702348709106\n",
      "[580/1000]\n",
      "- Train Loss : 0.09795684595074919 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04980393499135971 Score : 0.9484702348709106\n",
      "[581/1000]\n",
      "- Train Loss : 0.09789453446865082 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04976993426680565 Score : 0.9484702348709106\n",
      "[582/1000]\n",
      "- Train Loss : 0.09783527337842518 Score : 0.9593180550469292\n",
      "- Val Loss : 0.049737222492694855 Score : 0.9484702348709106\n",
      "[583/1000]\n",
      "- Train Loss : 0.09777771971291965 Score : 0.9593180550469292\n",
      "- Val Loss : 0.049703892320394516 Score : 0.9484702348709106\n",
      "[584/1000]\n",
      "- Train Loss : 0.09772016646133529 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04966999217867851 Score : 0.9484702348709106\n",
      "[585/1000]\n",
      "- Train Loss : 0.09766222826308674 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04963604360818863 Score : 0.9484702348709106\n",
      "[586/1000]\n",
      "- Train Loss : 0.09760414250195026 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04960247501730919 Score : 0.9484702348709106\n",
      "[587/1000]\n",
      "- Train Loss : 0.09754638787772921 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04956935718655586 Score : 0.9484702348709106\n",
      "[588/1000]\n",
      "- Train Loss : 0.0974889937788248 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04953638091683388 Score : 0.9484702348709106\n",
      "[589/1000]\n",
      "- Train Loss : 0.0974319852474663 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04950360581278801 Score : 0.9484702348709106\n",
      "[590/1000]\n",
      "- Train Loss : 0.09737734248240788 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04946788772940636 Score : 0.9484702348709106\n",
      "[591/1000]\n",
      "- Train Loss : 0.09731746092438698 Score : 0.9593180550469292\n",
      "- Val Loss : 0.049435678869485855 Score : 0.9484702348709106\n",
      "[592/1000]\n",
      "- Train Loss : 0.09726059498886268 Score : 0.9593180550469292\n",
      "- Val Loss : 0.049404729157686234 Score : 0.9484702348709106\n",
      "[593/1000]\n",
      "- Train Loss : 0.09720538763536347 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04937324672937393 Score : 0.9484702348709106\n",
      "[594/1000]\n",
      "- Train Loss : 0.09715023719602162 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04934109374880791 Score : 0.9484702348709106\n",
      "[595/1000]\n",
      "- Train Loss : 0.0970945983297295 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0493089035153389 Score : 0.9484702348709106\n",
      "[596/1000]\n",
      "- Train Loss : 0.0970388636406925 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04927709698677063 Score : 0.9484702348709106\n",
      "[597/1000]\n",
      "- Train Loss : 0.09698346215817663 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04924571514129639 Score : 0.9484702348709106\n",
      "[598/1000]\n",
      "- Train Loss : 0.09692839367522134 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04921450838446617 Score : 0.9484702348709106\n",
      "[599/1000]\n",
      "- Train Loss : 0.09687364660203457 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04918339103460312 Score : 0.9484702348709106\n",
      "[600/1000]\n",
      "- Train Loss : 0.09681913732654518 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0491524264216423 Score : 0.9484702348709106\n",
      "[601/1000]\n",
      "- Train Loss : 0.09676691889762878 Score : 0.9593180550469292\n",
      "- Val Loss : 0.049118559807538986 Score : 0.9484702348709106\n",
      "[602/1000]\n",
      "- Train Loss : 0.09670955025487477 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04908820241689682 Score : 0.9484702348709106\n",
      "[603/1000]\n",
      "- Train Loss : 0.09665516598357095 Score : 0.9593180550469292\n",
      "- Val Loss : 0.049059126526117325 Score : 0.9484702348709106\n",
      "[604/1000]\n",
      "- Train Loss : 0.09660240159266525 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04902944713830948 Score : 0.9484702348709106\n",
      "[605/1000]\n",
      "- Train Loss : 0.09654965127507846 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04899917170405388 Score : 0.9484702348709106\n",
      "[606/1000]\n",
      "- Train Loss : 0.09649650669760174 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04896879941225052 Score : 0.9484702348709106\n",
      "[607/1000]\n",
      "- Train Loss : 0.09644317606257068 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0489388033747673 Score : 0.9484702348709106\n",
      "[608/1000]\n",
      "- Train Loss : 0.09639014758997494 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04890915006399155 Score : 0.9484702348709106\n",
      "[609/1000]\n",
      "- Train Loss : 0.09633748936984274 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0488797128200531 Score : 0.9484702348709106\n",
      "[610/1000]\n",
      "- Train Loss : 0.09628507701887025 Score : 0.9593180550469292\n",
      "- Val Loss : 0.048850398510694504 Score : 0.9484702348709106\n",
      "[611/1000]\n",
      "- Train Loss : 0.0962328811486562 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04882117360830307 Score : 0.9484702348709106\n",
      "[612/1000]\n",
      "- Train Loss : 0.0961808684385485 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04879210516810417 Score : 0.9484702348709106\n",
      "[613/1000]\n",
      "- Train Loss : 0.0961311985221174 Score : 0.9593180550469292\n",
      "- Val Loss : 0.048760149627923965 Score : 0.9484702348709106\n",
      "[614/1000]\n",
      "- Train Loss : 0.0960764011575116 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04873169586062431 Score : 0.9484702348709106\n",
      "[615/1000]\n",
      "- Train Loss : 0.0960245103471809 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04870450124144554 Score : 0.9484702348709106\n",
      "[616/1000]\n",
      "- Train Loss : 0.0959742216186391 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04867669939994812 Score : 0.9484702348709106\n",
      "[617/1000]\n",
      "- Train Loss : 0.09592397552397516 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04864824563264847 Score : 0.9484702348709106\n",
      "[618/1000]\n",
      "- Train Loss : 0.0958732616984182 Score : 0.9593180550469292\n",
      "- Val Loss : 0.048619743436574936 Score : 0.9484702348709106\n",
      "[619/1000]\n",
      "- Train Loss : 0.09582242949141397 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04859157279133797 Score : 0.9484702348709106\n",
      "[620/1000]\n",
      "- Train Loss : 0.09577182390623623 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04856371507048607 Score : 0.9484702348709106\n",
      "[621/1000]\n",
      "- Train Loss : 0.09572161133918497 Score : 0.9593180550469292\n",
      "- Val Loss : 0.048536092042922974 Score : 0.9484702348709106\n",
      "[622/1000]\n",
      "- Train Loss : 0.09567159766124354 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04850858822464943 Score : 0.9484702348709106\n",
      "[623/1000]\n",
      "- Train Loss : 0.09562180750072002 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04848112538456917 Score : 0.9484702348709106\n",
      "[624/1000]\n",
      "- Train Loss : 0.09557212413185173 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04845381900668144 Score : 0.9484702348709106\n",
      "[625/1000]\n",
      "- Train Loss : 0.09552480859888925 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04842362552881241 Score : 0.9484702348709106\n",
      "[626/1000]\n",
      "- Train Loss : 0.09547240473330021 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04839690402150154 Score : 0.9484702348709106\n",
      "[627/1000]\n",
      "- Train Loss : 0.0954228521635135 Score : 0.9593180550469292\n",
      "- Val Loss : 0.048371415585279465 Score : 0.9484702348709106\n",
      "[628/1000]\n",
      "- Train Loss : 0.09537487187319332 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0483454205095768 Score : 0.9484702348709106\n",
      "[629/1000]\n",
      "- Train Loss : 0.0953269222130378 Score : 0.9593180550469292\n",
      "- Val Loss : 0.048318687826395035 Score : 0.9484702348709106\n",
      "[630/1000]\n",
      "- Train Loss : 0.09527852365540133 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04829186201095581 Score : 0.9484702348709106\n",
      "[631/1000]\n",
      "- Train Loss : 0.09522999947269757 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04826538637280464 Score : 0.9484702348709106\n",
      "[632/1000]\n",
      "- Train Loss : 0.09518167604174879 Score : 0.9593180550469292\n",
      "- Val Loss : 0.048239219933748245 Score : 0.9484702348709106\n",
      "[633/1000]\n",
      "- Train Loss : 0.09513364401128557 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04821327328681946 Score : 0.9484702348709106\n",
      "[634/1000]\n",
      "- Train Loss : 0.0950858723372221 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04818742349743843 Score : 0.9484702348709106\n",
      "[635/1000]\n",
      "- Train Loss : 0.09503835170633262 Score : 0.9593180550469292\n",
      "- Val Loss : 0.048161622136831284 Score : 0.9484702348709106\n",
      "[636/1000]\n",
      "- Train Loss : 0.09499090640909141 Score : 0.9593180550469292\n",
      "- Val Loss : 0.048135966062545776 Score : 0.9484702348709106\n",
      "[637/1000]\n",
      "- Train Loss : 0.09494364344411427 Score : 0.9593180550469292\n",
      "- Val Loss : 0.048110462725162506 Score : 0.9484702348709106\n",
      "[638/1000]\n",
      "- Train Loss : 0.09489656467404631 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04808508977293968 Score : 0.9484702348709106\n",
      "[639/1000]\n",
      "- Train Loss : 0.09485172852873802 Score : 0.9593180550469292\n",
      "- Val Loss : 0.048056818544864655 Score : 0.9484702348709106\n",
      "[640/1000]\n",
      "- Train Loss : 0.09480197924292749 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04803197458386421 Score : 0.9484702348709106\n",
      "[641/1000]\n",
      "- Train Loss : 0.09475492830905649 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04800836369395256 Score : 0.9484702348709106\n",
      "[642/1000]\n",
      "- Train Loss : 0.09470941830012533 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04798417538404465 Score : 0.9484702348709106\n",
      "[643/1000]\n",
      "- Train Loss : 0.09466393343690369 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04795932397246361 Score : 0.9484702348709106\n",
      "[644/1000]\n",
      "- Train Loss : 0.09461806300613615 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04793437197804451 Score : 0.9484702348709106\n",
      "[645/1000]\n",
      "- Train Loss : 0.0945719855113162 Score : 0.9593180550469292\n",
      "- Val Loss : 0.047909677028656006 Score : 0.9484702348709106\n",
      "[646/1000]\n",
      "- Train Loss : 0.094526087347832 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04788535088300705 Score : 0.9484702348709106\n",
      "[647/1000]\n",
      "- Train Loss : 0.0944805284962058 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04786118119955063 Score : 0.9484702348709106\n",
      "[648/1000]\n",
      "- Train Loss : 0.0944351993708147 Score : 0.9593180550469292\n",
      "- Val Loss : 0.047837141901254654 Score : 0.9484702348709106\n",
      "[649/1000]\n",
      "- Train Loss : 0.09439004988720019 Score : 0.9593180550469292\n",
      "- Val Loss : 0.047813139855861664 Score : 0.9484702348709106\n",
      "[650/1000]\n",
      "- Train Loss : 0.09434502106159925 Score : 0.9593180550469292\n",
      "- Val Loss : 0.047789257019758224 Score : 0.9484702348709106\n",
      "[651/1000]\n",
      "- Train Loss : 0.09430014404157798 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04776548594236374 Score : 0.9484702348709106\n",
      "[652/1000]\n",
      "- Train Loss : 0.09425544521460931 Score : 0.9593180550469292\n",
      "- Val Loss : 0.047741860151290894 Score : 0.9484702348709106\n",
      "[653/1000]\n",
      "- Train Loss : 0.09421089622709486 Score : 0.9593180550469292\n",
      "- Val Loss : 0.047718364745378494 Score : 0.9484702348709106\n",
      "[654/1000]\n",
      "- Train Loss : 0.0941664781421423 Score : 0.9593180550469292\n",
      "- Val Loss : 0.047694992274045944 Score : 0.9484702348709106\n",
      "[655/1000]\n",
      "- Train Loss : 0.09412225315140353 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04767172783613205 Score : 0.9484702348709106\n",
      "[656/1000]\n",
      "- Train Loss : 0.09408027533855703 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04764552041888237 Score : 0.9484702348709106\n",
      "[657/1000]\n",
      "- Train Loss : 0.0940333597569002 Score : 0.9593180550469292\n",
      "- Val Loss : 0.047622740268707275 Score : 0.9484702348709106\n",
      "[658/1000]\n",
      "- Train Loss : 0.09398910568820106 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04760121554136276 Score : 0.9484702348709106\n",
      "[659/1000]\n",
      "- Train Loss : 0.09394634701311588 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04757910966873169 Score : 0.9484702348709106\n",
      "[660/1000]\n",
      "- Train Loss : 0.09390366211947468 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04755633696913719 Score : 0.9484702348709106\n",
      "[661/1000]\n",
      "- Train Loss : 0.09386054685132371 Score : 0.9593180550469292\n",
      "- Val Loss : 0.047533415257930756 Score : 0.9484702348709106\n",
      "[662/1000]\n",
      "- Train Loss : 0.0938172195520666 Score : 0.9593180550469292\n",
      "- Val Loss : 0.047510776668787 Score : 0.9484702348709106\n",
      "[663/1000]\n",
      "- Train Loss : 0.0937740755163961 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04748843237757683 Score : 0.9484702348709106\n",
      "[664/1000]\n",
      "- Train Loss : 0.09373122246729003 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04746628552675247 Score : 0.9484702348709106\n",
      "[665/1000]\n",
      "- Train Loss : 0.0936885957295696 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04744422435760498 Score : 0.9484702348709106\n",
      "[666/1000]\n",
      "- Train Loss : 0.09364610900067621 Score : 0.9593180550469292\n",
      "- Val Loss : 0.047422200441360474 Score : 0.9484702348709106\n",
      "[667/1000]\n",
      "- Train Loss : 0.0936037620736493 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04740030691027641 Score : 0.9484702348709106\n",
      "[668/1000]\n",
      "- Train Loss : 0.093561549567514 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04737851396203041 Score : 0.9484702348709106\n",
      "[669/1000]\n",
      "- Train Loss : 0.09351941487855381 Score : 0.9593180550469292\n",
      "- Val Loss : 0.047356825321912766 Score : 0.9484702348709106\n",
      "[670/1000]\n",
      "- Train Loss : 0.0934775002921621 Score : 0.9593180550469292\n",
      "- Val Loss : 0.047335248440504074 Score : 0.9484702348709106\n",
      "[671/1000]\n",
      "- Train Loss : 0.09343572323107058 Score : 0.9593180550469292\n",
      "- Val Loss : 0.047313809394836426 Score : 0.9484702348709106\n",
      "[672/1000]\n",
      "- Train Loss : 0.09339412167254421 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04729243740439415 Score : 0.9484702348709106\n",
      "[673/1000]\n",
      "- Train Loss : 0.09335264698084858 Score : 0.9593180550469292\n",
      "- Val Loss : 0.047271210700273514 Score : 0.9484702348709106\n",
      "[674/1000]\n",
      "- Train Loss : 0.09331127266503042 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04725004360079765 Score : 0.9484702348709106\n",
      "[675/1000]\n",
      "- Train Loss : 0.09327011203600301 Score : 0.9593180550469292\n",
      "- Val Loss : 0.047229018062353134 Score : 0.9484702348709106\n",
      "[676/1000]\n",
      "- Train Loss : 0.0932310697519117 Score : 0.9593180550469292\n",
      "- Val Loss : 0.047205060720443726 Score : 0.9484702348709106\n",
      "[677/1000]\n",
      "- Train Loss : 0.09318722929391596 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04718445613980293 Score : 0.9484702348709106\n",
      "[678/1000]\n",
      "- Train Loss : 0.09314595504353444 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04716511443257332 Score : 0.9484702348709106\n",
      "[679/1000]\n",
      "- Train Loss : 0.09310613220764531 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04714521765708923 Score : 0.9484702348709106\n",
      "[680/1000]\n",
      "- Train Loss : 0.09306636680331495 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04712461307644844 Score : 0.9484702348709106\n",
      "[681/1000]\n",
      "- Train Loss : 0.0930261676096254 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0471038818359375 Score : 0.9484702348709106\n",
      "[682/1000]\n",
      "- Train Loss : 0.0929857978804244 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04708334431052208 Score : 0.9484702348709106\n",
      "[683/1000]\n",
      "- Train Loss : 0.09294554404914379 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04706314206123352 Score : 0.9484702348709106\n",
      "[684/1000]\n",
      "- Train Loss : 0.09290556485454242 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0470430888235569 Score : 0.9484702348709106\n",
      "[685/1000]\n",
      "- Train Loss : 0.09286578175508314 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04702315106987953 Score : 0.9484702348709106\n",
      "[686/1000]\n",
      "- Train Loss : 0.09282616712152958 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04700324684381485 Score : 0.9484702348709106\n",
      "[687/1000]\n",
      "- Train Loss : 0.09278663754877117 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04698341712355614 Score : 0.9484702348709106\n",
      "[688/1000]\n",
      "- Train Loss : 0.09274723680896892 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04696372523903847 Score : 0.9484702348709106\n",
      "[689/1000]\n",
      "- Train Loss : 0.09270799015131262 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04694405570626259 Score : 0.9484702348709106\n",
      "[690/1000]\n",
      "- Train Loss : 0.09266882762312889 Score : 0.9593180550469292\n",
      "- Val Loss : 0.046924568712711334 Score : 0.9484702348709106\n",
      "[691/1000]\n",
      "- Train Loss : 0.09262984287407663 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04690513387322426 Score : 0.9484702348709106\n",
      "[692/1000]\n",
      "- Train Loss : 0.09259095788002014 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04688582569360733 Score : 0.9484702348709106\n",
      "[693/1000]\n",
      "- Train Loss : 0.09255221786184444 Score : 0.9593180550469292\n",
      "- Val Loss : 0.046866584569215775 Score : 0.9484702348709106\n",
      "[694/1000]\n",
      "- Train Loss : 0.0925136072975066 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04684746637940407 Score : 0.9484702348709106\n",
      "[695/1000]\n",
      "- Train Loss : 0.09247514232993126 Score : 0.9593180550469292\n",
      "- Val Loss : 0.046828389167785645 Score : 0.9484702348709106\n",
      "[696/1000]\n",
      "- Train Loss : 0.09243674762547016 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04680947586894035 Score : 0.9484702348709106\n",
      "[697/1000]\n",
      "- Train Loss : 0.0923985755071044 Score : 0.9593180550469292\n",
      "- Val Loss : 0.046790607273578644 Score : 0.9484702348709106\n",
      "[698/1000]\n",
      "- Train Loss : 0.09236045574976338 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04677187278866768 Score : 0.9484702348709106\n",
      "[699/1000]\n",
      "- Train Loss : 0.09232250466528866 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0467531718313694 Score : 0.9484702348709106\n",
      "[700/1000]\n",
      "- Train Loss : 0.09228465778546201 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04673460125923157 Score : 0.9484702348709106\n",
      "[701/1000]\n",
      "- Train Loss : 0.09224694035947323 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0467161126434803 Score : 0.9484702348709106\n",
      "[702/1000]\n",
      "- Train Loss : 0.09220933489915398 Score : 0.9593180550469292\n",
      "- Val Loss : 0.046697743237018585 Score : 0.9484702348709106\n",
      "[703/1000]\n",
      "- Train Loss : 0.09217184606111711 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04667943716049194 Score : 0.9484702348709106\n",
      "[704/1000]\n",
      "- Train Loss : 0.09213452599942684 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04666120558977127 Score : 0.9484702348709106\n",
      "[705/1000]\n",
      "- Train Loss : 0.09209931062327491 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04664004221558571 Score : 0.9484702348709106\n",
      "[706/1000]\n",
      "- Train Loss : 0.09205931404398547 Score : 0.9593180550469292\n",
      "- Val Loss : 0.046622227877378464 Score : 0.9484702348709106\n",
      "[707/1000]\n",
      "- Train Loss : 0.09202184900641441 Score : 0.9593180550469292\n",
      "- Val Loss : 0.046605635434389114 Score : 0.9484702348709106\n",
      "[708/1000]\n",
      "- Train Loss : 0.09198576315409607 Score : 0.9593180550469292\n",
      "- Val Loss : 0.046588536351919174 Score : 0.9484702348709106\n",
      "[709/1000]\n",
      "- Train Loss : 0.0919497213843796 Score : 0.9593180550469292\n",
      "- Val Loss : 0.046570759266614914 Score : 0.9484702348709106\n",
      "[710/1000]\n",
      "- Train Loss : 0.09191325286196338 Score : 0.9593180550469292\n",
      "- Val Loss : 0.046552740037441254 Score : 0.9484702348709106\n",
      "[711/1000]\n",
      "- Train Loss : 0.09187665592051214 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04653492942452431 Score : 0.9484702348709106\n",
      "[712/1000]\n",
      "- Train Loss : 0.09184011537581682 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04651739075779915 Score : 0.9484702348709106\n",
      "[713/1000]\n",
      "- Train Loss : 0.09180375964691241 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04650008678436279 Score : 0.9484702348709106\n",
      "[714/1000]\n",
      "- Train Loss : 0.09176767089714606 Score : 0.9593180550469292\n",
      "- Val Loss : 0.046482764184474945 Score : 0.9484702348709106\n",
      "[715/1000]\n",
      "- Train Loss : 0.0917316855241855 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04646551236510277 Score : 0.9484702348709106\n",
      "[716/1000]\n",
      "- Train Loss : 0.09169577372570832 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04644838720560074 Score : 0.9484702348709106\n",
      "[717/1000]\n",
      "- Train Loss : 0.09165999076018731 Score : 0.9593180550469292\n",
      "- Val Loss : 0.046431269496679306 Score : 0.9484702348709106\n",
      "[718/1000]\n",
      "- Train Loss : 0.09162430320349005 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04641425609588623 Score : 0.9484702348709106\n",
      "[719/1000]\n",
      "- Train Loss : 0.09158872947510746 Score : 0.9593180550469292\n",
      "- Val Loss : 0.046397361904382706 Score : 0.9484702348709106\n",
      "[720/1000]\n",
      "- Train Loss : 0.09155325053466691 Score : 0.9593180550469292\n",
      "- Val Loss : 0.046380553394556046 Score : 0.9484702348709106\n",
      "[721/1000]\n",
      "- Train Loss : 0.09151793850792779 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04636380076408386 Score : 0.9484702348709106\n",
      "[722/1000]\n",
      "- Train Loss : 0.09148272385613786 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04634714499115944 Score : 0.9484702348709106\n",
      "[723/1000]\n",
      "- Train Loss : 0.09144757170644072 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0463305339217186 Score : 0.9484702348709106\n",
      "[724/1000]\n",
      "- Train Loss : 0.09141259919852018 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04631403088569641 Score : 0.9484702348709106\n",
      "[725/1000]\n",
      "- Train Loss : 0.09137771485580339 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0462975837290287 Score : 0.9484702348709106\n",
      "[726/1000]\n",
      "- Train Loss : 0.0913429209548566 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04628126695752144 Score : 0.9484702348709106\n",
      "[727/1000]\n",
      "- Train Loss : 0.09130823632909192 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04626500606536865 Score : 0.9484702348709106\n",
      "[728/1000]\n",
      "- Train Loss : 0.09127368560681741 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04624881595373154 Score : 0.9484702348709106\n",
      "[729/1000]\n",
      "- Train Loss : 0.09123921818617317 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04623273387551308 Score : 0.9484702348709106\n",
      "[730/1000]\n",
      "- Train Loss : 0.09120487483839194 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0462166890501976 Score : 0.9484702348709106\n",
      "[731/1000]\n",
      "- Train Loss : 0.09117067739781406 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04620075225830078 Score : 0.9484702348709106\n",
      "[732/1000]\n",
      "- Train Loss : 0.09113653128345807 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04618487507104874 Score : 0.9484702348709106\n",
      "[733/1000]\n",
      "- Train Loss : 0.09110250272270706 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04616907984018326 Score : 0.9484702348709106\n",
      "[734/1000]\n",
      "- Train Loss : 0.09106861282553938 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04615337401628494 Score : 0.9484702348709106\n",
      "[735/1000]\n",
      "- Train Loss : 0.09103476908057928 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04613775759935379 Score : 0.9484702348709106\n",
      "[736/1000]\n",
      "- Train Loss : 0.09100108366045687 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04612218216061592 Score : 0.9484702348709106\n",
      "[737/1000]\n",
      "- Train Loss : 0.09096748609509733 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04610669985413551 Score : 0.9484702348709106\n",
      "[738/1000]\n",
      "- Train Loss : 0.09093398962997729 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04609130695462227 Score : 0.9484702348709106\n",
      "[739/1000]\n",
      "- Train Loss : 0.09090061206370592 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0460759662091732 Score : 0.9484702348709106\n",
      "[740/1000]\n",
      "- Train Loss : 0.09086732152435514 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0460607074201107 Score : 0.9484702348709106\n",
      "[741/1000]\n",
      "- Train Loss : 0.09083408789916171 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04604553058743477 Score : 0.9484702348709106\n",
      "[742/1000]\n",
      "- Train Loss : 0.09080106558071242 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0460304357111454 Score : 0.9484702348709106\n",
      "[743/1000]\n",
      "- Train Loss : 0.09076807834208012 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04601535573601723 Score : 0.9484702348709106\n",
      "[744/1000]\n",
      "- Train Loss : 0.09073521590067281 Score : 0.9593180550469292\n",
      "- Val Loss : 0.046000413596630096 Score : 0.9484702348709106\n",
      "[745/1000]\n",
      "- Train Loss : 0.09070244079662694 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04598553106188774 Score : 0.9484702348709106\n",
      "[746/1000]\n",
      "- Train Loss : 0.09066973833574189 Score : 0.9593180550469292\n",
      "- Val Loss : 0.045970696955919266 Score : 0.9484702348709106\n",
      "[747/1000]\n",
      "- Train Loss : 0.09063717619412476 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04595594480633736 Score : 0.9484702348709106\n",
      "[748/1000]\n",
      "- Train Loss : 0.09060471660147111 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04594125226140022 Score : 0.9484702348709106\n",
      "[749/1000]\n",
      "- Train Loss : 0.090572331721584 Score : 0.9593180550469292\n",
      "- Val Loss : 0.045926693826913834 Score : 0.9484702348709106\n",
      "[750/1000]\n",
      "- Train Loss : 0.0905400700867176 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04591214656829834 Score : 0.9484702348709106\n",
      "[751/1000]\n",
      "- Train Loss : 0.0905079095520907 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04589766636490822 Score : 0.9484702348709106\n",
      "[752/1000]\n",
      "- Train Loss : 0.09047582041886118 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04588331654667854 Score : 0.9484702348709106\n",
      "[753/1000]\n",
      "- Train Loss : 0.0904438022731079 Score : 0.9593180550469292\n",
      "- Val Loss : 0.045869000256061554 Score : 0.9484702348709106\n",
      "[754/1000]\n",
      "- Train Loss : 0.09041193707121743 Score : 0.9593180550469292\n",
      "- Val Loss : 0.045854754745960236 Score : 0.9484702348709106\n",
      "[755/1000]\n",
      "- Train Loss : 0.09038015661968125 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04584052041172981 Score : 0.9484702348709106\n",
      "[756/1000]\n",
      "- Train Loss : 0.09034844011896187 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04582641273736954 Score : 0.9484702348709106\n",
      "[757/1000]\n",
      "- Train Loss : 0.09031686693843868 Score : 0.9593180550469292\n",
      "- Val Loss : 0.045812394469976425 Score : 0.9484702348709106\n",
      "[758/1000]\n",
      "- Train Loss : 0.09028535719133085 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0457984022796154 Score : 0.9484702348709106\n",
      "[759/1000]\n",
      "- Train Loss : 0.09025395527068111 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04578450694680214 Score : 0.9484702348709106\n",
      "[760/1000]\n",
      "- Train Loss : 0.09022263147764736 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04577066749334335 Score : 0.9484702348709106\n",
      "[761/1000]\n",
      "- Train Loss : 0.09019142120248741 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04575690999627113 Score : 0.9484702348709106\n",
      "[762/1000]\n",
      "- Train Loss : 0.09016029257327318 Score : 0.9593180550469292\n",
      "- Val Loss : 0.045743200927972794 Score : 0.9484702348709106\n",
      "[763/1000]\n",
      "- Train Loss : 0.09012926949395074 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04572956636548042 Score : 0.9484702348709106\n",
      "[764/1000]\n",
      "- Train Loss : 0.09009832775013314 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04571598023176193 Score : 0.9484702348709106\n",
      "[765/1000]\n",
      "- Train Loss : 0.0900674947640962 Score : 0.9593180550469292\n",
      "- Val Loss : 0.045702483505010605 Score : 0.9484702348709106\n",
      "[766/1000]\n",
      "- Train Loss : 0.09003669747875796 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04568905383348465 Score : 0.9484702348709106\n",
      "[767/1000]\n",
      "- Train Loss : 0.09000607745514975 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04567567631602287 Score : 0.9484702348709106\n",
      "[768/1000]\n",
      "- Train Loss : 0.0899755001688997 Score : 0.9593180550469292\n",
      "- Val Loss : 0.045662377029657364 Score : 0.9484702348709106\n",
      "[769/1000]\n",
      "- Train Loss : 0.0899450174636311 Score : 0.9593180550469292\n",
      "- Val Loss : 0.045649148523807526 Score : 0.9484702348709106\n",
      "[770/1000]\n",
      "- Train Loss : 0.08991462406184939 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04563596472144127 Score : 0.9484702348709106\n",
      "[771/1000]\n",
      "- Train Loss : 0.08988432751761542 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04562285915017128 Score : 0.9484702348709106\n",
      "[772/1000]\n",
      "- Train Loss : 0.08985412917617294 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04560982808470726 Score : 0.9484702348709106\n",
      "[773/1000]\n",
      "- Train Loss : 0.08982403131408824 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04559680074453354 Score : 0.9484702348709106\n",
      "[774/1000]\n",
      "- Train Loss : 0.08979397101534738 Score : 0.9593180550469292\n",
      "- Val Loss : 0.045583877712488174 Score : 0.9484702348709106\n",
      "[775/1000]\n",
      "- Train Loss : 0.08976405382984215 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04557103291153908 Score : 0.9484702348709106\n",
      "[776/1000]\n",
      "- Train Loss : 0.08973417669120762 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04555826261639595 Score : 0.9484702348709106\n",
      "[777/1000]\n",
      "- Train Loss : 0.08970446005049679 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04554546996951103 Score : 0.9484702348709106\n",
      "[778/1000]\n",
      "- Train Loss : 0.08967472954342763 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04553280398249626 Score : 0.9484702348709106\n",
      "[779/1000]\n",
      "- Train Loss : 0.08964514773752955 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04552021250128746 Score : 0.9484702348709106\n",
      "[780/1000]\n",
      "- Train Loss : 0.08961567665553755 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04550765082240105 Score : 0.9484702348709106\n",
      "[781/1000]\n",
      "- Train Loss : 0.0895862187155419 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0454951636493206 Score : 0.9484702348709106\n",
      "[782/1000]\n",
      "- Train Loss : 0.08955688960850239 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04548274725675583 Score : 0.9484702348709106\n",
      "[783/1000]\n",
      "- Train Loss : 0.08952764245784944 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04547038674354553 Score : 0.9484702348709106\n",
      "[784/1000]\n",
      "- Train Loss : 0.08949847364177306 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04545808583498001 Score : 0.9484702348709106\n",
      "[785/1000]\n",
      "- Train Loss : 0.08946939909623729 Score : 0.9593180550469292\n",
      "- Val Loss : 0.045445848256349564 Score : 0.9484702348709106\n",
      "[786/1000]\n",
      "- Train Loss : 0.0894404011261132 Score : 0.9593180550469292\n",
      "- Val Loss : 0.045433659106492996 Score : 0.9484702348709106\n",
      "[787/1000]\n",
      "- Train Loss : 0.08941150539451176 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04542151838541031 Score : 0.9484702348709106\n",
      "[788/1000]\n",
      "- Train Loss : 0.08938267257892424 Score : 0.9593180550469292\n",
      "- Val Loss : 0.045409463346004486 Score : 0.9484702348709106\n",
      "[789/1000]\n",
      "- Train Loss : 0.08935391313085954 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04539743438363075 Score : 0.9484702348709106\n",
      "[790/1000]\n",
      "- Train Loss : 0.08932524878117773 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04538550227880478 Score : 0.9484702348709106\n",
      "[791/1000]\n",
      "- Train Loss : 0.08929669639716546 Score : 0.9593180550469292\n",
      "- Val Loss : 0.045373592525720596 Score : 0.9484702348709106\n",
      "[792/1000]\n",
      "- Train Loss : 0.08926818685399161 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04536176100373268 Score : 0.9484702348709106\n",
      "[793/1000]\n",
      "- Train Loss : 0.08923978627555901 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04535001143813133 Score : 0.9484702348709106\n",
      "[794/1000]\n",
      "- Train Loss : 0.08921143671290742 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04533829167485237 Score : 0.9484702348709106\n",
      "[795/1000]\n",
      "- Train Loss : 0.08918319466627306 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04532663896679878 Score : 0.9484702348709106\n",
      "[796/1000]\n",
      "- Train Loss : 0.08915502640108268 Score : 0.9593180550469292\n",
      "- Val Loss : 0.045315008610486984 Score : 0.9484702348709106\n",
      "[797/1000]\n",
      "- Train Loss : 0.0891269300546911 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04530346021056175 Score : 0.9484702348709106\n",
      "[798/1000]\n",
      "- Train Loss : 0.08909891876909468 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0452919527888298 Score : 0.9484702348709106\n",
      "[799/1000]\n",
      "- Train Loss : 0.08907103000415696 Score : 0.9593180550469292\n",
      "- Val Loss : 0.045280538499355316 Score : 0.9484702348709106\n",
      "[800/1000]\n",
      "- Train Loss : 0.08904312478585376 Score : 0.9593180550469292\n",
      "- Val Loss : 0.045269161462783813 Score : 0.9484702348709106\n",
      "[801/1000]\n",
      "- Train Loss : 0.08901538554992941 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04525785520672798 Score : 0.9484702348709106\n",
      "[802/1000]\n",
      "- Train Loss : 0.08898768118686146 Score : 0.9593180550469292\n",
      "- Val Loss : 0.045246563851833344 Score : 0.9484702348709106\n",
      "[803/1000]\n",
      "- Train Loss : 0.088960077199671 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04523535817861557 Score : 0.9484702348709106\n",
      "[804/1000]\n",
      "- Train Loss : 0.08893256737954086 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04522422328591347 Score : 0.9484702348709106\n",
      "[805/1000]\n",
      "- Train Loss : 0.08890510040024917 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04521310329437256 Score : 0.9484702348709106\n",
      "[806/1000]\n",
      "- Train Loss : 0.08887775676945846 Score : 0.9593180550469292\n",
      "- Val Loss : 0.045202042907476425 Score : 0.9484702348709106\n",
      "[807/1000]\n",
      "- Train Loss : 0.08885042214145263 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04519105330109596 Score : 0.9484702348709106\n",
      "[808/1000]\n",
      "- Train Loss : 0.08882315622435676 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04518011212348938 Score : 0.9484702348709106\n",
      "[809/1000]\n",
      "- Train Loss : 0.08879603755970795 Score : 0.9593180550469292\n",
      "- Val Loss : 0.045169223099946976 Score : 0.9484702348709106\n",
      "[810/1000]\n",
      "- Train Loss : 0.08876899505654971 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04515842720866203 Score : 0.9484702348709106\n",
      "[811/1000]\n",
      "- Train Loss : 0.08874200170652734 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0451476164162159 Score : 0.9484702348709106\n",
      "[812/1000]\n",
      "- Train Loss : 0.08871507996486293 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04513689503073692 Score : 0.9484702348709106\n",
      "[813/1000]\n",
      "- Train Loss : 0.08868821472343472 Score : 0.9593180550469292\n",
      "- Val Loss : 0.045126207172870636 Score : 0.9484702348709106\n",
      "[814/1000]\n",
      "- Train Loss : 0.08866145844674772 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04511558264493942 Score : 0.9484702348709106\n",
      "[815/1000]\n",
      "- Train Loss : 0.08863475525544749 Score : 0.9593180550469292\n",
      "- Val Loss : 0.045105043798685074 Score : 0.9484702348709106\n",
      "[816/1000]\n",
      "- Train Loss : 0.08860814509292443 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04509451612830162 Score : 0.9484702348709106\n",
      "[817/1000]\n",
      "- Train Loss : 0.08858158460093869 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04508404806256294 Score : 0.9484702348709106\n",
      "[818/1000]\n",
      "- Train Loss : 0.08855515045838223 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04507363960146904 Score : 0.9484702348709106\n",
      "[819/1000]\n",
      "- Train Loss : 0.08852873866756757 Score : 0.9593180550469292\n",
      "- Val Loss : 0.045063264667987823 Score : 0.9484702348709106\n",
      "[820/1000]\n",
      "- Train Loss : 0.0885024308744404 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04505297914147377 Score : 0.9484702348709106\n",
      "[821/1000]\n",
      "- Train Loss : 0.08847618351380031 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04504271596670151 Score : 0.9484702348709106\n",
      "[822/1000]\n",
      "- Train Loss : 0.08844999172207382 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04503250494599342 Score : 0.9484702348709106\n",
      "[823/1000]\n",
      "- Train Loss : 0.0884238983400994 Score : 0.9593180550469292\n",
      "- Val Loss : 0.045022331178188324 Score : 0.9484702348709106\n",
      "[824/1000]\n",
      "- Train Loss : 0.08839784428063366 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0450122244656086 Score : 0.9484702348709106\n",
      "[825/1000]\n",
      "- Train Loss : 0.08837190580864747 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04500218480825424 Score : 0.9484702348709106\n",
      "[826/1000]\n",
      "- Train Loss : 0.08834602104292975 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044992197304964066 Score : 0.9484702348709106\n",
      "[827/1000]\n",
      "- Train Loss : 0.08832021502570973 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04498220607638359 Score : 0.9484702348709106\n",
      "[828/1000]\n",
      "- Train Loss : 0.08829449644933145 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04497227445244789 Score : 0.9484702348709106\n",
      "[829/1000]\n",
      "- Train Loss : 0.08826879805160893 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04496242478489876 Score : 0.9484702348709106\n",
      "[830/1000]\n",
      "- Train Loss : 0.08824321151607567 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0449526272714138 Score : 0.9484702348709106\n",
      "[831/1000]\n",
      "- Train Loss : 0.08821765364458163 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04494285583496094 Score : 0.9484702348709106\n",
      "[832/1000]\n",
      "- Train Loss : 0.08819220556567113 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044933151453733444 Score : 0.9484702348709106\n",
      "[833/1000]\n",
      "- Train Loss : 0.08816682464546627 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04492349177598953 Score : 0.9484702348709106\n",
      "[834/1000]\n",
      "- Train Loss : 0.08814149091227187 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04491385072469711 Score : 0.9484702348709106\n",
      "[835/1000]\n",
      "- Train Loss : 0.08811627069695128 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04490429908037186 Score : 0.9484702348709106\n",
      "[836/1000]\n",
      "- Train Loss : 0.08809108049091366 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0448947548866272 Score : 0.9484702348709106\n",
      "[837/1000]\n",
      "- Train Loss : 0.08806597006817658 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04488527774810791 Score : 0.9484702348709106\n",
      "[838/1000]\n",
      "- Train Loss : 0.08804092038836744 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044875867664813995 Score : 0.9484702348709106\n",
      "[839/1000]\n",
      "- Train Loss : 0.08801595877028173 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04486646503210068 Score : 0.9484702348709106\n",
      "[840/1000]\n",
      "- Train Loss : 0.08799102778236072 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04485715180635452 Score : 0.9484702348709106\n",
      "[841/1000]\n",
      "- Train Loss : 0.08796620306869347 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044847846031188965 Score : 0.9484702348709106\n",
      "[842/1000]\n",
      "- Train Loss : 0.08794141850537723 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044838614761829376 Score : 0.9484702348709106\n",
      "[843/1000]\n",
      "- Train Loss : 0.0879167119661967 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04482940956950188 Score : 0.9484702348709106\n",
      "[844/1000]\n",
      "- Train Loss : 0.08789207889801925 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044820260256528854 Score : 0.9484702348709106\n",
      "[845/1000]\n",
      "- Train Loss : 0.08786751754168007 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04481114074587822 Score : 0.9484702348709106\n",
      "[846/1000]\n",
      "- Train Loss : 0.0878466090394391 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044798433780670166 Score : 0.9484702348709106\n",
      "[847/1000]\n",
      "- Train Loss : 0.08782152728074127 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04478587582707405 Score : 0.9484702348709106\n",
      "[848/1000]\n",
      "- Train Loss : 0.08779539747370614 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044775452464818954 Score : 0.9484702348709106\n",
      "[849/1000]\n",
      "- Train Loss : 0.08777015035351117 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044766511768102646 Score : 0.9484702348709106\n",
      "[850/1000]\n",
      "- Train Loss : 0.08774587356795867 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0447579100728035 Score : 0.9484702348709106\n",
      "[851/1000]\n",
      "- Train Loss : 0.08772214077827004 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04474899172782898 Score : 0.9484702348709106\n",
      "[852/1000]\n",
      "- Train Loss : 0.08769836276769638 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044739801436662674 Score : 0.9484702348709106\n",
      "[853/1000]\n",
      "- Train Loss : 0.08767448365688324 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04473050683736801 Score : 0.9484702348709106\n",
      "[854/1000]\n",
      "- Train Loss : 0.08765050789548291 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044721364974975586 Score : 0.9484702348709106\n",
      "[855/1000]\n",
      "- Train Loss : 0.08762666583061218 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04471232369542122 Score : 0.9484702348709106\n",
      "[856/1000]\n",
      "- Train Loss : 0.08760286639961931 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0447034016251564 Score : 0.9484702348709106\n",
      "[857/1000]\n",
      "- Train Loss : 0.08757922725958957 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04469456151127815 Score : 0.9484702348709106\n",
      "[858/1000]\n",
      "- Train Loss : 0.08755561895668507 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044685691595077515 Score : 0.9484702348709106\n",
      "[859/1000]\n",
      "- Train Loss : 0.08753206984450419 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04467688873410225 Score : 0.9484702348709106\n",
      "[860/1000]\n",
      "- Train Loss : 0.08750859347896443 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044668156653642654 Score : 0.9484702348709106\n",
      "[861/1000]\n",
      "- Train Loss : 0.08748518830786149 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044659458100795746 Score : 0.9484702348709106\n",
      "[862/1000]\n",
      "- Train Loss : 0.08746181045555407 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04465080052614212 Score : 0.9484702348709106\n",
      "[863/1000]\n",
      "- Train Loss : 0.08743848061809938 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04464220255613327 Score : 0.9484702348709106\n",
      "[864/1000]\n",
      "- Train Loss : 0.08741526916209194 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044633664190769196 Score : 0.9484702348709106\n",
      "[865/1000]\n",
      "- Train Loss : 0.08739209930515951 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04462510719895363 Score : 0.9484702348709106\n",
      "[866/1000]\n",
      "- Train Loss : 0.08736894528071086 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0446166954934597 Score : 0.9484702348709106\n",
      "[867/1000]\n",
      "- Train Loss : 0.08734593333469497 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04460827633738518 Score : 0.9484702348709106\n",
      "[868/1000]\n",
      "- Train Loss : 0.08732290752232075 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044599879533052444 Score : 0.9484702348709106\n",
      "[869/1000]\n",
      "- Train Loss : 0.08729999212341176 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04459154233336449 Score : 0.9484702348709106\n",
      "[870/1000]\n",
      "- Train Loss : 0.08727708489944537 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0445832684636116 Score : 0.9484702348709106\n",
      "[871/1000]\n",
      "- Train Loss : 0.08725426759984758 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04457502439618111 Score : 0.9484702348709106\n",
      "[872/1000]\n",
      "- Train Loss : 0.08723154488123125 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04456682875752449 Score : 0.9484702348709106\n",
      "[873/1000]\n",
      "- Train Loss : 0.08720883861598042 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044558681547641754 Score : 0.9484702348709106\n",
      "[874/1000]\n",
      "- Train Loss : 0.08718622372382218 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04455055668950081 Score : 0.9484702348709106\n",
      "[875/1000]\n",
      "- Train Loss : 0.08716361949013339 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04454249516129494 Score : 0.9484702348709106\n",
      "[876/1000]\n",
      "- Train Loss : 0.08714112090981668 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04453445225954056 Score : 0.9484702348709106\n",
      "[877/1000]\n",
      "- Train Loss : 0.0871186778984136 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04452647641301155 Score : 0.9484702348709106\n",
      "[878/1000]\n",
      "- Train Loss : 0.0870962580665946 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04451855272054672 Score : 0.9484702348709106\n",
      "[879/1000]\n",
      "- Train Loss : 0.08707391336146328 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0445106215775013 Score : 0.9484702348709106\n",
      "[880/1000]\n",
      "- Train Loss : 0.08705164978487624 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04450279474258423 Score : 0.9484702348709106\n",
      "[881/1000]\n",
      "- Train Loss : 0.08702941714889473 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04449496045708656 Score : 0.9484702348709106\n",
      "[882/1000]\n",
      "- Train Loss : 0.08700726057092349 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04448721185326576 Score : 0.9484702348709106\n",
      "[883/1000]\n",
      "- Train Loss : 0.08698513038042519 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044479478150606155 Score : 0.9484702348709106\n",
      "[884/1000]\n",
      "- Train Loss : 0.08696307086696227 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044471774250268936 Score : 0.9484702348709106\n",
      "[885/1000]\n",
      "- Train Loss : 0.08694108523842362 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044464148581027985 Score : 0.9484702348709106\n",
      "[886/1000]\n",
      "- Train Loss : 0.08691914027763738 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04445648193359375 Score : 0.9484702348709106\n",
      "[887/1000]\n",
      "- Train Loss : 0.08689727541059256 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04444893077015877 Score : 0.9484702348709106\n",
      "[888/1000]\n",
      "- Train Loss : 0.08687544779645072 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04444136470556259 Score : 0.9484702348709106\n",
      "[889/1000]\n",
      "- Train Loss : 0.08685364408625497 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04443388059735298 Score : 0.9484702348709106\n",
      "[890/1000]\n",
      "- Train Loss : 0.08683196041319105 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04442644119262695 Score : 0.9484702348709106\n",
      "[891/1000]\n",
      "- Train Loss : 0.08681028988212347 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04441901296377182 Score : 0.9484702348709106\n",
      "[892/1000]\n",
      "- Train Loss : 0.08678869582298729 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04441162571310997 Score : 0.9484702348709106\n",
      "[893/1000]\n",
      "- Train Loss : 0.08676714584645298 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044404324144124985 Score : 0.9484702348709106\n",
      "[894/1000]\n",
      "- Train Loss : 0.08674566975484292 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0443970151245594 Score : 0.9484702348709106\n",
      "[895/1000]\n",
      "- Train Loss : 0.08672424354073074 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0443897470831871 Score : 0.9484702348709106\n",
      "[896/1000]\n",
      "- Train Loss : 0.0867028564421667 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04438253119587898 Score : 0.9484702348709106\n",
      "[897/1000]\n",
      "- Train Loss : 0.08668154571205378 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044375352561473846 Score : 0.9484702348709106\n",
      "[898/1000]\n",
      "- Train Loss : 0.08666024294992287 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04436822608113289 Score : 0.9484702348709106\n",
      "[899/1000]\n",
      "- Train Loss : 0.08663906860682699 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04436108469963074 Score : 0.9484702348709106\n",
      "[900/1000]\n",
      "- Train Loss : 0.08661789167672396 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044354040175676346 Score : 0.9484702348709106\n",
      "[901/1000]\n",
      "- Train Loss : 0.08659675955358478 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04434698447585106 Score : 0.9484702348709106\n",
      "[902/1000]\n",
      "- Train Loss : 0.08657573484298256 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04433996230363846 Score : 0.9484702348709106\n",
      "[903/1000]\n",
      "- Train Loss : 0.0865547277757691 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04433305189013481 Score : 0.9484702348709106\n",
      "[904/1000]\n",
      "- Train Loss : 0.08653378171018428 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044326119124889374 Score : 0.9484702348709106\n",
      "[905/1000]\n",
      "- Train Loss : 0.08651289230005609 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044319238513708115 Score : 0.9484702348709106\n",
      "[906/1000]\n",
      "- Train Loss : 0.08649205985582537 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044312380254268646 Score : 0.9484702348709106\n",
      "[907/1000]\n",
      "- Train Loss : 0.08647127128723595 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04430558905005455 Score : 0.9484702348709106\n",
      "[908/1000]\n",
      "- Train Loss : 0.08645059370125334 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04429879039525986 Score : 0.9484702348709106\n",
      "[909/1000]\n",
      "- Train Loss : 0.08642988936561677 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04429206997156143 Score : 0.9484702348709106\n",
      "[910/1000]\n",
      "- Train Loss : 0.08640925953578618 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04428534954786301 Score : 0.9484702348709106\n",
      "[911/1000]\n",
      "- Train Loss : 0.08638865614516868 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04427871108055115 Score : 0.9484702348709106\n",
      "[912/1000]\n",
      "- Train Loss : 0.08636816208147341 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044272083789110184 Score : 0.9484702348709106\n",
      "[913/1000]\n",
      "- Train Loss : 0.08634771732613444 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04426546022295952 Score : 0.9484702348709106\n",
      "[914/1000]\n",
      "- Train Loss : 0.08632729440513584 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04425889253616333 Score : 0.9484702348709106\n",
      "[915/1000]\n",
      "- Train Loss : 0.08630691349713339 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04425240308046341 Score : 0.9484702348709106\n",
      "[916/1000]\n",
      "- Train Loss : 0.08628660885410176 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044245924800634384 Score : 0.9484702348709106\n",
      "[917/1000]\n",
      "- Train Loss : 0.08626633323729038 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044239480048418045 Score : 0.9484702348709106\n",
      "[918/1000]\n",
      "- Train Loss : 0.08624613248846597 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0442330464720726 Score : 0.9484702348709106\n",
      "[919/1000]\n",
      "- Train Loss : 0.08622602233663201 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04422663897275925 Score : 0.9484702348709106\n",
      "[920/1000]\n",
      "- Train Loss : 0.08620588114071223 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04422027990221977 Score : 0.9484702348709106\n",
      "[921/1000]\n",
      "- Train Loss : 0.08618581626150343 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044213976711034775 Score : 0.9484702348709106\n",
      "[922/1000]\n",
      "- Train Loss : 0.0861658347874052 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044207699596881866 Score : 0.9484702348709106\n",
      "[923/1000]\n",
      "- Train Loss : 0.08614587701029247 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044201456010341644 Score : 0.9484702348709106\n",
      "[924/1000]\n",
      "- Train Loss : 0.08612598390835854 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04419522359967232 Score : 0.9484702348709106\n",
      "[925/1000]\n",
      "- Train Loss : 0.08610613090503547 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04418907314538956 Score : 0.9484702348709106\n",
      "[926/1000]\n",
      "- Train Loss : 0.08608632617526585 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04418293014168739 Score : 0.9484702348709106\n",
      "[927/1000]\n",
      "- Train Loss : 0.08606657810095283 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0441768579185009 Score : 0.9484702348709106\n",
      "[928/1000]\n",
      "- Train Loss : 0.08604689470181863 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04417072981595993 Score : 0.9484702348709106\n",
      "[929/1000]\n",
      "- Train Loss : 0.08602721989154816 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044164709746837616 Score : 0.9484702348709106\n",
      "[930/1000]\n",
      "- Train Loss : 0.086007635264347 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04415872320532799 Score : 0.9484702348709106\n",
      "[931/1000]\n",
      "- Train Loss : 0.0859880890800721 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04415271431207657 Score : 0.9484702348709106\n",
      "[932/1000]\n",
      "- Train Loss : 0.08596856736888488 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04414680227637291 Score : 0.9484702348709106\n",
      "[933/1000]\n",
      "- Train Loss : 0.08594912031872405 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04414089396595955 Score : 0.9484702348709106\n",
      "[934/1000]\n",
      "- Train Loss : 0.08592973861636387 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04413500428199768 Score : 0.9484702348709106\n",
      "[935/1000]\n",
      "- Train Loss : 0.08591035603442126 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044129181653261185 Score : 0.9484702348709106\n",
      "[936/1000]\n",
      "- Train Loss : 0.08589106725735797 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0441233329474926 Score : 0.9484702348709106\n",
      "[937/1000]\n",
      "- Train Loss : 0.08587182111417253 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04411757364869118 Score : 0.9484702348709106\n",
      "[938/1000]\n",
      "- Train Loss : 0.0858525910621716 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044111840426921844 Score : 0.9484702348709106\n",
      "[939/1000]\n",
      "- Train Loss : 0.08583345709161626 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044106122106313705 Score : 0.9484702348709106\n",
      "[940/1000]\n",
      "- Train Loss : 0.08581433781526154 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04410044103860855 Score : 0.9484702348709106\n",
      "[941/1000]\n",
      "- Train Loss : 0.08579525812011626 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04409480467438698 Score : 0.9484702348709106\n",
      "[942/1000]\n",
      "- Train Loss : 0.08577626343402597 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0440891869366169 Score : 0.9484702348709106\n",
      "[943/1000]\n",
      "- Train Loss : 0.08575731810803215 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044083595275878906 Score : 0.9484702348709106\n",
      "[944/1000]\n",
      "- Train Loss : 0.0857383409101102 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0440780445933342 Score : 0.9484702348709106\n",
      "[945/1000]\n",
      "- Train Loss : 0.08571949632217486 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044072508811950684 Score : 0.9484702348709106\n",
      "[946/1000]\n",
      "- Train Loss : 0.08570067051591145 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04406704381108284 Score : 0.9484702348709106\n",
      "[947/1000]\n",
      "- Train Loss : 0.08568187037275897 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044061560183763504 Score : 0.9484702348709106\n",
      "[948/1000]\n",
      "- Train Loss : 0.08566313650872973 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04405612498521805 Score : 0.9484702348709106\n",
      "[949/1000]\n",
      "- Train Loss : 0.085644473373476 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04405074939131737 Score : 0.9484702348709106\n",
      "[950/1000]\n",
      "- Train Loss : 0.08562584914680985 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044045355170965195 Score : 0.9484702348709106\n",
      "[951/1000]\n",
      "- Train Loss : 0.08560724292571346 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044040024280548096 Score : 0.9484702348709106\n",
      "[952/1000]\n",
      "- Train Loss : 0.08558868171854152 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044034771621227264 Score : 0.9484702348709106\n",
      "[953/1000]\n",
      "- Train Loss : 0.08557020272645685 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044029463082551956 Score : 0.9484702348709106\n",
      "[954/1000]\n",
      "- Train Loss : 0.08555176916221778 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044024210423231125 Score : 0.9484702348709106\n",
      "[955/1000]\n",
      "- Train Loss : 0.08553405416508515 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044018518179655075 Score : 0.9484702348709106\n",
      "[956/1000]\n",
      "- Train Loss : 0.08551581328113873 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04401255026459694 Score : 0.9484702348709106\n",
      "[957/1000]\n",
      "- Train Loss : 0.08549722568649384 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044006966054439545 Score : 0.9484702348709106\n",
      "[958/1000]\n",
      "- Train Loss : 0.08547872418744697 Score : 0.9593180550469292\n",
      "- Val Loss : 0.044001784175634384 Score : 0.9484702348709106\n",
      "[959/1000]\n",
      "- Train Loss : 0.08546042804502779 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04399671405553818 Score : 0.9484702348709106\n",
      "[960/1000]\n",
      "- Train Loss : 0.0854422686000665 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04399164766073227 Score : 0.9484702348709106\n",
      "[961/1000]\n",
      "- Train Loss : 0.08542424885349141 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04398658126592636 Score : 0.9484702348709106\n",
      "[962/1000]\n",
      "- Train Loss : 0.08540619356143805 Score : 0.9593180550469292\n",
      "- Val Loss : 0.043981488794088364 Score : 0.9484702348709106\n",
      "[963/1000]\n",
      "- Train Loss : 0.0853881865946783 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04397641867399216 Score : 0.9484702348709106\n",
      "[964/1000]\n",
      "- Train Loss : 0.08537020637757248 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04397136718034744 Score : 0.9484702348709106\n",
      "[965/1000]\n",
      "- Train Loss : 0.08535225016789304 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0439663827419281 Score : 0.9484702348709106\n",
      "[966/1000]\n",
      "- Train Loss : 0.08533435201065408 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04396143555641174 Score : 0.9484702348709106\n",
      "[967/1000]\n",
      "- Train Loss : 0.08531648670840594 Score : 0.9593180550469292\n",
      "- Val Loss : 0.043956540524959564 Score : 0.9484702348709106\n",
      "[968/1000]\n",
      "- Train Loss : 0.08529870832959811 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04395164176821709 Score : 0.9484702348709106\n",
      "[969/1000]\n",
      "- Train Loss : 0.08528097630995843 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04394679516553879 Score : 0.9484702348709106\n",
      "[970/1000]\n",
      "- Train Loss : 0.08526327419612142 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0439419224858284 Score : 0.9484702348709106\n",
      "[971/1000]\n",
      "- Train Loss : 0.08524560064284338 Score : 0.9593180550469292\n",
      "- Val Loss : 0.043937139213085175 Score : 0.9484702348709106\n",
      "[972/1000]\n",
      "- Train Loss : 0.08522796077239844 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04393240064382553 Score : 0.9484702348709106\n",
      "[973/1000]\n",
      "- Train Loss : 0.08521038718107674 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04392765089869499 Score : 0.9484702348709106\n",
      "[974/1000]\n",
      "- Train Loss : 0.08519285994892319 Score : 0.9593180550469292\n",
      "- Val Loss : 0.043922942131757736 Score : 0.9484702348709106\n",
      "[975/1000]\n",
      "- Train Loss : 0.08517533540725708 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04391825571656227 Score : 0.9484702348709106\n",
      "[976/1000]\n",
      "- Train Loss : 0.08515792308996122 Score : 0.9593180550469292\n",
      "- Val Loss : 0.043913595378398895 Score : 0.9484702348709106\n",
      "[977/1000]\n",
      "- Train Loss : 0.08514050440862775 Score : 0.9593180550469292\n",
      "- Val Loss : 0.0439089760184288 Score : 0.9484702348709106\n",
      "[978/1000]\n",
      "- Train Loss : 0.08512316504493356 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04390436410903931 Score : 0.9484702348709106\n",
      "[979/1000]\n",
      "- Train Loss : 0.08510584756731987 Score : 0.9593180550469292\n",
      "- Val Loss : 0.043899815529584885 Score : 0.9484702348709106\n",
      "[980/1000]\n",
      "- Train Loss : 0.08508857443100876 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04389524832367897 Score : 0.9484702348709106\n",
      "[981/1000]\n",
      "- Train Loss : 0.08507132028333014 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04389071837067604 Score : 0.9484702348709106\n",
      "[982/1000]\n",
      "- Train Loss : 0.08505411032173368 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04388624429702759 Score : 0.9484702348709106\n",
      "[983/1000]\n",
      "- Train Loss : 0.08503695670515299 Score : 0.9593180550469292\n",
      "- Val Loss : 0.043881792575120926 Score : 0.9484702348709106\n",
      "[984/1000]\n",
      "- Train Loss : 0.0850198490338193 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04387737065553665 Score : 0.9484702348709106\n",
      "[985/1000]\n",
      "- Train Loss : 0.08500281183256043 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04387296363711357 Score : 0.9484702348709106\n",
      "[986/1000]\n",
      "- Train Loss : 0.08498577463130157 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04386857524514198 Score : 0.9484702348709106\n",
      "[987/1000]\n",
      "- Train Loss : 0.08496880821055836 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04386422410607338 Score : 0.9484702348709106\n",
      "[988/1000]\n",
      "- Train Loss : 0.08495186833250853 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04385989159345627 Score : 0.9484702348709106\n",
      "[989/1000]\n",
      "- Train Loss : 0.084934926022672 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04385559633374214 Score : 0.9484702348709106\n",
      "[990/1000]\n",
      "- Train Loss : 0.0849180883831448 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04385131597518921 Score : 0.9484702348709106\n",
      "[991/1000]\n",
      "- Train Loss : 0.08490126414431466 Score : 0.9593180550469292\n",
      "- Val Loss : 0.043847065418958664 Score : 0.9484702348709106\n",
      "[992/1000]\n",
      "- Train Loss : 0.08488450380456117 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04384287819266319 Score : 0.9484702348709106\n",
      "[993/1000]\n",
      "- Train Loss : 0.08486777937246694 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04383862763643265 Score : 0.9484702348709106\n",
      "[994/1000]\n",
      "- Train Loss : 0.08485108469095495 Score : 0.9593180550469292\n",
      "- Val Loss : 0.043834488838911057 Score : 0.9484702348709106\n",
      "[995/1000]\n",
      "- Train Loss : 0.08483445778903034 Score : 0.9593180550469292\n",
      "- Val Loss : 0.043830353766679764 Score : 0.9484702348709106\n",
      "[996/1000]\n",
      "- Train Loss : 0.08481780915624565 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04382622614502907 Score : 0.9484702348709106\n",
      "[997/1000]\n",
      "- Train Loss : 0.08480125194829372 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04382214695215225 Score : 0.9484702348709106\n",
      "[998/1000]\n",
      "- Train Loss : 0.0847847279575136 Score : 0.9593180550469292\n",
      "- Val Loss : 0.043818067759275436 Score : 0.9484702348709106\n",
      "[999/1000]\n",
      "- Train Loss : 0.0847682109516528 Score : 0.9593180550469292\n",
      "- Val Loss : 0.04381402209401131 Score : 0.9484702348709106\n"
     ]
    }
   ],
   "source": [
    "#학습 확인 w. 손실값, 성능평가 지표\n",
    "\n",
    "loss_history=[[],[]]\n",
    "score_history=[[],[]]\n",
    "\n",
    "CNT = len(iris_dl)\n",
    "print(f'CNT : {CNT}')\n",
    "\n",
    "#BATCH_CNT=iris_ds.n_rows/BATCH_SIZE\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "\n",
    "    #학습 모드로 모델 설정\n",
    "    model.train()\n",
    "\n",
    "    total_loss=0\n",
    "    total_score=0\n",
    "\n",
    "    for feature_ts,target_ts in iris_dl:\n",
    "\n",
    "        #학습 진행\n",
    "        pre_y=model(feature_ts)\n",
    "\n",
    "        #손실 계산 w. CrossEntropyLoss (target(정답)이 0차원 또는 1차원, long type이어야 함)\n",
    "        loss=multi_loss(pre_y,target_ts.reshape(-1).long())\n",
    "        total_loss+=loss.item()\n",
    "\n",
    "        #성능 평가\n",
    "        score=MulticlassF1Score(num_classes=3)(pre_y,target_ts.reshape(-1))\n",
    "        #score=F1Score(task='binary)(pre_y,target_ts)\n",
    "        total_score+=score.item()\n",
    "\n",
    "        #최적화 진행\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    #각 에포크 당 검증 수행: 모델을 검증 모드로 설정\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #검증 데이터셋\n",
    "        val_feature_ts=torch.FloatTensor(val_ds.feature_df.values)\n",
    "        val_target_ts=torch.FloatTensor(val_ds.target_df.values)\n",
    "        \n",
    "        #평가\n",
    "        pre_val=model(val_feature_ts)\n",
    "\n",
    "        #손실 계산\n",
    "        val_loss=multi_loss(pre_val,val_target_ts.reshape(-1).long())\n",
    "\n",
    "        #성능 평가\n",
    "        val_score=MulticlassF1Score(num_classes=3)(pre_val,val_target_ts.reshape(-1))\n",
    "\n",
    "    #손실값, 성능평가값 저장\n",
    "    loss_history[0].append(total_loss/CNT)\n",
    "    score_history[0].append(total_score/CNT)\n",
    "\n",
    "    loss_history[1].append(val_loss)\n",
    "    score_history[1].append(val_score)\n",
    "\n",
    "    print(f'[{epoch}/{EPOCH}]\\n- Train Loss : {loss_history[0][-1]} Score : {score_history[0][-1]}')\n",
    "    print(f'- Val Loss : {loss_history[1][-1]} Score : {score_history[1][-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    #테스트 데이터셋\n",
    "    test_feature_ts=torch.FloatTensor(test_ds.feature_df.values)\n",
    "    test_target_ts=torch.FloatTensor(test_ds.target_df.values)\n",
    "    \n",
    "    #평가\n",
    "    pre_val=model(test_feature_ts)\n",
    "\n",
    "    #손실 계산\n",
    "    test_loss=multi_loss(pre_val,test_target_ts.reshape(-1).long())\n",
    "\n",
    "    #성능 평가\n",
    "    test_score=MulticlassF1Score(num_classes=3)(pre_val,test_target_ts.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
