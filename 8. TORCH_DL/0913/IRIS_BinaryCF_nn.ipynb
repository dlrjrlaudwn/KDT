{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[IRIS - Regression]\n",
    "- feature: 3개 (Sepal_Length, Sepal_Width, Petal_Width)\n",
    "- target: 1개 (Petal_Length)\n",
    "\n",
    "[IRIS - Binary Classification]\n",
    "- feature: 4개 (Sepal_Length, Sepal_Width, Petal_Width, Petal_Length)\n",
    "- target: 1개 (Petal_Length)\n",
    "- class: 품종 3개 (두 개의 품종만 선택_ Setosa or other)\n",
    "\n",
    "[IRIS - MultiClassification]\n",
    "- feature: 4개 (Sepal_Length, Sepal_Width, Petal_Width, Petal_Length)\n",
    "- target: 1개 (Petal_Length)\n",
    "- class: 품종 3개\n",
    "\n",
    "- - -\n",
    "\n",
    "[MNIST-Digit]\n",
    "- feature: 64개\n",
    "- target: 1ro\n",
    "- class: 0-9 (10개)\n",
    "\n",
    "[MNIST-Fashion]\n",
    "- feature: 784개\n",
    "- target: 1개\n",
    "- class: 0-9 (10개)\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] 모듈 로딩 및 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모듈 로딩\n",
    "# Model 관련\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.optim as optim\n",
    "from torchmetrics.classification import F1Score,BinaryF1Score\n",
    "from torchmetrics.classification import BinaryConfusionMatrix\n",
    "from torchinfo import summary\n",
    "\n",
    "# Data 및 시각화 관련\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch  v.2.4.1\n",
      "pandas v.2.0.3\n"
     ]
    }
   ],
   "source": [
    "#활용 패키지 버전 체크\n",
    "print(f'torch  v.{torch.__version__}')\n",
    "print(f'pandas v.{pd.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal.length  sepal.width  petal.length  petal.width variety\n",
       "0           5.1          3.5           1.4          0.2  Setosa\n",
       "1           4.9          3.0           1.4          0.2  Setosa\n",
       "2           4.7          3.2           1.3          0.2  Setosa\n",
       "3           4.6          3.1           1.5          0.2  Setosa\n",
       "4           5.0          3.6           1.4          0.2  Setosa"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터 로딩\n",
    "data_file='../data/iris.csv'\n",
    "\n",
    "#csv -> DataFrame\n",
    "iris_df=pd.read_csv(data_file)\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Setosa', 'Versicolor', 'Virginica'], dtype=object)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#타겟 변경: 정수화, 클래스 2개로 바꾸는 작업 필요\n",
    "\n",
    "#3개의 클래스를 2개로 바꾸기기\n",
    "iris_df['variety'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df['variety']=(iris_df['variety']=='Setosa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal.length  sepal.width  petal.length  petal.width  variety\n",
       "0             5.1          3.5           1.4          0.2     True\n",
       "1             4.9          3.0           1.4          0.2     True\n",
       "2             4.7          3.2           1.3          0.2     True\n",
       "3             4.6          3.1           1.5          0.2     True\n",
       "4             5.0          3.6           1.4          0.2     True\n",
       "..            ...          ...           ...          ...      ...\n",
       "145           6.7          3.0           5.2          2.3    False\n",
       "146           6.3          2.5           5.0          1.9    False\n",
       "147           6.5          3.0           5.2          2.0    False\n",
       "148           6.2          3.4           5.4          2.3    False\n",
       "149           5.9          3.0           5.1          1.8    False\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df['variety']=iris_df['variety'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df['variety'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal.length  sepal.width  petal.length  petal.width  variety\n",
       "0             5.1          3.5           1.4          0.2        1\n",
       "1             4.9          3.0           1.4          0.2        1\n",
       "2             4.7          3.2           1.3          0.2        1\n",
       "3             4.6          3.1           1.5          0.2        1\n",
       "4             5.0          3.6           1.4          0.2        1\n",
       "..            ...          ...           ...          ...      ...\n",
       "145           6.7          3.0           5.2          2.3        0\n",
       "146           6.3          2.5           5.0          1.9        0\n",
       "147           6.5          3.0           5.2          2.0        0\n",
       "148           6.2          3.4           5.4          2.3        0\n",
       "149           5.9          3.0           5.1          1.8        0\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels=dict(zip(iris_df['variety'].unique().tolist(),range(3)))\n",
    "# iris_df['variety']=iris_df['variety'].replace(labels)\n",
    "\n",
    "iris_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - - \n",
    "[2] 모델 클래스 설계 및 정의\n",
    "- 클래스 목적: iris 데이터 학습 및 추론\n",
    "- 클래스 이름: iris_bcf_model\n",
    "- 부모 클래스: nn.Module\n",
    "- 매개변수: 각 층별 입출력 개수 고정 => 필요 X\n",
    "- 클래스 속성: feature_df, target_df, n_rows, n_features\n",
    "- 클래스 기능: __init__() <- 모델 구조 설정, forward() <- 순방향 학습(오버라이딩(상속 관계에서만 가능) 필요)\n",
    "- 클래스 구조\n",
    "    - 입력층:  4개 입력 (feature 개수) 10개 출력 (=퍼셉트론 10개) \n",
    "    - 은닉층: 10개 입력                 5개 출력 (=퍼셉트론 30개)\n",
    "    - 출력층:  5개 입력                 1개 출력 (이진분류의 결과)\n",
    "\n",
    "- 활성화함수\n",
    "    - 클래스 형태 (대문자로 시작)\n",
    "        - nn.MSELoss, nn.ReLU 등\n",
    "        - _ _init_ _() 메서드에서 사용됨\n",
    "        - 하나의 층처럼 사용 가능\n",
    "    - 함수 형태 (소문자로 시작)\n",
    "        - torch.nn.functional.relu 등\n",
    "        - forward 메서드에 사용됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iris_bcf_model(nn.Module):\n",
    "\n",
    "    #모델 구조 구성 및 인스턴스 생성 메서드\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_layer=nn.Linear(4,10)\n",
    "        self.hidden_layer=nn.Linear(10,5)\n",
    "        self.out_layer=nn.Linear(5,1)\n",
    "\n",
    "    #순방향 학습 진행 메서드\n",
    "    def forward(self,input_data):\n",
    "\n",
    "        #입력층\n",
    "        y=self.in_layer(input_data)     \n",
    "        y=F.relu(y)                     # y 값: 0 이상\n",
    "\n",
    "        #은닉층\n",
    "        y=self.hidden_layer(y)\n",
    "        y=F.relu(y)\n",
    "\n",
    "        #출력층\n",
    "        return F.sigmoid(self.out_layer(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris_bcf_model(\n",
      "  (in_layer): Linear(in_features=4, out_features=10, bias=True)\n",
      "  (hidden_layer): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (out_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#모델 인스턴스 생성_확인용\n",
    "model=iris_bcf_model()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "iris_bcf_model                           [17, 1]                   --\n",
       "├─Linear: 1-1                            [17, 10]                  50\n",
       "├─Linear: 1-2                            [17, 5]                   55\n",
       "├─Linear: 1-3                            [17, 1]                   6\n",
       "==========================================================================================\n",
       "Total params: 111\n",
       "Trainable params: 111\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model,input_size=(17,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "[3] 데이터셋 클래스 설계 및 정의\n",
    "- 데이터셋: iris.csv\n",
    "- feature: 4개\n",
    "- target: 1개\n",
    "- 클래스 이름: iris_ds\n",
    "- 부모 클래스: utils.data.Dataset\n",
    "- 클래스 속성(필드): feature_df,target_df,n_rows,n_features\n",
    "- 필수 메서드: \n",
    "    - _ _ init _ _ (self): 데이터셋 저장 및 전처리, 개발자가 필요한 속성 설정\n",
    "    - _ _ len _ _ (self): 데이터 개수 반환\n",
    "    - _ _ getItem _ _ (self): 특정 인덱스의 feature/target 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iris_dataset(Dataset):\n",
    "    def __init__(self,feature_df,target_df):\n",
    "        self.feature_df=feature_df\n",
    "        self.target_df=target_df\n",
    "\n",
    "        self.n_rows=feature_df.shape[0]\n",
    "        self.n_features=feature_df.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_rows\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        #tensor화\n",
    "        feature_ts=torch.FloatTensor(self.feature_df.iloc[index].values)\n",
    "        target_ts=torch.FloatTensor(self.target_df.iloc[index].values)\n",
    "\n",
    "        #feature/target 반환\n",
    "        return feature_ts,target_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4]) torch.Size([1, 1]) tensor([[5.1000, 3.5000, 1.4000, 0.2000]]) tensor([[1.]])\n"
     ]
    }
   ],
   "source": [
    "#데이터셋 인스턴스 생성\n",
    "\n",
    "feature_df=iris_df[iris_df.columns[:-1]]    #2D (150,3)\n",
    "target_df=iris_df[iris_df.columns[-1:]]     #2D (150,1)\n",
    "\n",
    "iris_ds=iris_dataset(feature_df,target_df)\n",
    "\n",
    "iris_dl=DataLoader(iris_ds)\n",
    "for feature,label in iris_dl:\n",
    "    print(feature.shape,label.shape, feature,label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "[4] 학습 준비\n",
    "- 학습 횟수: EPOCH          #회독 횟수\n",
    "- 배치 크기: BATCH_SIZE     #한번에 학습할 데이터 양\n",
    "- 위치 지정: DEVICE         #텐서 저장 및 실행 위치 (GPU or CPU)\n",
    "- 학습률(LR)\n",
    "    - 가중치와 절편 업데이트 시 경사하강법으로 업데이트 간격 설정\n",
    "    - 0.001 ~ 0.1 사이 값 주로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_CNT: 15\n"
     ]
    }
   ],
   "source": [
    "#학습 진행 관련 설정\n",
    "\n",
    "EPOCH=1000         #처음에 1로 설정해서 잘 돌아가는지 확인하고 올리기\n",
    "BATCH_SIZE=10\n",
    "BATCH_CNT=iris_df.shape[0]//BATCH_SIZE\n",
    "DEVICE='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "LR=0.001\n",
    "\n",
    "print(f'BATCH_CNT: {BATCH_CNT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 인스턴스: 모델, 데이터셋, 최적화, (손실함수, 성능지표)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (84, 4), x_test: (38, 4), x_val: (28, 4)\n",
      "y_train: (84, 1), y_test: (38, 1), y_val: (28, 1)\n",
      "<class 'pandas.core.frame.DataFrame'>, <class 'pandas.core.frame.DataFrame'>, <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#모델 인스턴스\n",
    "model=iris_bcf_model()\n",
    "\n",
    "#데이터셋 인스턴스\n",
    "x_train,x_test,y_train,y_test=train_test_split(feature_df,target_df,random_state=1)\n",
    "x_train,x_val,y_train,y_val=train_test_split(x_train,y_train,random_state=1)\n",
    "print(f'x_train: {x_train.shape}, x_test: {x_test.shape}, x_val: {x_val.shape}')\n",
    "print(f'y_train: {y_train.shape}, y_test: {y_test.shape}, y_val: {y_val.shape}')\n",
    "print(f'{type(x_train)}, {type(x_test)}, {type(x_val)}')\n",
    "\n",
    "#iris_ds=iris_dataset(feature_df,target_df)\n",
    "\n",
    "train_ds=iris_dataset(x_train,y_train)\n",
    "val_ds=iris_dataset(x_val,y_val)\n",
    "test_ds=iris_dataset(x_test,y_test)\n",
    "\n",
    "#최적화 인스턴스\n",
    "iris_dl=DataLoader(train_ds,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최적화, 손실함수 인스턴스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#최적화 인스턴스: model.parameters() 전달\n",
    "optimizer=optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "#손실함수 인스턴스: 이진분류 => BinaryCrossEntropyLoss(BCELoss) 사용\n",
    "#                             예측값을 확률값으로 전달 => Sigmoid() AF 처리 후 전달\n",
    "binary_loss=nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "[5] 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNT : 150\n",
      "[0/1000]\n",
      "- Train Loss : 3.1309355684191245e-09 Score : 0.3333333333333333\n",
      "- Val Loss : 6.712311956258432e-11 Score : 1.0\n",
      "[1/1000]\n",
      "- Train Loss : 2.4783997493825845e-09 Score : 0.3333333333333333\n",
      "- Val Loss : 8.861952144334495e-11 Score : 1.0\n",
      "[2/1000]\n",
      "- Train Loss : 1.2590038390833909e-09 Score : 0.3333333333333333\n",
      "- Val Loss : 9.223051489204437e-11 Score : 1.0\n",
      "[3/1000]\n",
      "- Train Loss : 1.7593189218578698e-10 Score : 0.3333333333333333\n",
      "- Val Loss : 7.559910336629727e-11 Score : 1.0\n",
      "[4/1000]\n",
      "- Train Loss : 1.2304577842547862e-09 Score : 0.3333333333333333\n",
      "- Val Loss : 8.169314530404037e-11 Score : 1.0\n",
      "[5/1000]\n",
      "- Train Loss : 1.5672459222058549e-10 Score : 0.3333333333333333\n",
      "- Val Loss : 6.695886206609103e-11 Score : 1.0\n",
      "[6/1000]\n",
      "- Train Loss : 1.0889420453599322e-10 Score : 0.3333333333333333\n",
      "- Val Loss : 5.786725959522343e-11 Score : 1.0\n",
      "[7/1000]\n",
      "- Train Loss : 1.1653032445094732e-09 Score : 0.3333333333333333\n",
      "- Val Loss : 6.918492861940351e-11 Score : 1.0\n",
      "[8/1000]\n",
      "- Train Loss : 1.3241492735620395e-10 Score : 0.3333333333333333\n",
      "- Val Loss : 5.6654052976723435e-11 Score : 1.0\n",
      "[9/1000]\n",
      "- Train Loss : 9.19996783748572e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 4.893530372584998e-11 Score : 1.0\n",
      "[10/1000]\n",
      "- Train Loss : 7.096349816198644e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 4.340872800101536e-11 Score : 1.0\n",
      "[11/1000]\n",
      "- Train Loss : 5.849374689612006e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 4.000368439283086e-11 Score : 1.0\n",
      "[12/1000]\n",
      "- Train Loss : 1.1487112289948787e-09 Score : 0.3333333333333333\n",
      "- Val Loss : 5.156449919829775e-11 Score : 1.0\n",
      "[13/1000]\n",
      "- Train Loss : 9.686616292746578e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 4.251048818515457e-11 Score : 1.0\n",
      "[14/1000]\n",
      "- Train Loss : 6.84863481767262e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 3.683573931989592e-11 Score : 1.0\n",
      "[15/1000]\n",
      "- Train Loss : 5.325160672570468e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 3.2737083877076856e-11 Score : 1.0\n",
      "[16/1000]\n",
      "- Train Loss : 4.5106276062665835e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 3.0132545764116614e-11 Score : 1.0\n",
      "[17/1000]\n",
      "- Train Loss : 4.155995956009145e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 2.7779334388355892e-11 Score : 1.0\n",
      "[18/1000]\n",
      "- Train Loss : 3.8346782475123696e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 2.564537471272388e-11 Score : 1.0\n",
      "[19/1000]\n",
      "- Train Loss : 3.5430555258798465e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 2.37066043878853e-11 Score : 1.0\n",
      "[20/1000]\n",
      "- Train Loss : 3.277881103869256e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 2.1941951727777464e-11 Score : 1.0\n",
      "[21/1000]\n",
      "- Train Loss : 3.036337256651045e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 2.0333410302719734e-11 Score : 1.0\n",
      "[22/1000]\n",
      "- Train Loss : 2.8159777569251164e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 1.8865031065073978e-11 Score : 1.0\n",
      "[23/1000]\n",
      "- Train Loss : 1.2538455519852903e-09 Score : 0.3333333333333333\n",
      "- Val Loss : 2.068796349508073e-11 Score : 1.0\n",
      "[24/1000]\n",
      "- Train Loss : 3.253276343332225e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 1.858587936331979e-11 Score : 1.0\n",
      "[25/1000]\n",
      "- Train Loss : 2.7323216378020927e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 1.697743334805324e-11 Score : 1.0\n",
      "[26/1000]\n",
      "- Train Loss : 2.4434309274476275e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 1.586227503513893e-11 Score : 1.0\n",
      "[27/1000]\n",
      "- Train Loss : 2.28512230972834e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 1.4830633249252045e-11 Score : 1.0\n",
      "[28/1000]\n",
      "- Train Loss : 2.1383438283411625e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 1.3874602855512563e-11 Score : 1.0\n",
      "[29/1000]\n",
      "- Train Loss : 2.0022048772447828e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2988569421390483e-11 Score : 1.0\n",
      "[30/1000]\n",
      "- Train Loss : 1.875958528494841e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2167262856965788e-11 Score : 1.0\n",
      "[31/1000]\n",
      "- Train Loss : 1.7588373661306106e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 1.140573659824673e-11 Score : 1.0\n",
      "[32/1000]\n",
      "- Train Loss : 1.650166058082889e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0699502915600956e-11 Score : 1.0\n",
      "[33/1000]\n",
      "- Train Loss : 1.549271746116892e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0044065405778735e-11 Score : 1.0\n",
      "[34/1000]\n",
      "- Train Loss : 1.4556040127440542e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 9.43588263080386e-12 Score : 1.0\n",
      "[35/1000]\n",
      "- Train Loss : 1.3686148351055928e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 8.871326416526326e-12 Score : 1.0\n",
      "[36/1000]\n",
      "- Train Loss : 1.287763871190749e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 8.346928183355917e-12 Score : 1.0\n",
      "[37/1000]\n",
      "- Train Loss : 1.21265625184249e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 7.859954874456232e-12 Score : 1.0\n",
      "[38/1000]\n",
      "- Train Loss : 1.1428381841845184e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 7.407380264723429e-12 Score : 1.0\n",
      "[39/1000]\n",
      "- Train Loss : 1.0779015841780566e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 6.986578850176617e-12 Score : 1.0\n",
      "[40/1000]\n",
      "- Train Loss : 1.017466307444171e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 6.5951996468249785e-12 Score : 1.0\n",
      "[41/1000]\n",
      "- Train Loss : 9.61225795830112e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 6.23106687774877e-12 Score : 1.0\n",
      "[42/1000]\n",
      "- Train Loss : 9.08857710861762e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 5.8920472667534085e-12 Score : 1.0\n",
      "[43/1000]\n",
      "- Train Loss : 8.600863256084681e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 5.576512342175821e-12 Score : 1.0\n",
      "[44/1000]\n",
      "- Train Loss : 8.146445144505117e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 5.2826159245566995e-12 Score : 1.0\n",
      "[45/1000]\n",
      "- Train Loss : 7.722949187529347e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 5.008687908869547e-12 Score : 1.0\n",
      "[46/1000]\n",
      "- Train Loss : 1.2351093397962156e-09 Score : 0.3333333333333333\n",
      "- Val Loss : 6.633280296569621e-12 Score : 1.0\n",
      "[47/1000]\n",
      "- Train Loss : 1.2015966736403013e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 6.2295997353689625e-12 Score : 1.0\n",
      "[48/1000]\n",
      "- Train Loss : 1.0939204868812581e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 5.908998117198916e-12 Score : 1.0\n",
      "[49/1000]\n",
      "- Train Loss : 1.0004722085256706e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 5.6162774084156375e-12 Score : 1.0\n",
      "[50/1000]\n",
      "- Train Loss : 9.186972714262201e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 5.347793822357838e-12 Score : 1.0\n",
      "[51/1000]\n",
      "- Train Loss : 8.466631559481777e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 5.100437867194829e-12 Score : 1.0\n",
      "[52/1000]\n",
      "- Train Loss : 7.880356639704694e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 4.913923001143017e-12 Score : 1.0\n",
      "[53/1000]\n",
      "- Train Loss : 7.59656996502466e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 4.7327189910129075e-12 Score : 1.0\n",
      "[54/1000]\n",
      "- Train Loss : 7.320367974183134e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 4.5567191513107286e-12 Score : 1.0\n",
      "[55/1000]\n",
      "- Train Loss : 7.0520442653764054e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 4.38615203185444e-12 Score : 1.0\n",
      "[56/1000]\n",
      "- Train Loss : 6.791914506311776e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 4.221082684774391e-12 Score : 1.0\n",
      "[57/1000]\n",
      "- Train Loss : 6.539985491819382e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 4.0615770295626685e-12 Score : 1.0\n",
      "[58/1000]\n",
      "- Train Loss : 6.2964058223698935e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 3.907658484986198e-12 Score : 1.0\n",
      "[59/1000]\n",
      "- Train Loss : 6.061312704043072e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 3.759343964598871e-12 Score : 1.0\n",
      "[60/1000]\n",
      "- Train Loss : 5.834664787157599e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 3.6166805227749732e-12 Score : 1.0\n",
      "[61/1000]\n",
      "- Train Loss : 5.6165951072144986e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 3.4796054926289344e-12 Score : 1.0\n",
      "[62/1000]\n",
      "- Train Loss : 5.406914554493985e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 3.3480134897095892e-12 Score : 1.0\n",
      "[63/1000]\n",
      "- Train Loss : 5.20549169628341e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 3.221854857557438e-12 Score : 1.0\n",
      "[64/1000]\n",
      "- Train Loss : 5.0122613802677195e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 3.1009359576644746e-12 Score : 1.0\n",
      "[65/1000]\n",
      "- Train Loss : 4.8270236857198264e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 2.985223179763352e-12 Score : 1.0\n",
      "[66/1000]\n",
      "- Train Loss : 4.649556012973698e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 2.874515079090423e-12 Score : 1.0\n",
      "[67/1000]\n",
      "- Train Loss : 4.479721848023626e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 2.7687168963758113e-12 Score : 1.0\n",
      "[68/1000]\n",
      "- Train Loss : 4.317362587267897e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 2.6676636160488654e-12 Score : 1.0\n",
      "[69/1000]\n",
      "- Train Loss : 4.1621633706403325e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 2.5711635511654896e-12 Score : 1.0\n",
      "[70/1000]\n",
      "- Train Loss : 4.013877440013241e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 2.4790443135802587e-12 Score : 1.0\n",
      "[71/1000]\n",
      "- Train Loss : 3.87223758967958e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 2.3911898936607168e-12 Score : 1.0\n",
      "[72/1000]\n",
      "- Train Loss : 3.7370651983589686e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 2.3074181454418863e-12 Score : 1.0\n",
      "[73/1000]\n",
      "- Train Loss : 3.6081315237294917e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 2.2275473566396586e-12 Score : 1.0\n",
      "[74/1000]\n",
      "- Train Loss : 3.485071508280286e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 2.1513819371821175e-12 Score : 1.0\n",
      "[75/1000]\n",
      "- Train Loss : 3.3677437022360425e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 2.078821273107656e-12 Score : 1.0\n",
      "[76/1000]\n",
      "- Train Loss : 3.2558639525153735e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 2.009675412195655e-12 Score : 1.0\n",
      "[77/1000]\n",
      "- Train Loss : 3.149169832861424e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.9437424760015976e-12 Score : 1.0\n",
      "[78/1000]\n",
      "- Train Loss : 3.0473732679600904e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.880892143424351e-12 Score : 1.0\n",
      "[79/1000]\n",
      "- Train Loss : 2.9503067689093883e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.8209886723519197e-12 Score : 1.0\n",
      "[80/1000]\n",
      "- Train Loss : 2.8577376362532415e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.7638887312571017e-12 Score : 1.0\n",
      "[81/1000]\n",
      "- Train Loss : 2.7694596359985658e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.709480864156565e-12 Score : 1.0\n",
      "[82/1000]\n",
      "- Train Loss : 2.6852605020597032e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.657557880015148e-12 Score : 1.0\n",
      "[83/1000]\n",
      "- Train Loss : 2.6049144983574833e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.6080442099414283e-12 Score : 1.0\n",
      "[84/1000]\n",
      "- Train Loss : 2.5282634094315092e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.560826121127512e-12 Score : 1.0\n",
      "[85/1000]\n",
      "- Train Loss : 2.455102202263407e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.5157556199768552e-12 Score : 1.0\n",
      "[86/1000]\n",
      "- Train Loss : 2.3851947379795214e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.4727096495428804e-12 Score : 1.0\n",
      "[87/1000]\n",
      "- Train Loss : 2.318468280861467e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.4316265187219734e-12 Score : 1.0\n",
      "[88/1000]\n",
      "- Train Loss : 2.254701749324874e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.3923514034439033e-12 Score : 1.0\n",
      "[89/1000]\n",
      "- Train Loss : 2.1937454245360238e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.3548239136476625e-12 Score : 1.0\n",
      "[90/1000]\n",
      "- Train Loss : 2.135452689161324e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.3189500490048967e-12 Score : 1.0\n",
      "[91/1000]\n",
      "- Train Loss : 2.0797003889496626e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2846025241805559e-12 Score : 1.0\n",
      "[92/1000]\n",
      "- Train Loss : 2.0263236209426055e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2517515236148968e-12 Score : 1.0\n",
      "[93/1000]\n",
      "- Train Loss : 1.9752426802114866e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2203094437723827e-12 Score : 1.0\n",
      "[94/1000]\n",
      "- Train Loss : 1.926325182985233e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.190204510469195e-12 Score : 1.0\n",
      "[95/1000]\n",
      "- Train Loss : 1.8794561396499017e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1613368686852477e-12 Score : 1.0\n",
      "[96/1000]\n",
      "- Train Loss : 1.8345027716247084e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1336649934773346e-12 Score : 1.0\n",
      "[97/1000]\n",
      "- Train Loss : 1.7914014561896252e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.107119604326634e-12 Score : 1.0\n",
      "[98/1000]\n",
      "- Train Loss : 1.7500184551843163e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0816336975388863e-12 Score : 1.0\n",
      "[99/1000]\n",
      "- Train Loss : 1.710306940369608e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.057190576400635e-12 Score : 1.0\n",
      "[100/1000]\n",
      "- Train Loss : 1.6721817060707145e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0337062152435128e-12 Score : 1.0\n",
      "[101/1000]\n",
      "- Train Loss : 1.6355436824509998e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0111297649856299e-12 Score : 1.0\n",
      "[102/1000]\n",
      "- Train Loss : 1.6003099889603117e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 9.894242543329046e-13 Score : 1.0\n",
      "[103/1000]\n",
      "- Train Loss : 1.5664168027677129e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 9.685361236980161e-13 Score : 1.0\n",
      "[104/1000]\n",
      "- Train Loss : 1.5338175187832894e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 9.484526879155464e-13 Score : 1.0\n",
      "[105/1000]\n",
      "- Train Loss : 1.502438255850186e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 9.291193031960021e-13 Score : 1.0\n",
      "[106/1000]\n",
      "- Train Loss : 1.4722265601424528e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 9.104946614366116e-13 Score : 1.0\n",
      "[107/1000]\n",
      "- Train Loss : 1.44310668138794e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 8.925433634364432e-13 Score : 1.0\n",
      "[108/1000]\n",
      "- Train Loss : 1.4150444102010548e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 8.752299557844567e-13 Score : 1.0\n",
      "[109/1000]\n",
      "- Train Loss : 1.3879706544023745e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 8.585327544372023e-13 Score : 1.0\n",
      "[110/1000]\n",
      "- Train Loss : 1.3618456178301459e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 8.42424220659499e-13 Score : 1.0\n",
      "[111/1000]\n",
      "- Train Loss : 1.336626688108283e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 8.268610405745558e-13 Score : 1.0\n",
      "[112/1000]\n",
      "- Train Loss : 1.3122665720591692e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 8.118364379187948e-13 Score : 1.0\n",
      "[113/1000]\n",
      "- Train Loss : 1.2887252886230906e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 7.973163145338913e-13 Score : 1.0\n",
      "[114/1000]\n",
      "- Train Loss : 1.2659794480166195e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 7.832636991257635e-13 Score : 1.0\n",
      "[115/1000]\n",
      "- Train Loss : 1.2439835729271013e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 7.696954510381937e-13 Score : 1.0\n",
      "[116/1000]\n",
      "- Train Loss : 1.2227045365673977e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 7.565542159762573e-13 Score : 1.0\n",
      "[117/1000]\n",
      "- Train Loss : 1.2021015172777174e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 7.438234056467152e-13 Score : 1.0\n",
      "[118/1000]\n",
      "- Train Loss : 1.1821284272466317e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 7.315009058553312e-13 Score : 1.0\n",
      "[119/1000]\n",
      "- Train Loss : 1.1628042439087265e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 7.19557172092905e-13 Score : 1.0\n",
      "[120/1000]\n",
      "- Train Loss : 1.144059444727047e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 7.079814165478204e-13 Score : 1.0\n",
      "[121/1000]\n",
      "- Train Loss : 9.045996112860183e-10 Score : 0.3333333333333333\n",
      "- Val Loss : 4.392666785868471e-12 Score : 1.0\n",
      "[122/1000]\n",
      "- Train Loss : 3.036675923116225e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 3.5797372011092676e-12 Score : 1.0\n",
      "[123/1000]\n",
      "- Train Loss : 2.081483032508546e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 3.0944311783104306e-12 Score : 1.0\n",
      "[124/1000]\n",
      "- Train Loss : 1.5893948665269398e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 2.7549969684043107e-12 Score : 1.0\n",
      "[125/1000]\n",
      "- Train Loss : 1.2828162834138351e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 2.498284782173621e-12 Score : 1.0\n",
      "[126/1000]\n",
      "- Train Loss : 1.0717783453115974e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 2.294476024108927e-12 Score : 1.0\n",
      "[127/1000]\n",
      "- Train Loss : 9.181599316937046e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 2.127562015452611e-12 Score : 1.0\n",
      "[128/1000]\n",
      "- Train Loss : 8.013629617185448e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.986755161428877e-12 Score : 1.0\n",
      "[129/1000]\n",
      "- Train Loss : 7.086716190544099e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.8657445380321214e-12 Score : 1.0\n",
      "[130/1000]\n",
      "- Train Loss : 6.33294952563e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.7602461372182021e-12 Score : 1.0\n",
      "[131/1000]\n",
      "- Train Loss : 5.708393961472694e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.6672108572174382e-12 Score : 1.0\n",
      "[132/1000]\n",
      "- Train Loss : 5.1827176423203334e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.5843603555845687e-12 Score : 1.0\n",
      "[133/1000]\n",
      "- Train Loss : 4.734447343907533e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.5099684740407793e-12 Score : 1.0\n",
      "[134/1000]\n",
      "- Train Loss : 4.348055415840901e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.442706956504558e-12 Score : 1.0\n",
      "[135/1000]\n",
      "- Train Loss : 4.011849033942365e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.3815404983213986e-12 Score : 1.0\n",
      "[136/1000]\n",
      "- Train Loss : 3.716935428253488e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.3256248312595864e-12 Score : 1.0\n",
      "[137/1000]\n",
      "- Train Loss : 3.45645468290101e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2742984835736881e-12 Score : 1.0\n",
      "[138/1000]\n",
      "- Train Loss : 3.2250191562823142e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2269657945901402e-12 Score : 1.0\n",
      "[139/1000]\n",
      "- Train Loss : 3.0180942535548807e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1831857108651755e-12 Score : 1.0\n",
      "[140/1000]\n",
      "- Train Loss : 2.832303183895297e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.142557294435409e-12 Score : 1.0\n",
      "[141/1000]\n",
      "- Train Loss : 2.6646727370429683e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1047404310793318e-12 Score : 1.0\n",
      "[142/1000]\n",
      "- Train Loss : 2.5129089684148134e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0694547461151394e-12 Score : 1.0\n",
      "[143/1000]\n",
      "- Train Loss : 2.3749254191265427e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0364532582879393e-12 Score : 1.0\n",
      "[144/1000]\n",
      "- Train Loss : 2.2491121604751123e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0055157661365e-12 Score : 1.0\n",
      "[145/1000]\n",
      "- Train Loss : 2.1339915889502306e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 9.764652194460544e-13 Score : 1.0\n",
      "[146/1000]\n",
      "- Train Loss : 2.028422398223585e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 9.491361689650812e-13 Score : 1.0\n",
      "[147/1000]\n",
      "- Train Loss : 1.9313177414325675e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 9.233754169266084e-13 Score : 1.0\n",
      "[148/1000]\n",
      "- Train Loss : 1.8417808824961643e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 8.990638637219883e-13 Score : 1.0\n",
      "[149/1000]\n",
      "- Train Loss : 1.7590424054751855e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 8.760827892133338e-13 Score : 1.0\n",
      "[150/1000]\n",
      "- Train Loss : 1.6824192599728612e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 8.543235021328532e-13 Score : 1.0\n",
      "[151/1000]\n",
      "- Train Loss : 1.6113228410756567e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 8.336965015912079e-13 Score : 1.0\n",
      "[152/1000]\n",
      "- Train Loss : 1.545232442061394e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 8.141316939179466e-13 Score : 1.0\n",
      "[153/1000]\n",
      "- Train Loss : 1.4836771776773651e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 7.955375724497116e-13 Score : 1.0\n",
      "[154/1000]\n",
      "- Train Loss : 1.4262471677376368e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 7.77846157710288e-13 Score : 1.0\n",
      "[155/1000]\n",
      "- Train Loss : 1.3725877992473336e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 7.610101242748468e-13 Score : 1.0\n",
      "[156/1000]\n",
      "- Train Loss : 1.322385429328847e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 7.449735815213965e-13 Score : 1.0\n",
      "[157/1000]\n",
      "- Train Loss : 1.2753380257168711e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 7.296676826119841e-13 Score : 1.0\n",
      "[158/1000]\n",
      "- Train Loss : 1.2392623828109713e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 7.181110090251352e-13 Score : 1.0\n",
      "[159/1000]\n",
      "- Train Loss : 1.219936700303713e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 7.068157365820726e-13 Score : 1.0\n",
      "[160/1000]\n",
      "- Train Loss : 1.2010297480423826e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 6.957764442719339e-13 Score : 1.0\n",
      "[161/1000]\n",
      "- Train Loss : 1.1825349627479545e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 6.849906926398308e-13 Score : 1.0\n",
      "[162/1000]\n",
      "- Train Loss : 1.1644553639888379e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 6.744507838503389e-13 Score : 1.0\n",
      "[163/1000]\n",
      "- Train Loss : 1.146808583542289e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 6.64163060486167e-13 Score : 1.0\n",
      "[164/1000]\n",
      "- Train Loss : 1.129561736284733e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 6.541244867812324e-13 Score : 1.0\n",
      "[165/1000]\n",
      "- Train Loss : 1.11272134220346e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 6.443214559982702e-13 Score : 1.0\n",
      "[166/1000]\n",
      "- Train Loss : 1.0962638889595974e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 6.347405240303416e-13 Score : 1.0\n",
      "[167/1000]\n",
      "- Train Loss : 1.0801923804518158e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 6.254023449288326e-13 Score : 1.0\n",
      "[168/1000]\n",
      "- Train Loss : 1.0645126613704665e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 6.162826325650794e-13 Score : 1.0\n",
      "[169/1000]\n",
      "- Train Loss : 1.049193859205584e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 6.073782427527818e-13 Score : 1.0\n",
      "[170/1000]\n",
      "- Train Loss : 1.0342351062570464e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 5.986883623403105e-13 Score : 1.0\n",
      "[171/1000]\n",
      "- Train Loss : 1.0196442336727154e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 5.902129371175568e-13 Score : 1.0\n",
      "[172/1000]\n",
      "- Train Loss : 1.0054007185398714e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 5.819441608288789e-13 Score : 1.0\n",
      "[173/1000]\n",
      "- Train Loss : 9.91497758076516e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 5.738613252127822e-13 Score : 1.0\n",
      "[174/1000]\n",
      "- Train Loss : 9.779077243256472e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 5.659749470303399e-13 Score : 1.0\n",
      "[175/1000]\n",
      "- Train Loss : 9.646517967889575e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 5.582791173797119e-13 Score : 1.0\n",
      "[176/1000]\n",
      "- Train Loss : 9.516948137345379e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 5.507625605583044e-13 Score : 1.0\n",
      "[177/1000]\n",
      "- Train Loss : 9.390538512333187e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 5.434290712737211e-13 Score : 1.0\n",
      "[178/1000]\n",
      "- Train Loss : 9.267056009921059e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 5.362631454348954e-13 Score : 1.0\n",
      "[179/1000]\n",
      "- Train Loss : 9.146383490636373e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 5.292681982786707e-13 Score : 1.0\n",
      "[180/1000]\n",
      "- Train Loss : 9.028664041031198e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 5.224404350974432e-13 Score : 1.0\n",
      "[181/1000]\n",
      "- Train Loss : 8.91368796604618e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 5.157725917366573e-13 Score : 1.0\n",
      "[182/1000]\n",
      "- Train Loss : 8.801294853880099e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 5.092452609774256e-13 Score : 1.0\n",
      "[183/1000]\n",
      "- Train Loss : 8.691358270734021e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 5.028754105837474e-13 Score : 1.0\n",
      "[184/1000]\n",
      "- Train Loss : 8.584015125477438e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 4.966516564328116e-13 Score : 1.0\n",
      "[185/1000]\n",
      "- Train Loss : 8.479077006011505e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 4.905667343700626e-13 Score : 1.0\n",
      "[186/1000]\n",
      "- Train Loss : 8.376578085720723e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 4.846201565045227e-13 Score : 1.0\n",
      "[187/1000]\n",
      "- Train Loss : 8.276290463923656e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 4.788069355061986e-13 Score : 1.0\n",
      "[188/1000]\n",
      "- Train Loss : 8.178235496300812e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 4.731194277497741e-13 Score : 1.0\n",
      "[189/1000]\n",
      "- Train Loss : 8.082302333395521e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 4.675628374056773e-13 Score : 1.0\n",
      "[190/1000]\n",
      "- Train Loss : 7.988517745955222e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 4.62126214031966e-13 Score : 1.0\n",
      "[191/1000]\n",
      "- Train Loss : 7.896765385780039e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 4.56803377676257e-13 Score : 1.0\n",
      "[192/1000]\n",
      "- Train Loss : 7.806956985279863e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 4.516012943375086e-13 Score : 1.0\n",
      "[193/1000]\n",
      "- Train Loss : 7.719138126943351e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 4.4650801068676915e-13 Score : 1.0\n",
      "[194/1000]\n",
      "- Train Loss : 7.633143357726791e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 4.4152184621067125e-13 Score : 1.0\n",
      "[195/1000]\n",
      "- Train Loss : 7.548951349045651e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 4.3664217749296574e-13 Score : 1.0\n",
      "[196/1000]\n",
      "- Train Loss : 7.466552878472669e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 4.318626890559979e-13 Score : 1.0\n",
      "[197/1000]\n",
      "- Train Loss : 7.385771674253408e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 4.271802909235761e-13 Score : 1.0\n",
      "[198/1000]\n",
      "- Train Loss : 7.306707036278886e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 4.225946578350487e-13 Score : 1.0\n",
      "[199/1000]\n",
      "- Train Loss : 7.229220134122676e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 4.1810459716802584e-13 Score : 1.0\n",
      "[200/1000]\n",
      "- Train Loss : 7.153369230216306e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 4.1370796762321693e-13 Score : 1.0\n",
      "[201/1000]\n",
      "- Train Loss : 7.079041581529291e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 4.0939511980128684e-13 Score : 1.0\n",
      "[202/1000]\n",
      "- Train Loss : 7.006050749427594e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 4.051643460838139e-13 Score : 1.0\n",
      "[203/1000]\n",
      "- Train Loss : 6.934579969399909e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 4.010225040495391e-13 Score : 1.0\n",
      "[204/1000]\n",
      "- Train Loss : 6.864524098596624e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.9695780299983663e-13 Score : 1.0\n",
      "[205/1000]\n",
      "- Train Loss : 6.795828142073974e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.9297262817948597e-13 Score : 1.0\n",
      "[206/1000]\n",
      "- Train Loss : 6.728411303334592e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.8906527197006546e-13 Score : 1.0\n",
      "[207/1000]\n",
      "- Train Loss : 6.662296623204909e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.85232861235818e-13 Score : 1.0\n",
      "[208/1000]\n",
      "- Train Loss : 6.597439664934819e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.8147021891137e-13 Score : 1.0\n",
      "[209/1000]\n",
      "- Train Loss : 6.533770378929313e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.777767757905809e-13 Score : 1.0\n",
      "[210/1000]\n",
      "- Train Loss : 6.471312562160733e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.7415833235507345e-13 Score : 1.0\n",
      "[211/1000]\n",
      "- Train Loss : 6.410040709665117e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.706057270964902e-13 Score : 1.0\n",
      "[212/1000]\n",
      "- Train Loss : 6.349880593674087e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.671148400465757e-13 Score : 1.0\n",
      "[213/1000]\n",
      "- Train Loss : 6.290795619596383e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.6368537304973247e-13 Score : 1.0\n",
      "[214/1000]\n",
      "- Train Loss : 6.232732408164442e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.6031954872041416e-13 Score : 1.0\n",
      "[215/1000]\n",
      "- Train Loss : 6.175736146347195e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.5701419576726623e-13 Score : 1.0\n",
      "[216/1000]\n",
      "- Train Loss : 6.119727117020806e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.5376611579387984e-13 Score : 1.0\n",
      "[217/1000]\n",
      "- Train Loss : 6.064739987770399e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.505770706287853e-13 Score : 1.0\n",
      "[218/1000]\n",
      "- Train Loss : 6.01068160292087e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.4744004006291573e-13 Score : 1.0\n",
      "[219/1000]\n",
      "- Train Loss : 5.95754786707065e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.4435927958979817e-13 Score : 1.0\n",
      "[220/1000]\n",
      "- Train Loss : 5.90535828507568e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.4132931398846156e-13 Score : 1.0\n",
      "[221/1000]\n",
      "- Train Loss : 5.85402683679005e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.3835382954629234e-13 Score : 1.0\n",
      "[222/1000]\n",
      "- Train Loss : 5.803605167742527e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.3542913997590407e-13 Score : 1.0\n",
      "[223/1000]\n",
      "- Train Loss : 5.753986315689863e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.3255142346463873e-13 Score : 1.0\n",
      "[224/1000]\n",
      "- Train Loss : 5.705261979998912e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.2972474577064315e-13 Score : 1.0\n",
      "[225/1000]\n",
      "- Train Loss : 5.65734582612279e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.2694024354115725e-13 Score : 1.0\n",
      "[226/1000]\n",
      "- Train Loss : 5.610121886128445e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.2420219937476236e-13 Score : 1.0\n",
      "[227/1000]\n",
      "- Train Loss : 5.563683544266557e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.2150787566097294e-13 Score : 1.0\n",
      "[228/1000]\n",
      "- Train Loss : 5.517982215454363e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.188554292560958e-13 Score : 1.0\n",
      "[229/1000]\n",
      "- Train Loss : 5.47300323051102e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.1624662198866116e-13 Score : 1.0\n",
      "[230/1000]\n",
      "- Train Loss : 5.428750835924137e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.1368074912725696e-13 Score : 1.0\n",
      "[231/1000]\n",
      "- Train Loss : 5.385205080284801e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.1115347386319325e-13 Score : 1.0\n",
      "[232/1000]\n",
      "- Train Loss : 5.34235608851269e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.0866775064739005e-13 Score : 1.0\n",
      "[233/1000]\n",
      "- Train Loss : 5.300185331068442e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.0621889030545135e-13 Score : 1.0\n",
      "[234/1000]\n",
      "- Train Loss : 5.258663312052197e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.038091967669937e-13 Score : 1.0\n",
      "[235/1000]\n",
      "- Train Loss : 5.217704570850119e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 3.0143566137098843e-13 Score : 1.0\n",
      "[236/1000]\n",
      "- Train Loss : 5.177437778463823e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.9909779622645793e-13 Score : 1.0\n",
      "[237/1000]\n",
      "- Train Loss : 5.137777537262996e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.9679679395579195e-13 Score : 1.0\n",
      "[238/1000]\n",
      "- Train Loss : 5.098701312864907e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.9452921221709283e-13 Score : 1.0\n",
      "[239/1000]\n",
      "- Train Loss : 5.060208999020024e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.9229236760998367e-13 Score : 1.0\n",
      "[240/1000]\n",
      "- Train Loss : 5.022218284826984e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.900896482662535e-13 Score : 1.0\n",
      "[241/1000]\n",
      "- Train Loss : 4.984830351744339e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.8792116260611955e-13 Score : 1.0\n",
      "[242/1000]\n",
      "- Train Loss : 4.94799497421898e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.8578197750969703e-13 Score : 1.0\n",
      "[243/1000]\n",
      "- Train Loss : 4.911663348227247e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.8367325849432135e-13 Score : 1.0\n",
      "[244/1000]\n",
      "- Train Loss : 4.875822887947072e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.8159462608923214e-13 Score : 1.0\n",
      "[245/1000]\n",
      "- Train Loss : 4.840553430830846e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.795483300139373e-13 Score : 1.0\n",
      "[246/1000]\n",
      "- Train Loss : 4.805792837806881e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.7753014187996417e-13 Score : 1.0\n",
      "[247/1000]\n",
      "- Train Loss : 4.771506483965974e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.755422300916577e-13 Score : 1.0\n",
      "[248/1000]\n",
      "- Train Loss : 4.737719243517736e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.7358014942011066e-13 Score : 1.0\n",
      "[249/1000]\n",
      "- Train Loss : 4.704383734797932e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.71644496176518e-13 Score : 1.0\n",
      "[250/1000]\n",
      "- Train Loss : 4.671484148244252e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.697378724460936e-13 Score : 1.0\n",
      "[251/1000]\n",
      "- Train Loss : 4.639070748633202e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.678564835212338e-13 Score : 1.0\n",
      "[252/1000]\n",
      "- Train Loss : 4.607063547522938e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.659998686160153e-13 Score : 1.0\n",
      "[253/1000]\n",
      "- Train Loss : 4.575495896895729e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.641682174658183e-13 Score : 1.0\n",
      "[254/1000]\n",
      "- Train Loss : 4.5443668481483934e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.6235919903597194e-13 Score : 1.0\n",
      "[255/1000]\n",
      "- Train Loss : 4.513613507371293e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.6057403305392024e-13 Score : 1.0\n",
      "[256/1000]\n",
      "- Train Loss : 4.48324831789264e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.5881383082689e-13 Score : 1.0\n",
      "[257/1000]\n",
      "- Train Loss : 4.4533342535651276e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.5707826709422954e-13 Score : 1.0\n",
      "[258/1000]\n",
      "- Train Loss : 4.4237773867035574e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.5536118900860993e-13 Score : 1.0\n",
      "[259/1000]\n",
      "- Train Loss : 4.3946045151981107e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.5366866810219713e-13 Score : 1.0\n",
      "[260/1000]\n",
      "- Train Loss : 4.365820598214064e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.519993491222755e-13 Score : 1.0\n",
      "[261/1000]\n",
      "- Train Loss : 4.3374349649234327e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.503515515554777e-13 Score : 1.0\n",
      "[262/1000]\n",
      "- Train Loss : 4.309400415623037e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.4872546513718397e-13 Score : 1.0\n",
      "[263/1000]\n",
      "- Train Loss : 4.2817197178838096e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.4711984303489587e-13 Score : 1.0\n",
      "[264/1000]\n",
      "- Train Loss : 4.254399552697628e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.455334384161151e-13 Score : 1.0\n",
      "[265/1000]\n",
      "- Train Loss : 4.2274131577173654e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.4396706443247096e-13 Score : 1.0\n",
      "[266/1000]\n",
      "- Train Loss : 4.2007512843211577e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.4241993503738846e-13 Score : 1.0\n",
      "[267/1000]\n",
      "- Train Loss : 4.1744181596143246e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.4089221286119344e-13 Score : 1.0\n",
      "[268/1000]\n",
      "- Train Loss : 4.148399023614928e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.3938286791182206e-13 Score : 1.0\n",
      "[269/1000]\n",
      "- Train Loss : 4.1227148111878597e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.3789293018133817e-13 Score : 1.0\n",
      "[270/1000]\n",
      "- Train Loss : 4.0973542626052704e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.3642039389572267e-13 Score : 1.0\n",
      "[271/1000]\n",
      "- Train Loss : 4.0722836364481417e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.3496371406687977e-13 Score : 1.0\n",
      "[272/1000]\n",
      "- Train Loss : 4.047490320654041e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.335258180406752e-13 Score : 1.0\n",
      "[273/1000]\n",
      "- Train Loss : 4.0229893244039393e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.321046729380355e-13 Score : 1.0\n",
      "[274/1000]\n",
      "- Train Loss : 3.9987860837625253e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.3070057691455814e-13 Score : 1.0\n",
      "[275/1000]\n",
      "- Train Loss : 3.9748778549081357e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.29312825238831e-13 Score : 1.0\n",
      "[276/1000]\n",
      "- Train Loss : 3.951248186450406e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.2794217685237483e-13 Score : 1.0\n",
      "[277/1000]\n",
      "- Train Loss : 3.927899223531416e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.265865717710619e-13 Score : 1.0\n",
      "[278/1000]\n",
      "- Train Loss : 3.904824854182653e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.2524721616980908e-13 Score : 1.0\n",
      "[279/1000]\n",
      "- Train Loss : 3.881987516561683e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.2392263282315639e-13 Score : 1.0\n",
      "[280/1000]\n",
      "- Train Loss : 3.859428716715063e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.226139059332763e-13 Score : 1.0\n",
      "[281/1000]\n",
      "- Train Loss : 3.8371429508564233e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.2132083221226145e-13 Score : 1.0\n",
      "[282/1000]\n",
      "- Train Loss : 3.8151141379890026e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.2004076891731644e-13 Score : 1.0\n",
      "[283/1000]\n",
      "- Train Loss : 3.793317010748948e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.187756134022431e-13 Score : 1.0\n",
      "[284/1000]\n",
      "- Train Loss : 3.7717322655589446e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.1752394265169006e-13 Score : 1.0\n",
      "[285/1000]\n",
      "- Train Loss : 3.750397907766649e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.1628772178209493e-13 Score : 1.0\n",
      "[286/1000]\n",
      "- Train Loss : 3.729321785640945e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.1506509409723734e-13 Score : 1.0\n",
      "[287/1000]\n",
      "- Train Loss : 3.708480463166779e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.138563035426061e-13 Score : 1.0\n",
      "[288/1000]\n",
      "- Train Loss : 3.687891291130181e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.1266080801711496e-13 Score : 1.0\n",
      "[289/1000]\n",
      "- Train Loss : 3.6675181270999383e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.1147882436119841e-13 Score : 1.0\n",
      "[290/1000]\n",
      "- Train Loss : 3.6473558985519186e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.1030705931075755e-13 Score : 1.0\n",
      "[291/1000]\n",
      "- Train Loss : 3.6273629773754396e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.0914696298619806e-13 Score : 1.0\n",
      "[292/1000]\n",
      "- Train Loss : 3.6076001760032944e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.08001232340424e-13 Score : 1.0\n",
      "[293/1000]\n",
      "- Train Loss : 3.5880710645209375e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.0686806488732362e-13 Score : 1.0\n",
      "[294/1000]\n",
      "- Train Loss : 3.568734905532648e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.0574656616010462e-13 Score : 1.0\n",
      "[295/1000]\n",
      "- Train Loss : 3.5496047623709173e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.0463624826778937e-13 Score : 1.0\n",
      "[296/1000]\n",
      "- Train Loss : 3.530703316973687e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.035381547549689e-13 Score : 1.0\n",
      "[297/1000]\n",
      "- Train Loss : 3.511962341712591e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.0245210943879016e-13 Score : 1.0\n",
      "[298/1000]\n",
      "- Train Loss : 3.493419324750154e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.0137644535841298e-13 Score : 1.0\n",
      "[299/1000]\n",
      "- Train Loss : 3.4750912779046376e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.0031265329182452e-13 Score : 1.0\n",
      "[300/1000]\n",
      "- Train Loss : 3.456923193505303e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.9925905272565742e-13 Score : 1.0\n",
      "[301/1000]\n",
      "- Train Loss : 3.4389515826353e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.9821664654692123e-13 Score : 1.0\n",
      "[302/1000]\n",
      "- Train Loss : 3.421176308926488e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.9718459449893228e-13 Score : 1.0\n",
      "[303/1000]\n",
      "- Train Loss : 3.4035582896644634e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.9616202921995257e-13 Score : 1.0\n",
      "[304/1000]\n",
      "- Train Loss : 3.386106698741495e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.9515033306775204e-13 Score : 1.0\n",
      "[305/1000]\n",
      "- Train Loss : 3.368851866248899e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.941494653847492e-13 Score : 1.0\n",
      "[306/1000]\n",
      "- Train Loss : 3.351771042519643e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.931596023537971e-13 Score : 1.0\n",
      "[307/1000]\n",
      "- Train Loss : 3.3348615381410696e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.9217848070286064e-13 Score : 1.0\n",
      "[308/1000]\n",
      "- Train Loss : 3.3181398908512085e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.9120849922924649e-13 Score : 1.0\n",
      "[309/1000]\n",
      "- Train Loss : 3.3015796855222206e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.9024887188637957e-13 Score : 1.0\n",
      "[310/1000]\n",
      "- Train Loss : 3.2851818883184314e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.8929661711828555e-13 Score : 1.0\n",
      "[311/1000]\n",
      "- Train Loss : 3.2689508809633244e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.883534425433861e-13 Score : 1.0\n",
      "[312/1000]\n",
      "- Train Loss : 3.2528354762325e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.874185214575247e-13 Score : 1.0\n",
      "[313/1000]\n",
      "- Train Loss : 3.2368875580336856e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.8649394094988336e-13 Score : 1.0\n",
      "[314/1000]\n",
      "- Train Loss : 3.221094715332426e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.8557922668201166e-13 Score : 1.0\n",
      "[315/1000]\n",
      "- Train Loss : 3.205472914557235e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.846725490627435e-13 Score : 1.0\n",
      "[316/1000]\n",
      "- Train Loss : 3.1899913524451017e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.8377520913468587e-13 Score : 1.0\n",
      "[317/1000]\n",
      "- Train Loss : 3.174666670454656e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.828860549330305e-13 Score : 1.0\n",
      "[318/1000]\n",
      "- Train Loss : 3.1594841267366257e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.8200400225560492e-13 Score : 1.0\n",
      "[319/1000]\n",
      "- Train Loss : 3.1444179677412986e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.81129647413604e-13 Score : 1.0\n",
      "[320/1000]\n",
      "- Train Loss : 3.12949876004847e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.8026602617311066e-13 Score : 1.0\n",
      "[321/1000]\n",
      "- Train Loss : 3.114746982724254e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.7940891014565225e-13 Score : 1.0\n",
      "[322/1000]\n",
      "- Train Loss : 3.1000996281357374e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.785606845760082e-13 Score : 1.0\n",
      "[323/1000]\n",
      "- Train Loss : 3.0856076660588074e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.7771981802860992e-13 Score : 1.0\n",
      "[324/1000]\n",
      "- Train Loss : 3.0712365963528957e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.7688632405598453e-13 Score : 1.0\n",
      "[325/1000]\n",
      "- Train Loss : 3.0570006931610076e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.7606041949856654e-13 Score : 1.0\n",
      "[326/1000]\n",
      "- Train Loss : 3.042881609772601e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.7524200948866586e-13 Score : 1.0\n",
      "[327/1000]\n",
      "- Train Loss : 3.028894923196216e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.744297929836755e-13 Score : 1.0\n",
      "[328/1000]\n",
      "- Train Loss : 3.015043811504912e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.736272800881289e-13 Score : 1.0\n",
      "[329/1000]\n",
      "- Train Loss : 3.001314849812787e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.7283178740164912e-13 Score : 1.0\n",
      "[330/1000]\n",
      "- Train Loss : 2.987710593729123e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.7204351821214353e-13 Score : 1.0\n",
      "[331/1000]\n",
      "- Train Loss : 2.9742455173799165e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.7126278422773672e-13 Score : 1.0\n",
      "[332/1000]\n",
      "- Train Loss : 2.9609167288812925e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.7048882650690794e-13 Score : 1.0\n",
      "[333/1000]\n",
      "- Train Loss : 2.94768467344527e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.6972194320525463e-13 Score : 1.0\n",
      "[334/1000]\n",
      "- Train Loss : 2.9345803012523714e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.6896044025688228e-13 Score : 1.0\n",
      "[335/1000]\n",
      "- Train Loss : 2.9215587031009104e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.68203978848612e-13 Score : 1.0\n",
      "[336/1000]\n",
      "- Train Loss : 2.9086195954675706e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.6745471383226157e-13 Score : 1.0\n",
      "[337/1000]\n",
      "- Train Loss : 2.895811779664793e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.6671230639465212e-13 Score : 1.0\n",
      "[338/1000]\n",
      "- Train Loss : 2.883132350159373e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.6597700048127245e-13 Score : 1.0\n",
      "[339/1000]\n",
      "- Train Loss : 2.8705609023890846e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.6524901293255706e-13 Score : 1.0\n",
      "[340/1000]\n",
      "- Train Loss : 2.8581156095819493e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.6452735441402355e-13 Score : 1.0\n",
      "[341/1000]\n",
      "- Train Loss : 2.845765435245688e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.6381041217494036e-13 Score : 1.0\n",
      "[342/1000]\n",
      "- Train Loss : 2.8335089019681994e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.6309941949527867e-13 Score : 1.0\n",
      "[343/1000]\n",
      "- Train Loss : 2.8213554186047225e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.6239523018424934e-13 Score : 1.0\n",
      "[344/1000]\n",
      "- Train Loss : 2.809310568507007e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.6169746477109198e-13 Score : 1.0\n",
      "[345/1000]\n",
      "- Train Loss : 2.7973691237840704e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.6100517457890567e-13 Score : 1.0\n",
      "[346/1000]\n",
      "- Train Loss : 2.7855204937845297e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.6031890170877666e-13 Score : 1.0\n",
      "[347/1000]\n",
      "- Train Loss : 2.7737722134884274e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.5963795498181999e-13 Score : 1.0\n",
      "[348/1000]\n",
      "- Train Loss : 2.76213168993604e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.589623750556171e-13 Score : 1.0\n",
      "[349/1000]\n",
      "- Train Loss : 2.7505843390133514e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.5829202640489648e-13 Score : 1.0\n",
      "[350/1000]\n",
      "- Train Loss : 2.739116747472939e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.5762694968723956e-13 Score : 1.0\n",
      "[351/1000]\n",
      "- Train Loss : 2.7277457671062033e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.569682697624003e-13 Score : 1.0\n",
      "[352/1000]\n",
      "- Train Loss : 2.71648710556507e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.5631591886774293e-13 Score : 1.0\n",
      "[353/1000]\n",
      "- Train Loss : 2.7053240206645836e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.556689212213122e-13 Score : 1.0\n",
      "[354/1000]\n",
      "- Train Loss : 2.6942400616940645e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.55027005772565e-13 Score : 1.0\n",
      "[355/1000]\n",
      "- Train Loss : 2.683268599563915e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.5439034870435436e-13 Score : 1.0\n",
      "[356/1000]\n",
      "- Train Loss : 2.6723719944736205e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.5375896356920743e-13 Score : 1.0\n",
      "[357/1000]\n",
      "- Train Loss : 2.6615579833845653e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.531330672075587e-13 Score : 1.0\n",
      "[358/1000]\n",
      "- Train Loss : 2.6508388288852714e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.5251149410207276e-13 Score : 1.0\n",
      "[359/1000]\n",
      "- Train Loss : 2.6401994485016666e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.518947728013087e-13 Score : 1.0\n",
      "[360/1000]\n",
      "- Train Loss : 2.6296508307181045e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.5128196818089273e-13 Score : 1.0\n",
      "[361/1000]\n",
      "- Train Loss : 2.619165473103654e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.5067309379335203e-13 Score : 1.0\n",
      "[362/1000]\n",
      "- Train Loss : 2.6087463951281985e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.5006980304699963e-13 Score : 1.0\n",
      "[363/1000]\n",
      "- Train Loss : 2.5984295459713125e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.494714860781135e-13 Score : 1.0\n",
      "[364/1000]\n",
      "- Train Loss : 2.5881860185253313e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.4887842748976393e-13 Score : 1.0\n",
      "[365/1000]\n",
      "- Train Loss : 2.578034709648232e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.4828997676064742e-13 Score : 1.0\n",
      "[366/1000]\n",
      "- Train Loss : 2.567961498033025e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.4770667599185022e-13 Score : 1.0\n",
      "[367/1000]\n",
      "- Train Loss : 2.5579625700884036e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.4712756295394425e-13 Score : 1.0\n",
      "[368/1000]\n",
      "- Train Loss : 2.5480318544739645e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.4655163475991995e-13 Score : 1.0\n",
      "[369/1000]\n",
      "- Train Loss : 2.538191105413544e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.459801924523843e-13 Score : 1.0\n",
      "[370/1000]\n",
      "- Train Loss : 2.528406593293285e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.454131818212287e-13 Score : 1.0\n",
      "[371/1000]\n",
      "- Train Loss : 2.5186945567257306e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.448516870686256e-13 Score : 1.0\n",
      "[372/1000]\n",
      "- Train Loss : 2.5090715978930067e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.4429345847506714e-13 Score : 1.0\n",
      "[373/1000]\n",
      "- Train Loss : 2.4995206410439635e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.4373960734778007e-13 Score : 1.0\n",
      "[374/1000]\n",
      "- Train Loss : 2.490023282647146e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.4319056736763341e-13 Score : 1.0\n",
      "[375/1000]\n",
      "- Train Loss : 2.480613720446597e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.4264446828587962e-13 Score : 1.0\n",
      "[376/1000]\n",
      "- Train Loss : 2.4712693742926614e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.4210235364710971e-13 Score : 1.0\n",
      "[377/1000]\n",
      "- Train Loss : 2.4620000086268864e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.4156499594537159e-13 Score : 1.0\n",
      "[378/1000]\n",
      "- Train Loss : 2.452787106759178e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.4103208347254065e-13 Score : 1.0\n",
      "[379/1000]\n",
      "- Train Loss : 2.4436572835614706e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.405028166295147e-13 Score : 1.0\n",
      "[380/1000]\n",
      "- Train Loss : 2.4346006899007993e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.3997818475077611e-13 Score : 1.0\n",
      "[381/1000]\n",
      "- Train Loss : 2.4256025478955005e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.3945684613613646e-13 Score : 1.0\n",
      "[382/1000]\n",
      "- Train Loss : 2.416675742887074e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.38940386431273e-13 Score : 1.0\n",
      "[383/1000]\n",
      "- Train Loss : 2.407809265439894e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.3842631197118899e-13 Score : 1.0\n",
      "[384/1000]\n",
      "- Train Loss : 2.3990115991186993e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.3791628971672465e-13 Score : 1.0\n",
      "[385/1000]\n",
      "- Train Loss : 2.390273420703547e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.37410495850733e-13 Score : 1.0\n",
      "[386/1000]\n",
      "- Train Loss : 2.3816007291521664e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.36908144326639e-13 Score : 1.0\n",
      "[387/1000]\n",
      "- Train Loss : 2.372995370429937e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.364096823778388e-13 Score : 1.0\n",
      "[388/1000]\n",
      "- Train Loss : 2.364447311449911e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.359140800122685e-13 Score : 1.0\n",
      "[389/1000]\n",
      "- Train Loss : 2.355947446117665e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.3542079512884192e-13 Score : 1.0\n",
      "[390/1000]\n",
      "- Train Loss : 2.347508263779346e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.3493164376619793e-13 Score : 1.0\n",
      "[391/1000]\n",
      "- Train Loss : 2.339122425471113e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.344457450100714e-13 Score : 1.0\n",
      "[392/1000]\n",
      "- Train Loss : 2.330794750116637e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.3396267873212048e-13 Score : 1.0\n",
      "[393/1000]\n",
      "- Train Loss : 2.322521585146857e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.3348436939120134e-13 Score : 1.0\n",
      "[394/1000]\n",
      "- Train Loss : 2.3143124158215643e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.330090551587837e-13 Score : 1.0\n",
      "[395/1000]\n",
      "- Train Loss : 2.3061696973447776e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.32537210373318e-13 Score : 1.0\n",
      "[396/1000]\n",
      "- Train Loss : 2.298081158256235e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.3206928226820042e-13 Score : 1.0\n",
      "[397/1000]\n",
      "- Train Loss : 2.2900475766186927e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.3160353611995496e-13 Score : 1.0\n",
      "[398/1000]\n",
      "- Train Loss : 2.2820688453618204e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.3114116455097136e-13 Score : 1.0\n",
      "[399/1000]\n",
      "- Train Loss : 2.274133120488908e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.3068144927731035e-13 Score : 1.0\n",
      "[400/1000]\n",
      "- Train Loss : 2.2662532561045744e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.3022452582424349e-13 Score : 1.0\n",
      "[401/1000]\n",
      "- Train Loss : 2.2584281302489823e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2977172233943207e-13 Score : 1.0\n",
      "[402/1000]\n",
      "- Train Loss : 2.2506593987510872e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.293227000096972e-13 Score : 1.0\n",
      "[403/1000]\n",
      "- Train Loss : 2.2429620585581544e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2887598160957886e-13 Score : 1.0\n",
      "[404/1000]\n",
      "- Train Loss : 2.235298918313635e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2843227187048917e-13 Score : 1.0\n",
      "[405/1000]\n",
      "- Train Loss : 2.2276953585034355e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2799180118538978e-13 Score : 1.0\n",
      "[406/1000]\n",
      "- Train Loss : 2.2201417034225623e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2755452889669922e-13 Score : 1.0\n",
      "[407/1000]\n",
      "- Train Loss : 2.2126403781966738e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2712022461145583e-13 Score : 1.0\n",
      "[408/1000]\n",
      "- Train Loss : 2.2051878534746295e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2668785833759577e-13 Score : 1.0\n",
      "[409/1000]\n",
      "- Train Loss : 2.197779973286356e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.262594087440838e-13 Score : 1.0\n",
      "[410/1000]\n",
      "- Train Loss : 2.190428423251571e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.258340626792906e-13 Score : 1.0\n",
      "[411/1000]\n",
      "- Train Loss : 2.1831332479505945e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2541088501884234e-13 Score : 1.0\n",
      "[412/1000]\n",
      "- Train Loss : 2.1758660766674374e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2499032299613522e-13 Score : 1.0\n",
      "[413/1000]\n",
      "- Train Loss : 2.1686570051730197e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2457233595358774e-13 Score : 1.0\n",
      "[414/1000]\n",
      "- Train Loss : 2.1614883833927806e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.241576692801935e-13 Score : 1.0\n",
      "[415/1000]\n",
      "- Train Loss : 2.1543568241251823e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2374579442739342e-13 Score : 1.0\n",
      "[416/1000]\n",
      "- Train Loss : 2.1472913664501285e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.233362099516827e-13 Score : 1.0\n",
      "[417/1000]\n",
      "- Train Loss : 2.1402750288604312e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.229298916350166e-13 Score : 1.0\n",
      "[418/1000]\n",
      "- Train Loss : 2.1333047439541979e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2252628382378172e-13 Score : 1.0\n",
      "[419/1000]\n",
      "- Train Loss : 2.1263810464346047e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2212495283710906e-13 Score : 1.0\n",
      "[420/1000]\n",
      "- Train Loss : 2.1195027228604634e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2172677958926376e-13 Score : 1.0\n",
      "[421/1000]\n",
      "- Train Loss : 2.1126683576257489e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2133036816994874e-13 Score : 1.0\n",
      "[422/1000]\n",
      "- Train Loss : 2.105868434253088e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2093642331057614e-13 Score : 1.0\n",
      "[423/1000]\n",
      "- Train Loss : 2.0990998174332307e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.205448908010373e-13 Score : 1.0\n",
      "[424/1000]\n",
      "- Train Loss : 2.0923861588761847e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2015577064133226e-13 Score : 1.0\n",
      "[425/1000]\n",
      "- Train Loss : 2.0857030519543344e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.19769062831461e-13 Score : 1.0\n",
      "[426/1000]\n",
      "- Train Loss : 2.0790672670266718e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.193851739472382e-13 Score : 1.0\n",
      "[427/1000]\n",
      "- Train Loss : 2.07248875200559e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.19004117541191e-13 Score : 1.0\n",
      "[428/1000]\n",
      "- Train Loss : 2.0659332597455352e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1862564966783062e-13 Score : 1.0\n",
      "[429/1000]\n",
      "- Train Loss : 2.0594372452851428e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1824882165025613e-13 Score : 1.0\n",
      "[430/1000]\n",
      "- Train Loss : 2.0529872786994601e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1787497518865597e-13 Score : 1.0\n",
      "[431/1000]\n",
      "- Train Loss : 2.0465588712530898e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1750324292129216e-13 Score : 1.0\n",
      "[432/1000]\n",
      "- Train Loss : 2.040181649534795e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1713312340465992e-13 Score : 1.0\n",
      "[433/1000]\n",
      "- Train Loss : 2.0338280740385285e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.167661480743279e-13 Score : 1.0\n",
      "[434/1000]\n",
      "- Train Loss : 2.0275243779389104e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1640101588768909e-13 Score : 1.0\n",
      "[435/1000]\n",
      "- Train Loss : 2.0212639149412406e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1603944801569233e-13 Score : 1.0\n",
      "[436/1000]\n",
      "- Train Loss : 2.0150527730143537e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1567943868431851e-13 Score : 1.0\n",
      "[437/1000]\n",
      "- Train Loss : 2.0088750123676902e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.153218959128871e-13 Score : 1.0\n",
      "[438/1000]\n",
      "- Train Loss : 2.0027399992951168e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1496673838623517e-13 Score : 1.0\n",
      "[439/1000]\n",
      "- Train Loss : 1.9966347748299856e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1461423715490582e-13 Score : 1.0\n",
      "[440/1000]\n",
      "- Train Loss : 1.9905828808923556e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.142634570945253e-13 Score : 1.0\n",
      "[441/1000]\n",
      "- Train Loss : 1.9845584036273832e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1391441175762074e-13 Score : 1.0\n",
      "[442/1000]\n",
      "- Train Loss : 1.978569716474121e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1356750094374327e-13 Score : 1.0\n",
      "[443/1000]\n",
      "- Train Loss : 1.972609248440102e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1322272465289288e-13 Score : 1.0\n",
      "[444/1000]\n",
      "- Train Loss : 1.9666845653661215e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1288029972550406e-13 Score : 1.0\n",
      "[445/1000]\n",
      "- Train Loss : 1.9608094285114846e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1253978570444426e-13 Score : 1.0\n",
      "[446/1000]\n",
      "- Train Loss : 1.9549618676127997e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1220133166751217e-13 Score : 1.0\n",
      "[447/1000]\n",
      "- Train Loss : 1.9491452952323842e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1186478176064552e-13 Score : 1.0\n",
      "[448/1000]\n",
      "- Train Loss : 1.94336240078468e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1153008177373566e-13 Score : 1.0\n",
      "[449/1000]\n",
      "- Train Loss : 1.937618662559398e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1119785512303179e-13 Score : 1.0\n",
      "[450/1000]\n",
      "- Train Loss : 1.93190708854007e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.108677020089828e-13 Score : 1.0\n",
      "[451/1000]\n",
      "- Train Loss : 1.9262346356140212e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1053998157355832e-13 Score : 1.0\n",
      "[452/1000]\n",
      "- Train Loss : 1.920613008337344e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1021387388886542e-13 Score : 1.0\n",
      "[453/1000]\n",
      "- Train Loss : 1.9150081322492223e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0988958224281142e-13 Score : 1.0\n",
      "[454/1000]\n",
      "- Train Loss : 1.9094350159023023e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0956705242528772e-13 Score : 1.0\n",
      "[455/1000]\n",
      "- Train Loss : 1.9039047211705565e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0924736186220321e-13 Score : 1.0\n",
      "[456/1000]\n",
      "- Train Loss : 1.8984019272236697e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0892860642349247e-13 Score : 1.0\n",
      "[457/1000]\n",
      "- Train Loss : 1.8929298386541905e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0861179577242863e-13 Score : 1.0\n",
      "[458/1000]\n",
      "- Train Loss : 1.8874814182945317e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0829592702200214e-13 Score : 1.0\n",
      "[459/1000]\n",
      "- Train Loss : 1.882063032499155e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0798262647547172e-13 Score : 1.0\n",
      "[460/1000]\n",
      "- Train Loss : 1.8766709896787553e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0767063374781186e-13 Score : 1.0\n",
      "[461/1000]\n",
      "- Train Loss : 1.8713149180685362e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0736078231944265e-13 Score : 1.0\n",
      "[462/1000]\n",
      "- Train Loss : 1.8659823594890683e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0705263850949512e-13 Score : 1.0\n",
      "[463/1000]\n",
      "- Train Loss : 1.8606931471254112e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0674680540542769e-13 Score : 1.0\n",
      "[464/1000]\n",
      "- Train Loss : 1.855438789292803e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0644223946264936e-13 Score : 1.0\n",
      "[465/1000]\n",
      "- Train Loss : 1.8501996420856244e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0613997067322398e-13 Score : 1.0\n",
      "[466/1000]\n",
      "- Train Loss : 1.8450098740385284e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0583955858001898e-13 Score : 1.0\n",
      "[467/1000]\n",
      "- Train Loss : 1.8398437413973422e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0554021036019573e-13 Score : 1.0\n",
      "[468/1000]\n",
      "- Train Loss : 1.8346977916512014e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.052425223249491e-13 Score : 1.0\n",
      "[469/1000]\n",
      "- Train Loss : 1.8295849783444775e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0494604724088294e-13 Score : 1.0\n",
      "[470/1000]\n",
      "- Train Loss : 1.824494075473983e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.046520319404956e-13 Score : 1.0\n",
      "[471/1000]\n",
      "- Train Loss : 1.8194365712001505e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0435922959128874e-13 Score : 1.0\n",
      "[472/1000]\n",
      "- Train Loss : 1.8144048368214926e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0406921906267602e-13 Score : 1.0\n",
      "[473/1000]\n",
      "- Train Loss : 1.8094184032054903e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0378042826150735e-13 Score : 1.0\n",
      "[474/1000]\n",
      "- Train Loss : 1.8044478008181842e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0349322310601594e-13 Score : 1.0\n",
      "[475/1000]\n",
      "- Train Loss : 1.7995210213254188e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0320801017201647e-13 Score : 1.0\n",
      "[476/1000]\n",
      "- Train Loss : 1.794617021708447e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0292414571446903e-13 Score : 1.0\n",
      "[477/1000]\n",
      "- Train Loss : 1.7897445482907392e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0264244966126657e-13 Score : 1.0\n",
      "[478/1000]\n",
      "- Train Loss : 1.7848934351797082e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0236230537242347e-13 Score : 1.0\n",
      "[479/1000]\n",
      "- Train Loss : 1.7800779490164138e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0208369929541258e-13 Score : 1.0\n",
      "[480/1000]\n",
      "- Train Loss : 1.775278862283925e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0180642814232657e-13 Score : 1.0\n",
      "[481/1000]\n",
      "- Train Loss : 1.770503379805946e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0153010566614148e-13 Score : 1.0\n",
      "[482/1000]\n",
      "- Train Loss : 1.7657593590629225e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0125550436090522e-13 Score : 1.0\n",
      "[483/1000]\n",
      "- Train Loss : 1.7610411646072043e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0098242771497401e-13 Score : 1.0\n",
      "[484/1000]\n",
      "- Train Loss : 1.7563414541306608e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0071065211164978e-13 Score : 1.0\n",
      "[485/1000]\n",
      "- Train Loss : 1.7516668487281135e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0044038761510346e-13 Score : 1.0\n",
      "[486/1000]\n",
      "- Train Loss : 1.747016967723824e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0017179007939733e-13 Score : 1.0\n",
      "[487/1000]\n",
      "- Train Loss : 1.742382418026159e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.990391760389405e-14 Score : 1.0\n",
      "[488/1000]\n",
      "- Train Loss : 1.7377892941188728e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.963790182461116e-14 Score : 1.0\n",
      "[489/1000]\n",
      "- Train Loss : 1.7332090828771673e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.93735394536413e-14 Score : 1.0\n",
      "[490/1000]\n",
      "- Train Loss : 1.7286675600905633e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.911062720307715e-14 Score : 1.0\n",
      "[491/1000]\n",
      "- Train Loss : 1.7241456656530302e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.884936836082603e-14 Score : 1.0\n",
      "[492/1000]\n",
      "- Train Loss : 1.7196471533243658e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.858918016822024e-14 Score : 1.0\n",
      "[493/1000]\n",
      "- Train Loss : 1.7151734536991488e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.833003552020547e-14 Score : 1.0\n",
      "[494/1000]\n",
      "- Train Loss : 1.710725677043324e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.807288309368264e-14 Score : 1.0\n",
      "[495/1000]\n",
      "- Train Loss : 1.7062952275655108e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.781753315327157e-14 Score : 1.0\n",
      "[496/1000]\n",
      "- Train Loss : 1.7018994038414276e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.756247459219436e-14 Score : 1.0\n",
      "[497/1000]\n",
      "- Train Loss : 1.6975144957661178e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.730882549394138e-14 Score : 1.0\n",
      "[498/1000]\n",
      "- Train Loss : 1.693148482994053e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.705600309984491e-14 Score : 1.0\n",
      "[499/1000]\n",
      "- Train Loss : 1.6888012605627034e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.680495608680589e-14 Score : 1.0\n",
      "[500/1000]\n",
      "- Train Loss : 1.6844778384954355e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.655493906583074e-14 Score : 1.0\n",
      "[501/1000]\n",
      "- Train Loss : 1.6801755610926338e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.630629762636192e-14 Score : 1.0\n",
      "[502/1000]\n",
      "- Train Loss : 1.6758973690867312e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.605865229763907e-14 Score : 1.0\n",
      "[503/1000]\n",
      "- Train Loss : 1.6716361518754372e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.581257906206633e-14 Score : 1.0\n",
      "[504/1000]\n",
      "- Train Loss : 1.6673966220821624e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.556767134382901e-14 Score : 1.0\n",
      "[505/1000]\n",
      "- Train Loss : 1.6631904102833064e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.532447124401336e-14 Score : 1.0\n",
      "[506/1000]\n",
      "- Train Loss : 1.659002643426626e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.508209107209065e-14 Score : 1.0\n",
      "[507/1000]\n",
      "- Train Loss : 1.6548352340367282e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.484158114891547e-14 Score : 1.0\n",
      "[508/1000]\n",
      "- Train Loss : 1.6506889174983006e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.460166753693516e-14 Score : 1.0\n",
      "[509/1000]\n",
      "- Train Loss : 1.6465640087192732e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.436291944229028e-14 Score : 1.0\n",
      "[510/1000]\n",
      "- Train Loss : 1.6424474402618891e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.41244152931342e-14 Score : 1.0\n",
      "[511/1000]\n",
      "- Train Loss : 1.6383510097547496e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.388830994128475e-14 Score : 1.0\n",
      "[512/1000]\n",
      "- Train Loss : 1.6342870737558847e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.365314649007264e-14 Score : 1.0\n",
      "[513/1000]\n",
      "- Train Loss : 1.6302327253547165e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.341820665555861e-14 Score : 1.0\n",
      "[514/1000]\n",
      "- Train Loss : 1.626198258033057e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.318565206582405e-14 Score : 1.0\n",
      "[515/1000]\n",
      "- Train Loss : 1.6221932402935785e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.295455437275876e-14 Score : 1.0\n",
      "[516/1000]\n",
      "- Train Loss : 1.6182096550243228e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.272420884495064e-14 Score : 1.0\n",
      "[517/1000]\n",
      "- Train Loss : 1.614255301544712e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.249566580325427e-14 Score : 1.0\n",
      "[518/1000]\n",
      "- Train Loss : 1.6103158465835508e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.226820018746681e-14 Score : 1.0\n",
      "[519/1000]\n",
      "- Train Loss : 1.6064023955292652e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.204166969605312e-14 Score : 1.0\n",
      "[520/1000]\n",
      "- Train Loss : 1.6025026525289022e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.18165689962544e-14 Score : 1.0\n",
      "[521/1000]\n",
      "- Train Loss : 1.5986219250970607e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.159236276324798e-14 Score : 1.0\n",
      "[522/1000]\n",
      "- Train Loss : 1.5947607064225239e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.136904422077027e-14 Score : 1.0\n",
      "[523/1000]\n",
      "- Train Loss : 1.5909193453745109e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.114664047387561e-14 Score : 1.0\n",
      "[524/1000]\n",
      "- Train Loss : 1.5870811193919258e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.092528027157196e-14 Score : 1.0\n",
      "[525/1000]\n",
      "- Train Loss : 1.5832788589845465e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.070515334923951e-14 Score : 1.0\n",
      "[526/1000]\n",
      "- Train Loss : 1.5794819616232397e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.048625293061469e-14 Score : 1.0\n",
      "[527/1000]\n",
      "- Train Loss : 1.5757114953405638e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.026787428428537e-14 Score : 1.0\n",
      "[528/1000]\n",
      "- Train Loss : 1.571951016040346e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.005054595881065e-14 Score : 1.0\n",
      "[529/1000]\n",
      "- Train Loss : 1.5682103943140862e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.983426795419053e-14 Score : 1.0\n",
      "[530/1000]\n",
      "- Train Loss : 1.5644820954693604e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.961883020625408e-14 Score : 1.0\n",
      "[531/1000]\n",
      "- Train Loss : 1.560776795712502e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.94047612635604e-14 Score : 1.0\n",
      "[532/1000]\n",
      "- Train Loss : 1.5570898425624808e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.919155290634112e-14 Score : 1.0\n",
      "[533/1000]\n",
      "- Train Loss : 1.5534201131409492e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.897970657810103e-14 Score : 1.0\n",
      "[534/1000]\n",
      "- Train Loss : 1.5497663158319047e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.876920872631297e-14 Score : 1.0\n",
      "[535/1000]\n",
      "- Train Loss : 1.546138845752684e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.855904291144023e-14 Score : 1.0\n",
      "[536/1000]\n",
      "- Train Loss : 1.5425225856209208e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.835002906137576e-14 Score : 1.0\n",
      "[537/1000]\n",
      "- Train Loss : 1.5389227934773906e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.814203842711157e-14 Score : 1.0\n",
      "[538/1000]\n",
      "- Train Loss : 1.5353439351744657e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.793486094447675e-14 Score : 1.0\n",
      "[539/1000]\n",
      "- Train Loss : 1.5317726354288179e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.772867279632432e-14 Score : 1.0\n",
      "[540/1000]\n",
      "- Train Loss : 1.5282216757053522e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.752314194573896e-14 Score : 1.0\n",
      "[541/1000]\n",
      "- Train Loss : 1.5246827873063764e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.731889858523342e-14 Score : 1.0\n",
      "[542/1000]\n",
      "- Train Loss : 1.521165338133548e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.711548870514799e-14 Score : 1.0\n",
      "[543/1000]\n",
      "- Train Loss : 1.5176659677277847e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.691289197669191e-14 Score : 1.0\n",
      "[544/1000]\n",
      "- Train Loss : 1.5141767502911916e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.67114201079898e-14 Score : 1.0\n",
      "[545/1000]\n",
      "- Train Loss : 1.5106993390553125e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.65104022489474e-14 Score : 1.0\n",
      "[546/1000]\n",
      "- Train Loss : 1.5072430401996653e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.6310685432512e-14 Score : 1.0\n",
      "[547/1000]\n",
      "- Train Loss : 1.503801517838049e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.611192406924109e-14 Score : 1.0\n",
      "[548/1000]\n",
      "- Train Loss : 1.500378285726913e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.591379289848294e-14 Score : 1.0\n",
      "[549/1000]\n",
      "- Train Loss : 1.4969640877540942e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.571675270616086e-14 Score : 1.0\n",
      "[550/1000]\n",
      "- Train Loss : 1.4935694796617779e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.552067474326686e-14 Score : 1.0\n",
      "[551/1000]\n",
      "- Train Loss : 1.4901885095035785e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.532584361287121e-14 Score : 1.0\n",
      "[552/1000]\n",
      "- Train Loss : 1.486827119584261e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.513162912246117e-14 Score : 1.0\n",
      "[553/1000]\n",
      "- Train Loss : 1.483488876118889e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.493834298016131e-14 Score : 1.0\n",
      "[554/1000]\n",
      "- Train Loss : 1.4801524931522809e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.474563959652917e-14 Score : 1.0\n",
      "[555/1000]\n",
      "- Train Loss : 1.476834707423506e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.455403396759667e-14 Score : 1.0\n",
      "[556/1000]\n",
      "- Train Loss : 1.4735252957615994e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.436237412855555e-14 Score : 1.0\n",
      "[557/1000]\n",
      "- Train Loss : 1.4702306998224654e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.417194756948562e-14 Score : 1.0\n",
      "[558/1000]\n",
      "- Train Loss : 1.4669495141065388e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.398293047323993e-14 Score : 1.0\n",
      "[559/1000]\n",
      "- Train Loss : 1.463689041342471e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.379480106752296e-14 Score : 1.0\n",
      "[560/1000]\n",
      "- Train Loss : 1.4604461681426006e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.36072544204737e-14 Score : 1.0\n",
      "[561/1000]\n",
      "- Train Loss : 1.457218204718638e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.342092072460491e-14 Score : 1.0\n",
      "[562/1000]\n",
      "- Train Loss : 1.45400867341601e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.323597616276962e-14 Score : 1.0\n",
      "[563/1000]\n",
      "- Train Loss : 1.4508194657668667e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.305126199389598e-14 Score : 1.0\n",
      "[564/1000]\n",
      "- Train Loss : 1.447632559198422e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.286759814587694e-14 Score : 1.0\n",
      "[565/1000]\n",
      "- Train Loss : 1.4444657646547967e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.268449672773487e-14 Score : 1.0\n",
      "[566/1000]\n",
      "- Train Loss : 1.4413112879378482e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.250258793198254e-14 Score : 1.0\n",
      "[567/1000]\n",
      "- Train Loss : 1.438173477169848e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.232092308171901e-14 Score : 1.0\n",
      "[568/1000]\n",
      "- Train Loss : 1.4350478480886454e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.214074900944265e-14 Score : 1.0\n",
      "[569/1000]\n",
      "- Train Loss : 1.4319410572937055e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.19611305907797e-14 Score : 1.0\n",
      "[570/1000]\n",
      "- Train Loss : 1.4288442475155423e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.178222367979243e-14 Score : 1.0\n",
      "[571/1000]\n",
      "- Train Loss : 1.425765083787005e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.160416380175242e-14 Score : 1.0\n",
      "[572/1000]\n",
      "- Train Loss : 1.4226887025098419e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.142696450918682e-14 Score : 1.0\n",
      "[573/1000]\n",
      "- Train Loss : 1.4196390438892656e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.125030731770747e-14 Score : 1.0\n",
      "[574/1000]\n",
      "- Train Loss : 1.4165912173335902e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.107480886729995e-14 Score : 1.0\n",
      "[575/1000]\n",
      "- Train Loss : 1.413566943223699e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.089967633512565e-14 Score : 1.0\n",
      "[576/1000]\n",
      "- Train Loss : 1.4105464437565944e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.072524175809989e-14 Score : 1.0\n",
      "[577/1000]\n",
      "- Train Loss : 1.40753880843074e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.055148480743193e-14 Score : 1.0\n",
      "[578/1000]\n",
      "- Train Loss : 1.40453989112689e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.037857488971123e-14 Score : 1.0\n",
      "[579/1000]\n",
      "- Train Loss : 1.40155751746025e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.020617319175888e-14 Score : 1.0\n",
      "[580/1000]\n",
      "- Train Loss : 1.3985863117509264e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.00346117504902e-14 Score : 1.0\n",
      "[581/1000]\n",
      "- Train Loss : 1.395631625997412e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.986372115931575e-14 Score : 1.0\n",
      "[582/1000]\n",
      "- Train Loss : 1.3926788675189706e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.969392154657737e-14 Score : 1.0\n",
      "[583/1000]\n",
      "- Train Loss : 1.3897543410450757e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.952481988898752e-14 Score : 1.0\n",
      "[584/1000]\n",
      "- Train Loss : 1.38683784943058e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.935653138302703e-14 Score : 1.0\n",
      "[585/1000]\n",
      "- Train Loss : 1.3839332489389268e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.918859524277261e-14 Score : 1.0\n",
      "[586/1000]\n",
      "- Train Loss : 1.3810410192016472e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.902145870162039e-14 Score : 1.0\n",
      "[587/1000]\n",
      "- Train Loss : 1.3781552256387515e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.88548371565001e-14 Score : 1.0\n",
      "[588/1000]\n",
      "- Train Loss : 1.3752886849267024e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.868901521048202e-14 Score : 1.0\n",
      "[589/1000]\n",
      "- Train Loss : 1.3724251972961923e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.852443332069872e-14 Score : 1.0\n",
      "[590/1000]\n",
      "- Train Loss : 1.369587610127299e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.836021057288506e-14 Score : 1.0\n",
      "[591/1000]\n",
      "- Train Loss : 1.366752313479654e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.819631308572314e-14 Score : 1.0\n",
      "[592/1000]\n",
      "- Train Loss : 1.363926092030795e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.803322197392701e-14 Score : 1.0\n",
      "[593/1000]\n",
      "- Train Loss : 8.11884671262183e-10 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0540093103861281e-12 Score : 1.0\n",
      "[594/1000]\n",
      "- Train Loss : 1.3932830642935276e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 9.508305601202416e-13 Score : 1.0\n",
      "[595/1000]\n",
      "- Train Loss : 1.1479762494972828e-11 Score : 0.3333333333333333\n",
      "- Val Loss : 8.701752968361121e-13 Score : 1.0\n",
      "[596/1000]\n",
      "- Train Loss : 9.71930237811517e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 8.047030381451437e-13 Score : 1.0\n",
      "[597/1000]\n",
      "- Train Loss : 8.391905203812202e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 7.500491655716701e-13 Score : 1.0\n",
      "[598/1000]\n",
      "- Train Loss : 7.354185604195714e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 7.034770444121208e-13 Score : 1.0\n",
      "[599/1000]\n",
      "- Train Loss : 6.52053414768223e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 6.631367547096922e-13 Score : 1.0\n",
      "[600/1000]\n",
      "- Train Loss : 5.836469221296904e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 6.277396679722769e-13 Score : 1.0\n",
      "[601/1000]\n",
      "- Train Loss : 5.265345787455417e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 5.963525029697991e-13 Score : 1.0\n",
      "[602/1000]\n",
      "- Train Loss : 4.7818499969904514e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 5.682581141752685e-13 Score : 1.0\n",
      "[603/1000]\n",
      "- Train Loss : 4.367512365929081e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 5.429332114101348e-13 Score : 1.0\n",
      "[604/1000]\n",
      "- Train Loss : 4.008987666061837e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 5.199643883860294e-13 Score : 1.0\n",
      "[605/1000]\n",
      "- Train Loss : 3.696010256507141e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 4.990131029745937e-13 Score : 1.0\n",
      "[606/1000]\n",
      "- Train Loss : 3.4208134263809185e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 4.798046725554284e-13 Score : 1.0\n",
      "[607/1000]\n",
      "- Train Loss : 3.1770820118719332e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 4.621121736138323e-13 Score : 1.0\n",
      "[608/1000]\n",
      "- Train Loss : 2.9600184194108174e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 4.457667687664951e-13 Score : 1.0\n",
      "[609/1000]\n",
      "- Train Loss : 2.765776907262508e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 4.3061561261215076e-13 Score : 1.0\n",
      "[610/1000]\n",
      "- Train Loss : 2.591091222401126e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 4.1652285461858674e-13 Score : 1.0\n",
      "[611/1000]\n",
      "- Train Loss : 2.433337303749175e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 4.033777977439923e-13 Score : 1.0\n",
      "[612/1000]\n",
      "- Train Loss : 2.290350093031349e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 3.910915103051693e-13 Score : 1.0\n",
      "[613/1000]\n",
      "- Train Loss : 2.1602958462454922e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 3.795850081738522e-13 Score : 1.0\n",
      "[614/1000]\n",
      "- Train Loss : 2.0416397645843542e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 3.687758919849321e-13 Score : 1.0\n",
      "[615/1000]\n",
      "- Train Loss : 1.933029130528077e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 3.586166465781998e-13 Score : 1.0\n",
      "[616/1000]\n",
      "- Train Loss : 1.8334215328616378e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 3.4904636689661594e-13 Score : 1.0\n",
      "[617/1000]\n",
      "- Train Loss : 1.7417904109667473e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 3.4000658733802924e-13 Score : 1.0\n",
      "[618/1000]\n",
      "- Train Loss : 1.6572699721368907e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 3.3146835970443433e-13 Score : 1.0\n",
      "[619/1000]\n",
      "- Train Loss : 1.5792303686184358e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 3.2338953563637585e-13 Score : 1.0\n",
      "[620/1000]\n",
      "- Train Loss : 1.507543352234395e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 3.1575466525289586e-13 Score : 1.0\n",
      "[621/1000]\n",
      "- Train Loss : 1.4414112088783869e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 3.085119779002582e-13 Score : 1.0\n",
      "[622/1000]\n",
      "- Train Loss : 1.3799360834666165e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 3.016265080584002e-13 Score : 1.0\n",
      "[623/1000]\n",
      "- Train Loss : 1.322655442930414e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 2.950760566878402e-13 Score : 1.0\n",
      "[624/1000]\n",
      "- Train Loss : 1.2692043809025876e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 2.8883701528627237e-13 Score : 1.0\n",
      "[625/1000]\n",
      "- Train Loss : 1.2192470871436364e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 2.828918197785024e-13 Score : 1.0\n",
      "[626/1000]\n",
      "- Train Loss : 1.172510375224788e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 2.772156148297261e-13 Score : 1.0\n",
      "[627/1000]\n",
      "- Train Loss : 1.1287037018210235e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 2.7179227293262775e-13 Score : 1.0\n",
      "[628/1000]\n",
      "- Train Loss : 1.0875855483845945e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 2.666159122904216e-13 Score : 1.0\n",
      "[629/1000]\n",
      "- Train Loss : 1.0489942524955867e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 2.6166156914808614e-13 Score : 1.0\n",
      "[630/1000]\n",
      "- Train Loss : 1.0126706661989955e-12 Score : 0.3333333333333333\n",
      "- Val Loss : 2.569173714918327e-13 Score : 1.0\n",
      "[631/1000]\n",
      "- Train Loss : 9.784746784479663e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.523767056884091e-13 Score : 1.0\n",
      "[632/1000]\n",
      "- Train Loss : 9.462424276219216e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.480180503246915e-13 Score : 1.0\n",
      "[633/1000]\n",
      "- Train Loss : 9.15805029317711e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.4383658070101233e-13 Score : 1.0\n",
      "[634/1000]\n",
      "- Train Loss : 8.87054541239793e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.398217529512442e-13 Score : 1.0\n",
      "[635/1000]\n",
      "- Train Loss : 8.598633286652324e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.359679834341988e-13 Score : 1.0\n",
      "[636/1000]\n",
      "- Train Loss : 8.341409518739529e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.322634814512503e-13 Score : 1.0\n",
      "[637/1000]\n",
      "- Train Loss : 8.097542969750006e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.2869832655252054e-13 Score : 1.0\n",
      "[638/1000]\n",
      "- Train Loss : 7.866270994865622e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.2526570181684996e-13 Score : 1.0\n",
      "[639/1000]\n",
      "- Train Loss : 7.646706422503253e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.2195999649799597e-13 Score : 1.0\n",
      "[640/1000]\n",
      "- Train Loss : 7.438050832642324e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.187729300018662e-13 Score : 1.0\n",
      "[641/1000]\n",
      "- Train Loss : 7.239614300429219e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.1570080248854706e-13 Score : 1.0\n",
      "[642/1000]\n",
      "- Train Loss : 7.050705230827053e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.1273307009191111e-13 Score : 1.0\n",
      "[643/1000]\n",
      "- Train Loss : 6.870766967882599e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.098707085939136e-13 Score : 1.0\n",
      "[644/1000]\n",
      "- Train Loss : 6.699214042838831e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.0710648094505318e-13 Score : 1.0\n",
      "[645/1000]\n",
      "- Train Loss : 6.535511183197637e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.0443335338373586e-13 Score : 1.0\n",
      "[646/1000]\n",
      "- Train Loss : 6.379165567718046e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 2.0184780225290105e-13 Score : 1.0\n",
      "[647/1000]\n",
      "- Train Loss : 6.229753900628468e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.993471305996447e-13 Score : 1.0\n",
      "[648/1000]\n",
      "- Train Loss : 6.086844246884845e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.9692701516780403e-13 Score : 1.0\n",
      "[649/1000]\n",
      "- Train Loss : 5.950087348672071e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.945823466546412e-13 Score : 1.0\n",
      "[650/1000]\n",
      "- Train Loss : 5.819074657110076e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.923081512826899e-13 Score : 1.0\n",
      "[651/1000]\n",
      "- Train Loss : 5.693474750878045e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.9010589272488304e-13 Score : 1.0\n",
      "[652/1000]\n",
      "- Train Loss : 5.573095194711249e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.879704481259556e-13 Score : 1.0\n",
      "[653/1000]\n",
      "- Train Loss : 5.457586573971481e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.8589806343588533e-13 Score : 1.0\n",
      "[654/1000]\n",
      "- Train Loss : 5.3466198094668e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.838860417017682e-13 Score : 1.0\n",
      "[655/1000]\n",
      "- Train Loss : 5.239920933336038e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.8193052045336472e-13 Score : 1.0\n",
      "[656/1000]\n",
      "- Train Loss : 5.137324738528875e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.8003015799048644e-13 Score : 1.0\n",
      "[657/1000]\n",
      "- Train Loss : 5.038623276901812e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.7818296209164142e-13 Score : 1.0\n",
      "[658/1000]\n",
      "- Train Loss : 4.943626954370102e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.763876859243313e-13 Score : 1.0\n",
      "[659/1000]\n",
      "- Train Loss : 4.852042521326866e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.7463955899899714e-13 Score : 1.0\n",
      "[660/1000]\n",
      "- Train Loss : 4.763809572759322e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.729407226149296e-13 Score : 1.0\n",
      "[661/1000]\n",
      "- Train Loss : 4.678771914993588e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.7128585062895635e-13 Score : 1.0\n",
      "[662/1000]\n",
      "- Train Loss : 4.596629512217223e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.6967146004159828e-13 Score : 1.0\n",
      "[663/1000]\n",
      "- Train Loss : 4.517344592217813e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.6810085766948146e-13 Score : 1.0\n",
      "[664/1000]\n",
      "- Train Loss : 4.440761217459536e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.66568581844162e-13 Score : 1.0\n",
      "[665/1000]\n",
      "- Train Loss : 4.3667136580828844e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.6507517466672617e-13 Score : 1.0\n",
      "[666/1000]\n",
      "- Train Loss : 4.2951488130925246e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.6361955193500144e-13 Score : 1.0\n",
      "[667/1000]\n",
      "- Train Loss : 4.2259552808418656e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.6219653658361421e-13 Score : 1.0\n",
      "[668/1000]\n",
      "- Train Loss : 4.1588896356334337e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.6080799886131203e-13 Score : 1.0\n",
      "[669/1000]\n",
      "- Train Loss : 4.0939158220892174e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.5945233956989047e-13 Score : 1.0\n",
      "[670/1000]\n",
      "- Train Loss : 4.031054607594715e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.5812835253443264e-13 Score : 1.0\n",
      "[671/1000]\n",
      "- Train Loss : 3.9700690905029493e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.5683419461124531e-13 Score : 1.0\n",
      "[672/1000]\n",
      "- Train Loss : 3.91095995605427e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.5556845633750427e-13 Score : 1.0\n",
      "[673/1000]\n",
      "- Train Loss : 3.853596923957054e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.543305820595961e-13 Score : 1.0\n",
      "[674/1000]\n",
      "- Train Loss : 3.797909320472347e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.5312039559466778e-13 Score : 1.0\n",
      "[675/1000]\n",
      "- Train Loss : 3.7438611374172423e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.5193566077573856e-13 Score : 1.0\n",
      "[676/1000]\n",
      "- Train Loss : 3.6913477327052886e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.5077877640011506e-13 Score : 1.0\n",
      "[677/1000]\n",
      "- Train Loss : 3.6403691371929943e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.496455682894332e-13 Score : 1.0\n",
      "[678/1000]\n",
      "- Train Loss : 3.5908039004098564e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.4853560276282401e-13 Score : 1.0\n",
      "[679/1000]\n",
      "- Train Loss : 3.5426094740764155e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.4744879850512455e-13 Score : 1.0\n",
      "[680/1000]\n",
      "- Train Loss : 3.4957069573330667e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.4638473538799296e-13 Score : 1.0\n",
      "[681/1000]\n",
      "- Train Loss : 3.4500842156168157e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.4533999817458593e-13 Score : 1.0\n",
      "[682/1000]\n",
      "- Train Loss : 3.405628547144833e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.4431678237430273e-13 Score : 1.0\n",
      "[683/1000]\n",
      "- Train Loss : 3.36232539666348e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.4331225550896776e-13 Score : 1.0\n",
      "[684/1000]\n",
      "- Train Loss : 3.320176109205738e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.4232994123564158e-13 Score : 1.0\n",
      "[685/1000]\n",
      "- Train Loss : 3.27912398974272e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.4136486577685792e-13 Score : 1.0\n",
      "[686/1000]\n",
      "- Train Loss : 3.2390773615594943e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.404173001831599e-13 Score : 1.0\n",
      "[687/1000]\n",
      "- Train Loss : 3.200030495468501e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.3948807115870404e-13 Score : 1.0\n",
      "[688/1000]\n",
      "- Train Loss : 3.1619328095750407e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.3857717870349034e-13 Score : 1.0\n",
      "[689/1000]\n",
      "- Train Loss : 3.1248223963886735e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.376808958725509e-13 Score : 1.0\n",
      "[690/1000]\n",
      "- Train Loss : 3.088530003857197e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.3680113357221468e-13 Score : 1.0\n",
      "[691/1000]\n",
      "- Train Loss : 3.0531340928443694e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.3593736325392264e-13 Score : 1.0\n",
      "[692/1000]\n",
      "- Train Loss : 3.018570278337545e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.3509014057128815e-13 Score : 1.0\n",
      "[693/1000]\n",
      "- Train Loss : 2.984823498714841e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.3425763593314516e-13 Score : 1.0\n",
      "[694/1000]\n",
      "- Train Loss : 2.9518839192176607e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.3343793843316465e-13 Score : 1.0\n",
      "[695/1000]\n",
      "- Train Loss : 2.919654126473955e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.3263335200096316e-13 Score : 1.0\n",
      "[696/1000]\n",
      "- Train Loss : 2.888129359643316e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.318416811271414e-13 Score : 1.0\n",
      "[697/1000]\n",
      "- Train Loss : 2.8573203136890943e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.3106348146531277e-13 Score : 1.0\n",
      "[698/1000]\n",
      "- Train Loss : 2.827183357099619e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.3029709960716424e-13 Score : 1.0\n",
      "[699/1000]\n",
      "- Train Loss : 2.7976800889793685e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2954359264981397e-13 Score : 1.0\n",
      "[700/1000]\n",
      "- Train Loss : 2.7688170214895707e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2880182218098085e-13 Score : 1.0\n",
      "[701/1000]\n",
      "- Train Loss : 2.740583515656554e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.280735771342495e-13 Score : 1.0\n",
      "[702/1000]\n",
      "- Train Loss : 2.7129395691186597e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.273565129224219e-13 Score : 1.0\n",
      "[703/1000]\n",
      "- Train Loss : 2.685894408178402e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2664900324223932e-13 Score : 1.0\n",
      "[704/1000]\n",
      "- Train Loss : 2.659351745710603e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2595266084443335e-13 Score : 1.0\n",
      "[705/1000]\n",
      "- Train Loss : 2.6333691565268794e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2526730954615095e-13 Score : 1.0\n",
      "[706/1000]\n",
      "- Train Loss : 2.607879131642337e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2459304421508222e-13 Score : 1.0\n",
      "[707/1000]\n",
      "- Train Loss : 2.582947003072877e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2392810302269686e-13 Score : 1.0\n",
      "[708/1000]\n",
      "- Train Loss : 2.558498105814964e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2327281122964662e-13 Score : 1.0\n",
      "[709/1000]\n",
      "- Train Loss : 2.5345119622662175e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2262708752076856e-13 Score : 1.0\n",
      "[710/1000]\n",
      "- Train Loss : 2.5109759724221274e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2199055242530232e-13 Score : 1.0\n",
      "[711/1000]\n",
      "- Train Loss : 2.4878996216839263e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2136381580696992e-13 Score : 1.0\n",
      "[712/1000]\n",
      "- Train Loss : 2.465288636597183e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2074697253346145e-13 Score : 1.0\n",
      "[713/1000]\n",
      "- Train Loss : 2.4431224668255205e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.2013785420043194e-13 Score : 1.0\n",
      "[714/1000]\n",
      "- Train Loss : 2.421329497694504e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1953751790499956e-13 Score : 1.0\n",
      "[715/1000]\n",
      "- Train Loss : 2.3999320866097214e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1894539444102376e-13 Score : 1.0\n",
      "[716/1000]\n",
      "- Train Loss : 2.378939101557183e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1836049447402214e-13 Score : 1.0\n",
      "[717/1000]\n",
      "- Train Loss : 2.358306657007137e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1778498640833968e-13 Score : 1.0\n",
      "[718/1000]\n",
      "- Train Loss : 2.338084176012236e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1721698644270168e-13 Score : 1.0\n",
      "[719/1000]\n",
      "- Train Loss : 2.318200588638844e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1665618286898355e-13 Score : 1.0\n",
      "[720/1000]\n",
      "- Train Loss : 2.298659657755463e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.161026976599297e-13 Score : 1.0\n",
      "[721/1000]\n",
      "- Train Loss : 2.279462938174919e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.155580215935273e-13 Score : 1.0\n",
      "[722/1000]\n",
      "- Train Loss : 2.260640609123002e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1501896982589466e-13 Score : 1.0\n",
      "[723/1000]\n",
      "- Train Loss : 2.2421019813462636e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1448922864441824e-13 Score : 1.0\n",
      "[724/1000]\n",
      "- Train Loss : 2.2239214637432407e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1396561320521637e-13 Score : 1.0\n",
      "[725/1000]\n",
      "- Train Loss : 2.206031219479423e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1344783212895518e-13 Score : 1.0\n",
      "[726/1000]\n",
      "- Train Loss : 2.1884392096668467e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1293672567231836e-13 Score : 1.0\n",
      "[727/1000]\n",
      "- Train Loss : 2.1711322389788025e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.124322328489337e-13 Score : 1.0\n",
      "[728/1000]\n",
      "- Train Loss : 2.1541092288119475e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1193339142937311e-13 Score : 1.0\n",
      "[729/1000]\n",
      "- Train Loss : 2.1373638154796454e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1144209199117489e-13 Score : 1.0\n",
      "[730/1000]\n",
      "- Train Loss : 2.1209172336998862e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1095611869614899e-13 Score : 1.0\n",
      "[731/1000]\n",
      "- Train Loss : 2.104700852165943e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1047522759880662e-13 Score : 1.0\n",
      "[732/1000]\n",
      "- Train Loss : 2.0887427332429514e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.1000059776901036e-13 Score : 1.0\n",
      "[733/1000]\n",
      "- Train Loss : 2.0730460679760118e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0953240538961323e-13 Score : 1.0\n",
      "[734/1000]\n",
      "- Train Loss : 2.0576033894832986e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0906953915338843e-13 Score : 1.0\n",
      "[735/1000]\n",
      "- Train Loss : 2.0424101624264997e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0861237175483276e-13 Score : 1.0\n",
      "[736/1000]\n",
      "- Train Loss : 2.0274507283370041e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0816143851876889e-13 Score : 1.0\n",
      "[737/1000]\n",
      "- Train Loss : 2.0127363123550002e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0771506570809303e-13 Score : 1.0\n",
      "[738/1000]\n",
      "- Train Loss : 1.998226790596736e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0727420877596969e-13 Score : 1.0\n",
      "[739/1000]\n",
      "- Train Loss : 1.9839445036310853e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0683760056110977e-13 Score : 1.0\n",
      "[740/1000]\n",
      "- Train Loss : 1.9698610103283708e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0640581704591742e-13 Score : 1.0\n",
      "[741/1000]\n",
      "- Train Loss : 1.9574065897305602e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0607917403640185e-13 Score : 1.0\n",
      "[742/1000]\n",
      "- Train Loss : 1.9514639724175193e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0575474008881272e-13 Score : 1.0\n",
      "[743/1000]\n",
      "- Train Loss : 1.9455580038113026e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0543149876361332e-13 Score : 1.0\n",
      "[744/1000]\n",
      "- Train Loss : 1.9396992110842176e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0511184208184671e-13 Score : 1.0\n",
      "[745/1000]\n",
      "- Train Loss : 1.9338675551975726e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0479359486290432e-13 Score : 1.0\n",
      "[746/1000]\n",
      "- Train Loss : 1.928091429825702e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0447845794894425e-13 Score : 1.0\n",
      "[747/1000]\n",
      "- Train Loss : 1.9223584175396302e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.04165475886802e-13 Score : 1.0\n",
      "[748/1000]\n",
      "- Train Loss : 1.9166582346377078e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0385363223694086e-13 Score : 1.0\n",
      "[749/1000]\n",
      "- Train Loss : 1.910977152677579e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0354390955757964e-13 Score : 1.0\n",
      "[750/1000]\n",
      "- Train Loss : 1.9053362766178322e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0323472897930466e-13 Score : 1.0\n",
      "[751/1000]\n",
      "- Train Loss : 1.899721881675498e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0292802851349925e-13 Score : 1.0\n",
      "[752/1000]\n",
      "- Train Loss : 1.8941355480587842e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0262262231403724e-13 Score : 1.0\n",
      "[753/1000]\n",
      "- Train Loss : 1.8885750414061564e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0231874755014386e-13 Score : 1.0\n",
      "[754/1000]\n",
      "- Train Loss : 1.88304384486344e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0201708184817693e-13 Score : 1.0\n",
      "[755/1000]\n",
      "- Train Loss : 1.877549879061964e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0171710343584092e-13 Score : 1.0\n",
      "[756/1000]\n",
      "- Train Loss : 1.8720837475775088e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0141860224896493e-13 Score : 1.0\n",
      "[757/1000]\n",
      "- Train Loss : 1.8666644933978052e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0112288610641951e-13 Score : 1.0\n",
      "[758/1000]\n",
      "- Train Loss : 1.861273134554665e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0082862686054336e-13 Score : 1.0\n",
      "[759/1000]\n",
      "- Train Loss : 1.8559151956625042e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0053578385375503e-13 Score : 1.0\n",
      "[760/1000]\n",
      "- Train Loss : 1.850586998830734e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 1.0024514990889313e-13 Score : 1.0\n",
      "[761/1000]\n",
      "- Train Loss : 1.845297218309025e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.995534944445134e-14 Score : 1.0\n",
      "[762/1000]\n",
      "- Train Loss : 1.8400205133007277e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.966733113733056e-14 Score : 1.0\n",
      "[763/1000]\n",
      "- Train Loss : 1.8347788117708162e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.938186070531513e-14 Score : 1.0\n",
      "[764/1000]\n",
      "- Train Loss : 1.8295759656726456e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.909775907854246e-14 Score : 1.0\n",
      "[765/1000]\n",
      "- Train Loss : 1.8244030859084576e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.881561579194384e-14 Score : 1.0\n",
      "[766/1000]\n",
      "- Train Loss : 1.819272416757552e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.853597294660552e-14 Score : 1.0\n",
      "[767/1000]\n",
      "- Train Loss : 1.8141687506343733e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.825689930740775e-14 Score : 1.0\n",
      "[768/1000]\n",
      "- Train Loss : 1.8090916991663375e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.797901829059971e-14 Score : 1.0\n",
      "[769/1000]\n",
      "- Train Loss : 1.8040369528427752e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.770340732209032e-14 Score : 1.0\n",
      "[770/1000]\n",
      "- Train Loss : 1.7990185550694885e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.742875858300901e-14 Score : 1.0\n",
      "[771/1000]\n",
      "- Train Loss : 1.794016596577167e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.715620370937331e-14 Score : 1.0\n",
      "[772/1000]\n",
      "- Train Loss : 1.7890445843187072e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.688437389594046e-14 Score : 1.0\n",
      "[773/1000]\n",
      "- Train Loss : 1.7840994189268517e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.661406874181266e-14 Score : 1.0\n",
      "[774/1000]\n",
      "- Train Loss : 1.7791758517849922e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.634524081314488e-14 Score : 1.0\n",
      "[775/1000]\n",
      "- Train Loss : 1.774281610632585e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.607771392708409e-14 Score : 1.0\n",
      "[776/1000]\n",
      "- Train Loss : 1.7694088133553492e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.581220636756954e-14 Score : 1.0\n",
      "[777/1000]\n",
      "- Train Loss : 1.764567126224915e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.554780333901822e-14 Score : 1.0\n",
      "[778/1000]\n",
      "- Train Loss : 1.7597515085681717e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.528504016625278e-14 Score : 1.0\n",
      "[779/1000]\n",
      "- Train Loss : 1.754968463973677e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.502353737851285e-14 Score : 1.0\n",
      "[780/1000]\n",
      "- Train Loss : 1.750202341717286e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.47638303006211e-14 Score : 1.0\n",
      "[781/1000]\n",
      "- Train Loss : 1.7454691076261555e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.450520742490184e-14 Score : 1.0\n",
      "[782/1000]\n",
      "- Train Loss : 1.7407653134767195e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.424889525506269e-14 Score : 1.0\n",
      "[783/1000]\n",
      "- Train Loss : 1.7360995786280636e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.399419583595511e-14 Score : 1.0\n",
      "[784/1000]\n",
      "- Train Loss : 1.7314588342353e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.374124469285067e-14 Score : 1.0\n",
      "[785/1000]\n",
      "- Train Loss : 1.7268375427129547e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.348916091148421e-14 Score : 1.0\n",
      "[786/1000]\n",
      "- Train Loss : 1.722244419005581e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.323810034591803e-14 Score : 1.0\n",
      "[787/1000]\n",
      "- Train Loss : 1.7176787439063753e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.298824595526875e-14 Score : 1.0\n",
      "[788/1000]\n",
      "- Train Loss : 1.7131207550567508e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.273978069865296e-14 Score : 1.0\n",
      "[789/1000]\n",
      "- Train Loss : 1.7085895259665027e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.249215569872085e-14 Score : 1.0\n",
      "[790/1000]\n",
      "- Train Loss : 1.7040739064063147e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.224570299238774e-14 Score : 1.0\n",
      "[791/1000]\n",
      "- Train Loss : 1.6995809689736303e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.200044290844436e-14 Score : 1.0\n",
      "[792/1000]\n",
      "- Train Loss : 1.6951118620972093e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.175636867062714e-14 Score : 1.0\n",
      "[793/1000]\n",
      "- Train Loss : 1.6906625450178434e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.151345317388176e-14 Score : 1.0\n",
      "[794/1000]\n",
      "- Train Loss : 1.6862364861411208e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.1271892929852e-14 Score : 1.0\n",
      "[795/1000]\n",
      "- Train Loss : 1.6818364510025063e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.103183024007297e-14 Score : 1.0\n",
      "[796/1000]\n",
      "- Train Loss : 1.6774644583313564e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.079326510454469e-14 Score : 1.0\n",
      "[797/1000]\n",
      "- Train Loss : 1.6731196442677168e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.055619074700358e-14 Score : 1.0\n",
      "[798/1000]\n",
      "- Train Loss : 1.6687940302884677e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.031990243603752e-14 Score : 1.0\n",
      "[799/1000]\n",
      "- Train Loss : 1.6644867991368218e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 9.008509135053147e-14 Score : 1.0\n",
      "[800/1000]\n",
      "- Train Loss : 1.6602039997766654e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.98515813076324e-14 Score : 1.0\n",
      "[801/1000]\n",
      "- Train Loss : 1.65595234384048e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.961901994163426e-14 Score : 1.0\n",
      "[802/1000]\n",
      "- Train Loss : 1.6517095079896853e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.938722429342044e-14 Score : 1.0\n",
      "[803/1000]\n",
      "- Train Loss : 1.6474840874614626e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.915772579855957e-14 Score : 1.0\n",
      "[804/1000]\n",
      "- Train Loss : 1.6433004192358217e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.892917598059963e-14 Score : 1.0\n",
      "[805/1000]\n",
      "- Train Loss : 1.639144288088988e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.870170358854859e-14 Score : 1.0\n",
      "[806/1000]\n",
      "- Train Loss : 1.6349932706421013e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.847599980129142e-14 Score : 1.0\n",
      "[807/1000]\n",
      "- Train Loss : 1.630881821897877e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.825154284653261e-14 Score : 1.0\n",
      "[808/1000]\n",
      "- Train Loss : 1.6267827606223853e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.802749924385206e-14 Score : 1.0\n",
      "[809/1000]\n",
      "- Train Loss : 1.6227021228344516e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.780483799894143e-14 Score : 1.0\n",
      "[810/1000]\n",
      "- Train Loss : 1.6186451402587624e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.758292891928796e-14 Score : 1.0\n",
      "[811/1000]\n",
      "- Train Loss : 1.6145991839233915e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.736273423431973e-14 Score : 1.0\n",
      "[812/1000]\n",
      "- Train Loss : 1.6105799900932528e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.71430952029649e-14 Score : 1.0\n",
      "[813/1000]\n",
      "- Train Loss : 1.6065764443379185e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.692451326872824e-14 Score : 1.0\n",
      "[814/1000]\n",
      "- Train Loss : 1.6025896685935883e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.670730013973435e-14 Score : 1.0\n",
      "[815/1000]\n",
      "- Train Loss : 1.5986196155941202e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.64904732577644e-14 Score : 1.0\n",
      "[816/1000]\n",
      "- Train Loss : 1.594673325806266e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.627484577444777e-14 Score : 1.0\n",
      "[817/1000]\n",
      "- Train Loss : 1.5907455883746428e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.606055999131959e-14 Score : 1.0\n",
      "[818/1000]\n",
      "- Train Loss : 1.5868249598965834e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.584697216333995e-14 Score : 1.0\n",
      "[819/1000]\n",
      "- Train Loss : 1.5829421119379993e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.563475314060306e-14 Score : 1.0\n",
      "[820/1000]\n",
      "- Train Loss : 1.5790718612812037e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.542353022861215e-14 Score : 1.0\n",
      "[821/1000]\n",
      "- Train Loss : 1.5752176347962234e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.521382519966272e-14 Score : 1.0\n",
      "[822/1000]\n",
      "- Train Loss : 1.5713927586264342e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.500477746828036e-14 Score : 1.0\n",
      "[823/1000]\n",
      "- Train Loss : 1.5675829959972774e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.479673940017113e-14 Score : 1.0\n",
      "[824/1000]\n",
      "- Train Loss : 1.563791547086928e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.459016500499475e-14 Score : 1.0\n",
      "[825/1000]\n",
      "- Train Loss : 1.560017749134162e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.438459349682792e-14 Score : 1.0\n",
      "[826/1000]\n",
      "- Train Loss : 1.556270966150904e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.41803162550045e-14 Score : 1.0\n",
      "[827/1000]\n",
      "- Train Loss : 1.552540316187392e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.397685894107401e-14 Score : 1.0\n",
      "[828/1000]\n",
      "- Train Loss : 1.5488240931872841e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.37745197106339e-14 Score : 1.0\n",
      "[829/1000]\n",
      "- Train Loss : 1.5451368223958076e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.357300040808674e-14 Score : 1.0\n",
      "[830/1000]\n",
      "- Train Loss : 1.5414542734069898e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.337195544399004e-14 Score : 1.0\n",
      "[831/1000]\n",
      "- Train Loss : 1.5377897922166728e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.317204211591087e-14 Score : 1.0\n",
      "[832/1000]\n",
      "- Train Loss : 1.5341436237863005e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.297275898034445e-14 Score : 1.0\n",
      "[833/1000]\n",
      "- Train Loss : 1.530512534263329e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.277442452167896e-14 Score : 1.0\n",
      "[834/1000]\n",
      "- Train Loss : 1.5268887898064394e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.257672703178981e-14 Score : 1.0\n",
      "[835/1000]\n",
      "- Train Loss : 1.5232791720910646e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.238028992692617e-14 Score : 1.0\n",
      "[836/1000]\n",
      "- Train Loss : 1.519689496977996e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.218446946204813e-14 Score : 1.0\n",
      "[837/1000]\n",
      "- Train Loss : 1.516123655133449e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.198942149121799e-14 Score : 1.0\n",
      "[838/1000]\n",
      "- Train Loss : 1.5125647997657435e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.179547805135107e-14 Score : 1.0\n",
      "[839/1000]\n",
      "- Train Loss : 1.50902936029484e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.160276111519177e-14 Score : 1.0\n",
      "[840/1000]\n",
      "- Train Loss : 1.505512592757544e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.141112160494138e-14 Score : 1.0\n",
      "[841/1000]\n",
      "- Train Loss : 1.5020113263489503e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.122008518214943e-14 Score : 1.0\n",
      "[842/1000]\n",
      "- Train Loss : 1.4985277429176987e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.103011940900282e-14 Score : 1.0\n",
      "[843/1000]\n",
      "- Train Loss : 1.4950594473755338e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.084075672331464e-14 Score : 1.0\n",
      "[844/1000]\n",
      "- Train Loss : 1.4916058984947057e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.065260021254336e-14 Score : 1.0\n",
      "[845/1000]\n",
      "- Train Loss : 1.4881755197322144e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.046550757515383e-14 Score : 1.0\n",
      "[846/1000]\n",
      "- Train Loss : 1.4847578223130175e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.027867921204385e-14 Score : 1.0\n",
      "[847/1000]\n",
      "- Train Loss : 1.4813542099641705e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 8.009290116978846e-14 Score : 1.0\n",
      "[848/1000]\n",
      "- Train Loss : 1.4779673669133649e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.99087765358461e-14 Score : 1.0\n",
      "[849/1000]\n",
      "- Train Loss : 1.4746033183018049e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.972522788430789e-14 Score : 1.0\n",
      "[850/1000]\n",
      "- Train Loss : 1.4712596196892697e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.954270244856995e-14 Score : 1.0\n",
      "[851/1000]\n",
      "- Train Loss : 1.4679272557536037e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.936104437457e-14 Score : 1.0\n",
      "[852/1000]\n",
      "- Train Loss : 1.4646076107683005e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.918057214669622e-14 Score : 1.0\n",
      "[853/1000]\n",
      "- Train Loss : 1.4613075947818501e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.900049294210995e-14 Score : 1.0\n",
      "[854/1000]\n",
      "- Train Loss : 1.4580237384216025e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.882128787552525e-14 Score : 1.0\n",
      "[855/1000]\n",
      "- Train Loss : 1.4547513895635226e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.864264523881753e-14 Score : 1.0\n",
      "[856/1000]\n",
      "- Train Loss : 1.4514934020440361e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.846483608252991e-14 Score : 1.0\n",
      "[857/1000]\n",
      "- Train Loss : 1.4482481323478133e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.828774521018156e-14 Score : 1.0\n",
      "[858/1000]\n",
      "- Train Loss : 1.445016112309497e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.811165044857918e-14 Score : 1.0\n",
      "[859/1000]\n",
      "- Train Loss : 1.4417965219143711e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.79363891673969e-14 Score : 1.0\n",
      "[860/1000]\n",
      "- Train Loss : 1.438595114704391e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.776152090950214e-14 Score : 1.0\n",
      "[861/1000]\n",
      "- Train Loss : 1.4354080910127107e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.758793336542363e-14 Score : 1.0\n",
      "[862/1000]\n",
      "- Train Loss : 1.4322354726738065e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.741458299057036e-14 Score : 1.0\n",
      "[863/1000]\n",
      "- Train Loss : 1.4290765210343886e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.724280470886719e-14 Score : 1.0\n",
      "[864/1000]\n",
      "- Train Loss : 1.4259294219011999e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.707141267418796e-14 Score : 1.0\n",
      "[865/1000]\n",
      "- Train Loss : 1.4228030776835986e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.690039333400553e-14 Score : 1.0\n",
      "[866/1000]\n",
      "- Train Loss : 1.4196773871726696e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.673034977577833e-14 Score : 1.0\n",
      "[867/1000]\n",
      "- Train Loss : 1.4165712166532365e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.656097029138179e-14 Score : 1.0\n",
      "[868/1000]\n",
      "- Train Loss : 1.4134751533455028e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.639209225049001e-14 Score : 1.0\n",
      "[869/1000]\n",
      "- Train Loss : 1.4103947568068505e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.62240273612276e-14 Score : 1.0\n",
      "[870/1000]\n",
      "- Train Loss : 1.4073236530065657e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.605649779678786e-14 Score : 1.0\n",
      "[871/1000]\n",
      "- Train Loss : 1.4042733034102084e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.589034381385446e-14 Score : 1.0\n",
      "[872/1000]\n",
      "- Train Loss : 1.4012284124164347e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.572439989509197e-14 Score : 1.0\n",
      "[873/1000]\n",
      "- Train Loss : 1.3982100225283041e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.55602720149684e-14 Score : 1.0\n",
      "[874/1000]\n",
      "- Train Loss : 1.3952081819344527e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.539634742275217e-14 Score : 1.0\n",
      "[875/1000]\n",
      "- Train Loss : 1.3922149870000963e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.523335117864613e-14 Score : 1.0\n",
      "[876/1000]\n",
      "- Train Loss : 1.3892357984525514e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.507085637804486e-14 Score : 1.0\n",
      "[877/1000]\n",
      "- Train Loss : 1.3862685884281276e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.490914762401865e-14 Score : 1.0\n",
      "[878/1000]\n",
      "- Train Loss : 1.3833174070172835e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.474805550997804e-14 Score : 1.0\n",
      "[879/1000]\n",
      "- Train Loss : 1.380368745296026e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.458760036471376e-14 Score : 1.0\n",
      "[880/1000]\n",
      "- Train Loss : 1.3774378280918e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.442749758515554e-14 Score : 1.0\n",
      "[881/1000]\n",
      "- Train Loss : 1.3745126211230638e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.426816729964522e-14 Score : 1.0\n",
      "[882/1000]\n",
      "- Train Loss : 1.371604803321921e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.410959595565564e-14 Score : 1.0\n",
      "[883/1000]\n",
      "- Train Loss : 1.3687043898638023e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.395149217385294e-14 Score : 1.0\n",
      "[884/1000]\n",
      "- Train Loss : 1.3658201140604295e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.3794018584563e-14 Score : 1.0\n",
      "[885/1000]\n",
      "- Train Loss : 1.3629379997444274e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.363716163525866e-14 Score : 1.0\n",
      "[886/1000]\n",
      "- Train Loss : 1.3600756753352709e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.34809281022035e-14 Score : 1.0\n",
      "[887/1000]\n",
      "- Train Loss : 1.3572220308256921e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.332542640561476e-14 Score : 1.0\n",
      "[888/1000]\n",
      "- Train Loss : 1.3543820906468878e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.317053457274805e-14 Score : 1.0\n",
      "[889/1000]\n",
      "- Train Loss : 1.351554885733652e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.301625937986694e-14 Score : 1.0\n",
      "[890/1000]\n",
      "- Train Loss : 1.3487363740337319e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.286327167706566e-14 Score : 1.0\n",
      "[891/1000]\n",
      "- Train Loss : 1.345934914167907e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.271060923491612e-14 Score : 1.0\n",
      "[892/1000]\n",
      "- Train Loss : 1.3431432912171529e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.255812975188319e-14 Score : 1.0\n",
      "[893/1000]\n",
      "- Train Loss : 1.3403637380082106e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.240680900992211e-14 Score : 1.0\n",
      "[894/1000]\n",
      "- Train Loss : 1.3375942399636615e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.225565767455047e-14 Score : 1.0\n",
      "[895/1000]\n",
      "- Train Loss : 1.334834168654258e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.210550922618839e-14 Score : 1.0\n",
      "[896/1000]\n",
      "- Train Loss : 1.332090892642483e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.195580801122245e-14 Score : 1.0\n",
      "[897/1000]\n",
      "- Train Loss : 1.3293528509992123e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.180683185645936e-14 Score : 1.0\n",
      "[898/1000]\n",
      "- Train Loss : 1.3266326408966597e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.165830293509243e-14 Score : 1.0\n",
      "[899/1000]\n",
      "- Train Loss : 1.3239216208307903e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.151089209721587e-14 Score : 1.0\n",
      "[900/1000]\n",
      "- Train Loss : 1.3212288219371137e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.136434184481372e-14 Score : 1.0\n",
      "[901/1000]\n",
      "- Train Loss : 1.3185543899550175e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.121863184909524e-14 Score : 1.0\n",
      "[902/1000]\n",
      "- Train Loss : 1.3158835591839744e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.107334875798219e-14 Score : 1.0\n",
      "[903/1000]\n",
      "- Train Loss : 1.313229440189243e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.092863487300968e-14 Score : 1.0\n",
      "[904/1000]\n",
      "- Train Loss : 1.310592514903243e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.078436144516975e-14 Score : 1.0\n",
      "[905/1000]\n",
      "- Train Loss : 1.3079526212221012e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.064063689467964e-14 Score : 1.0\n",
      "[906/1000]\n",
      "- Train Loss : 1.305322368290807e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.049733924879495e-14 Score : 1.0\n",
      "[907/1000]\n",
      "- Train Loss : 1.302700378040806e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.035447528377925e-14 Score : 1.0\n",
      "[908/1000]\n",
      "- Train Loss : 1.3000914340774075e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.021255999566448e-14 Score : 1.0\n",
      "[909/1000]\n",
      "- Train Loss : 1.2974947936828818e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 7.007067858886759e-14 Score : 1.0\n",
      "[910/1000]\n",
      "- Train Loss : 1.294900718168752e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.992961033370007e-14 Score : 1.0\n",
      "[911/1000]\n",
      "- Train Loss : 1.292323446621835e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.978922648115393e-14 Score : 1.0\n",
      "[912/1000]\n",
      "- Train Loss : 1.289758429459966e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.964925598068605e-14 Score : 1.0\n",
      "[913/1000]\n",
      "- Train Loss : 1.2871974459866543e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.950982080504084e-14 Score : 1.0\n",
      "[914/1000]\n",
      "- Train Loss : 1.28464611981658e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.937119878102499e-14 Score : 1.0\n",
      "[915/1000]\n",
      "- Train Loss : 1.282113894241866e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.92329901090874e-14 Score : 1.0\n",
      "[916/1000]\n",
      "- Train Loss : 1.2795847577691522e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.909518801296449e-14 Score : 1.0\n",
      "[917/1000]\n",
      "- Train Loss : 1.2770667535275383e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.895779926891984e-14 Score : 1.0\n",
      "[918/1000]\n",
      "- Train Loss : 1.2745522944293499e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.88209322971707e-14 Score : 1.0\n",
      "[919/1000]\n",
      "- Train Loss : 1.2720534409767676e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.868487170078735e-14 Score : 1.0\n",
      "[920/1000]\n",
      "- Train Loss : 1.269562034842051e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.854933965296309e-14 Score : 1.0\n",
      "[921/1000]\n",
      "- Train Loss : 1.2670875768055324e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.841432937743433e-14 Score : 1.0\n",
      "[922/1000]\n",
      "- Train Loss : 1.2646224358799372e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.827998317573622e-14 Score : 1.0\n",
      "[923/1000]\n",
      "- Train Loss : 1.2621640309458893e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.814577249930967e-14 Score : 1.0\n",
      "[924/1000]\n",
      "- Train Loss : 1.2597129387182924e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.801221912045019e-14 Score : 1.0\n",
      "[925/1000]\n",
      "- Train Loss : 1.257264526987836e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.787968895739099e-14 Score : 1.0\n",
      "[926/1000]\n",
      "- Train Loss : 1.2548415446103708e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.774756537014648e-14 Score : 1.0\n",
      "[927/1000]\n",
      "- Train Loss : 1.252424885325145e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.761581447739876e-14 Score : 1.0\n",
      "[928/1000]\n",
      "- Train Loss : 1.2500143944910512e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.748471410595452e-14 Score : 1.0\n",
      "[929/1000]\n",
      "- Train Loss : 1.2476112908494925e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.735399320527066e-14 Score : 1.0\n",
      "[930/1000]\n",
      "- Train Loss : 1.2452239984308133e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.722390249709956e-14 Score : 1.0\n",
      "[931/1000]\n",
      "- Train Loss : 1.242843826026454e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.709381178892845e-14 Score : 1.0\n",
      "[932/1000]\n",
      "- Train Loss : 1.2404685960159603e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.69650017945736e-14 Score : 1.0\n",
      "[933/1000]\n",
      "- Train Loss : 1.2381109782982725e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.68359275259392e-14 Score : 1.0\n",
      "[934/1000]\n",
      "- Train Loss : 1.2357506424912192e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.670774094783352e-14 Score : 1.0\n",
      "[935/1000]\n",
      "- Train Loss : 1.2334045715255991e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.657966278994509e-14 Score : 1.0\n",
      "[936/1000]\n",
      "- Train Loss : 1.2310641470469388e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.6452221600833e-14 Score : 1.0\n",
      "[937/1000]\n",
      "- Train Loss : 1.2287342753545617e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.632540382797009e-14 Score : 1.0\n",
      "[938/1000]\n",
      "- Train Loss : 1.2264082449462556e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.619868092279727e-14 Score : 1.0\n",
      "[939/1000]\n",
      "- Train Loss : 1.224092264516349e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.607234426464839e-14 Score : 1.0\n",
      "[940/1000]\n",
      "- Train Loss : 1.2217873186433134e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.594674621922952e-14 Score : 1.0\n",
      "[941/1000]\n",
      "- Train Loss : 1.2194853762327694e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.582152086830745e-14 Score : 1.0\n",
      "[942/1000]\n",
      "- Train Loss : 1.2171925544638556e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.569702057758822e-14 Score : 1.0\n",
      "[943/1000]\n",
      "- Train Loss : 1.2149164140222495e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.557277100862138e-14 Score : 1.0\n",
      "[944/1000]\n",
      "- Train Loss : 1.2126463128749697e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.544937524886538e-14 Score : 1.0\n",
      "[945/1000]\n",
      "- Train Loss : 1.2103879923932404e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.532595916031864e-14 Score : 1.0\n",
      "[946/1000]\n",
      "- Train Loss : 1.2081279220410366e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.520315293549392e-14 Score : 1.0\n",
      "[947/1000]\n",
      "- Train Loss : 1.2058847651974608e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.508120051988003e-14 Score : 1.0\n",
      "[948/1000]\n",
      "- Train Loss : 1.2036512998553333e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.495934297195624e-14 Score : 1.0\n",
      "[949/1000]\n",
      "- Train Loss : 1.2014226747178775e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.483822403676245e-14 Score : 1.0\n",
      "[950/1000]\n",
      "- Train Loss : 1.1992066736363698e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.471768786023638e-14 Score : 1.0\n",
      "[951/1000]\n",
      "- Train Loss : 1.196998535081452e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.459726010392755e-14 Score : 1.0\n",
      "[952/1000]\n",
      "- Train Loss : 1.1947994613371294e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.447754385529442e-14 Score : 1.0\n",
      "[953/1000]\n",
      "- Train Loss : 1.1926091716678952e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.435805799962294e-14 Score : 1.0\n",
      "[954/1000]\n",
      "- Train Loss : 1.1904241360431073e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.42393920718444e-14 Score : 1.0\n",
      "[955/1000]\n",
      "- Train Loss : 1.1882535036655138e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.412156640074954e-14 Score : 1.0\n",
      "[956/1000]\n",
      "- Train Loss : 1.1860949586556912e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.400407954283358e-14 Score : 1.0\n",
      "[957/1000]\n",
      "- Train Loss : 1.1839420168717553e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.388655880359972e-14 Score : 1.0\n",
      "[958/1000]\n",
      "- Train Loss : 1.1817931255566184e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.37698647685224e-14 Score : 1.0\n",
      "[959/1000]\n",
      "- Train Loss : 1.179656865571535e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.36537467158492e-14 Score : 1.0\n",
      "[960/1000]\n",
      "- Train Loss : 1.1775277925078263e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.35378455036105e-14 Score : 1.0\n",
      "[961/1000]\n",
      "- Train Loss : 1.1754095211526927e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.342202560653473e-14 Score : 1.0\n",
      "[962/1000]\n",
      "- Train Loss : 1.17328817273755e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.330678846812668e-14 Score : 1.0\n",
      "[963/1000]\n",
      "- Train Loss : 1.1711805980654983e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.319199856311478e-14 Score : 1.0\n",
      "[964/1000]\n",
      "- Train Loss : 1.1690807920312578e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.307729674952939e-14 Score : 1.0\n",
      "[965/1000]\n",
      "- Train Loss : 1.1669815558988035e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.296328611482896e-14 Score : 1.0\n",
      "[966/1000]\n",
      "- Train Loss : 1.1648947314511626e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.284947199177229e-14 Score : 1.0\n",
      "[967/1000]\n",
      "- Train Loss : 1.1628070226649415e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.273610510211178e-14 Score : 1.0\n",
      "[968/1000]\n",
      "- Train Loss : 1.1607363407209378e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.262342939133622e-14 Score : 1.0\n",
      "[969/1000]\n",
      "- Train Loss : 1.158669519547982e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.25108349957236e-14 Score : 1.0\n",
      "[970/1000]\n",
      "- Train Loss : 1.156605278135436e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.239844388801832e-14 Score : 1.0\n",
      "[971/1000]\n",
      "- Train Loss : 1.1545557521288358e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.228720474512131e-14 Score : 1.0\n",
      "[972/1000]\n",
      "- Train Loss : 1.15250768565622e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.217532863344796e-14 Score : 1.0\n",
      "[973/1000]\n",
      "- Train Loss : 1.1504676261723985e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.206448929010205e-14 Score : 1.0\n",
      "[974/1000]\n",
      "- Train Loss : 1.1484349821550504e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.195371770939193e-14 Score : 1.0\n",
      "[975/1000]\n",
      "- Train Loss : 1.146410013907126e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.184386770052841e-14 Score : 1.0\n",
      "[976/1000]\n",
      "- Train Loss : 1.1443950415584565e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.173408545430067e-14 Score : 1.0\n",
      "[977/1000]\n",
      "- Train Loss : 1.1423863935638433e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.162449294345312e-14 Score : 1.0\n",
      "[978/1000]\n",
      "- Train Loss : 1.1403847970854746e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.151545608621897e-14 Score : 1.0\n",
      "[979/1000]\n",
      "- Train Loss : 1.138387171481184e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.14071984992963e-14 Score : 1.0\n",
      "[980/1000]\n",
      "- Train Loss : 1.1364050963259204e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.12990086750094e-14 Score : 1.0\n",
      "[981/1000]\n",
      "- Train Loss : 1.1344252055032371e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.11912525315915e-14 Score : 1.0\n",
      "[982/1000]\n",
      "- Train Loss : 1.1324493637638516e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.108379454377103e-14 Score : 1.0\n",
      "[983/1000]\n",
      "- Train Loss : 1.130484845592331e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.097675668429239e-14 Score : 1.0\n",
      "[984/1000]\n",
      "- Train Loss : 1.1285199635264327e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.087002375667477e-14 Score : 1.0\n",
      "[985/1000]\n",
      "- Train Loss : 1.1265686828036462e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.076406332310505e-14 Score : 1.0\n",
      "[986/1000]\n",
      "- Train Loss : 1.1246276271348132e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.065839426886918e-14 Score : 1.0\n",
      "[987/1000]\n",
      "- Train Loss : 1.1226940065084991e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.055336895967323e-14 Score : 1.0\n",
      "[988/1000]\n",
      "- Train Loss : 1.1207663805580946e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.044841818937663e-14 Score : 1.0\n",
      "[989/1000]\n",
      "- Train Loss : 1.1188437708546432e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.034377235094104e-14 Score : 1.0\n",
      "[990/1000]\n",
      "- Train Loss : 1.1169255641816981e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.023963473227381e-14 Score : 1.0\n",
      "[991/1000]\n",
      "- Train Loss : 1.1150173727641771e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.013557842876951e-14 Score : 1.0\n",
      "[992/1000]\n",
      "- Train Loss : 1.1131159892206062e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 6.003203034503357e-14 Score : 1.0\n",
      "[993/1000]\n",
      "- Train Loss : 1.1112156689813747e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 5.992878041689506e-14 Score : 1.0\n",
      "[994/1000]\n",
      "- Train Loss : 1.1093260880842795e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 5.98254830549115e-14 Score : 1.0\n",
      "[995/1000]\n",
      "- Train Loss : 1.1074379781536964e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 5.972293108192153e-14 Score : 1.0\n",
      "[996/1000]\n",
      "- Train Loss : 1.1055553889976564e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 5.962056206804817e-14 Score : 1.0\n",
      "[997/1000]\n",
      "- Train Loss : 1.1036823879841663e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 5.95184708809815e-14 Score : 1.0\n",
      "[998/1000]\n",
      "- Train Loss : 1.1018108647046723e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 5.94167930459931e-14 Score : 1.0\n",
      "[999/1000]\n",
      "- Train Loss : 1.0999442924080693e-13 Score : 0.3333333333333333\n",
      "- Val Loss : 5.931516942111331e-14 Score : 1.0\n"
     ]
    }
   ],
   "source": [
    "#학습 확인 w. 손실값, 성능평가 지표\n",
    "\n",
    "loss_history=[[],[]]\n",
    "score_history=[[],[]]\n",
    "\n",
    "CNT = len(iris_dl)\n",
    "print(f'CNT : {CNT}')\n",
    "\n",
    "#BATCH_CNT=iris_ds.n_rows/BATCH_SIZE\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "\n",
    "    #학습 모드로 모델 설정\n",
    "    model.train()\n",
    "\n",
    "    total_loss=0\n",
    "    total_score=0\n",
    "\n",
    "    for feature_ts,target_ts in iris_dl:\n",
    "\n",
    "        #학습 진행\n",
    "        pre_y=model(feature_ts)\n",
    "\n",
    "        #손실 계산\n",
    "        loss=binary_loss(pre_y,target_ts)\n",
    "        total_loss+=loss.item()\n",
    "\n",
    "        #성능 평가\n",
    "        score=BinaryF1Score()(pre_y,target_ts)\n",
    "        #score=F1Score(task='binary)(pre_y,target_ts)\n",
    "        total_score+=score.item()\n",
    "\n",
    "        #최적화 진행\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    #각 에포크 당 검증 수행: 모델을 검증 모드로 설정\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #검증 데이터셋\n",
    "        val_feature_ts=torch.FloatTensor(val_ds.feature_df.values)\n",
    "        val_target_ts=torch.FloatTensor(val_ds.target_df.values)\n",
    "        \n",
    "        #평가\n",
    "        pre_val=model(val_feature_ts)\n",
    "\n",
    "        #손실 계산\n",
    "        val_loss=binary_loss(pre_val,val_target_ts)\n",
    "\n",
    "        #성능 평가\n",
    "        val_score=BinaryF1Score()(pre_val,val_target_ts)\n",
    "\n",
    "    #손실값, 성능평가값 저장\n",
    "    loss_history[0].append(total_loss/CNT)\n",
    "    score_history[0].append(total_score/CNT)\n",
    "\n",
    "    loss_history[1].append(val_loss)\n",
    "    score_history[1].append(val_score)\n",
    "\n",
    "    print(f'[{epoch}/{EPOCH}]\\n- Train Loss : {loss_history[0][-1]} Score : {score_history[0][-1]}')\n",
    "    print(f'- Val Loss : {loss_history[1][-1]} Score : {score_history[1][-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습 결과 확인(학습과 검증의 Loss, 성능지표 변화 확인) w. 시각화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
